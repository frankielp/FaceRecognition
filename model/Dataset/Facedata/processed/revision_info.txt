arguments: app.py
--------------------
tensorflow version: 2.11.0
--------------------
git hash: b'17d35d505716b9dd3262e681f4746cf574a46864'
--------------------
b'diff --git a/app.py b/app.py\nindex 6b6c558..36970d9 100644\n--- a/app.py\n+++ b/app.py\n@@ -1,60 +1,56 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-\n-from imutils.video import VideoStream\n-\n-from flask import Flask, request, render_template, Response, jsonify\n-from imutils.video import VideoStream\n-from model.src import facenet_config as facenet\n-\n-import imutils\n-import os\n-import sys\n+import collections\n+import glob\n+import io\n import math\n+import os\n import pickle\n-from model.src.align import detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-from pymongo import MongoClient\n+import sys\n from datetime import datetime\n+\n+import cv2\n+import imutils\n+import numpy as np\n import pymongo\n-# from bson.binary import Binary\n-from PIL import Image\n-import io\n+import tensorflow as tf\n+from flask import Flask, Response, jsonify, render_template, request\n+from google.oauth2 import service_account\n from google.oauth2.credentials import Credentials\n from googleapiclient.discovery import build\n+from googleapiclient.errors import HttpError\n from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n+from httplib2 import Http\n+from imutils.video import VideoStream\n+from oauth2client import client, file, tools\n+\n+# from bson.binary import Binary\n+from PIL import Image\n from pydrive.auth import GoogleAuth\n from pydrive.drive import GoogleDrive\n-from oauth2client import file, client, tools\n-from httplib2 import Http\n-from google.oauth2 import service_account\n-from googleapiclient.errors import HttpError\n-import glob\n+from pymongo import MongoClient\n+from sklearn.svm import SVC\n+import certifi\n+\n+from model.src import align_dataset_mtcnn, classifier\n from model.src import facenet_config\n-from model.src import align_dataset_mtcnn\n-from model.src import classifier\n+from model.src import facenet_config as facenet\n+from model.src.align import detect_face\n \n disabled = True\n \n-SERVICE_ACCOUNT_FILE = \'service_account.json\'\n-SCOPES = [\'https://www.googleapis.com/auth/drive.file\']\n-DRIVE_API_VERSION = \'v3\'\n+SERVICE_ACCOUNT_FILE = "service_account.json"\n+SCOPES = ["https://www.googleapis.com/auth/drive.file"]\n+DRIVE_API_VERSION = "v3"\n \n creds = service_account.Credentials.from_service_account_file(\n-    SERVICE_ACCOUNT_FILE,\n-    scopes=SCOPES\n+    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n )\n-service = build(\'drive\', DRIVE_API_VERSION, credentials=creds)\n+service = build("drive", DRIVE_API_VERSION, credentials=creds)\n \n # connect to db\n uri = "mongodb+srv://npn279:grab2023@cluster0.ek6wvyn.mongodb.net/?retryWrites=true&w=majority"\n-client = MongoClient(uri)\n+client = MongoClient(uri,tlsCAFile=certifi.where())\n \n # select database Bootcamp\n db = client.Bootcamp\n@@ -75,61 +71,78 @@ THRESHOLD = [0.6, 0.7, 0.7]\n FACTOR = 0.709\n IMAGE_SIZE = 182\n INPUT_IMAGE_SIZE = 160\n-CLASSIFIER_PATH = \'model/Models/facemodel.pkl\'\n+CLASSIFIER_PATH = "model/pretrained/facemodel.pkl"\n # VIDEO_PATH = args.path\n-FACENET_MODEL_PATH = \'model/Models/20180402-114759.pb\'\n-@app.route(\'/\')\n+FACENET_MODEL_PATH = "model/pretrained/20180402-114759.pb"\n+\n+\n+@app.route("/")\n def index():\n-    return render_template(\'signin.html\')\n+    return render_template("signup.html")\n \n \n def generate_frames():\n     # Load The Custom Classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n+    with open(CLASSIFIER_PATH, "rb") as file:\n         model, class_names = pickle.load(file)\n     print("Custom Classifier, Successfully loaded")\n \n     with tf.Graph().as_default():\n \n         # Cai dat GPU neu co\n-        gpu_options = tf.compat.v1.GPUOptions(\n-            per_process_gpu_memory_fraction=0.6)\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n-            gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n+        sess = tf.compat.v1.Session(\n+            config=tf.compat.v1.ConfigProto(\n+                gpu_options=gpu_options, log_device_placement=False\n+            )\n+        )\n \n         with sess.as_default():\n \n             # Load the model\n-            print(\'Loading feature extraction model\')\n+            print("Loading feature extraction model")\n             facenet_config.load_model(FACENET_MODEL_PATH)\n \n             # Get input and output tensors\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.compat.v1.get_default_graph(\n-            ).get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "input:0"\n+            )\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "embeddings:0"\n+            )\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "phase_train:0"\n+            )\n             embedding_size = embeddings.get_shape()[1]\n \n-            pnet, rnet, onet = detect_face.create_mtcnn(\n-                sess, "model/src/align")\n+            pnet, rnet, onet = detect_face.create_mtcnn(sess, "model/src/align")\n \n             people_detected = set()\n             person_detected = collections.Counter()\n \n-            while (True):\n+            while True:\n                 # success, frame = video.read()\n                 frame = video.read()\n \n                 frame = cv2.flip(frame, 1)\n \n                 bounding_boxes, _ = detect_face.detect_face(\n-                    frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n+                    frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR\n+                )\n \n                 faces_found = bounding_boxes.shape[0]\n                 try:\n                     if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n+                        cv2.putText(\n+                            frame,\n+                            "Only one face",\n+                            (0, 100),\n+                            cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                            1,\n+                            (255, 255, 255),\n+                            thickness=1,\n+                            lineType=2,\n+                        )\n                     elif faces_found > 0:\n                         det = bounding_boxes[:, 0:4]\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\n@@ -138,43 +151,73 @@ def generate_frames():\n                             bb[i][1] = det[i][1]\n                             bb[i][2] = det[i][2]\n                             bb[i][3] = det[i][3]\n-                            print(bb[i][3]-bb[i][1])\n+                            print(bb[i][3] - bb[i][1])\n                             print(frame.shape[0])\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0] > 0.25:\n-                                cropped = frame[bb[i][1]:bb[i]\n-                                                [3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                    interpolation=cv2.INTER_CUBIC)\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\n+                                cropped = frame[\n+                                    bb[i][1] : bb[i][3], bb[i][0] : bb[i][2], :\n+                                ]\n+                                scaled = cv2.resize(\n+                                    cropped,\n+                                    (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n+                                    interpolation=cv2.INTER_CUBIC,\n+                                )\n                                 scaled = facenet_config.prewhiten(scaled)\n                                 scaled_reshape = scaled.reshape(\n-                                    -1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n+                                    -1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3\n+                                )\n                                 feed_dict = {\n-                                    images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(\n-                                    embeddings, feed_dict=feed_dict)\n+                                    images_placeholder: scaled_reshape,\n+                                    phase_train_placeholder: False,\n+                                }\n+                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n \n                                 predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(\n-                                    predictions, axis=1)\n+                                best_class_indices = np.argmax(predictions, axis=1)\n                                 best_class_probabilities = predictions[\n-                                    np.arange(len(best_class_indices)), best_class_indices]\n+                                    np.arange(len(best_class_indices)),\n+                                    best_class_indices,\n+                                ]\n                                 best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(\n-                                    best_name, best_class_probabilities))\n+                                print(\n+                                    "Name: {}, Probability: {}".format(\n+                                        best_name, best_class_probabilities\n+                                    )\n+                                )\n \n                                 if best_class_probabilities > 0.4:\n                                     cv2.rectangle(\n-                                        frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                                        frame,\n+                                        (bb[i][0], bb[i][1]),\n+                                        (bb[i][2], bb[i][3]),\n+                                        (0, 255, 0),\n+                                        2,\n+                                    )\n                                     text_x = bb[i][0]\n                                     text_y = bb[i][3] + 20\n \n                                     name = class_names[best_class_indices[0]]\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n+                                    cv2.putText(\n+                                        frame,\n+                                        name,\n+                                        (text_x, text_y),\n+                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                                        1,\n+                                        (255, 255, 255),\n+                                        thickness=1,\n+                                        lineType=2,\n+                                    )\n+                                    cv2.putText(\n+                                        frame,\n+                                        str(round(best_class_probabilities[0], 3)),\n+                                        (text_x, text_y + 17),\n+                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                                        1,\n+                                        (255, 255, 255),\n+                                        thickness=1,\n+                                        lineType=2,\n+                                    )\n                                     person_detected[best_name] += 1\n                                 else:\n                                     name = "Unknown"\n@@ -182,34 +225,37 @@ def generate_frames():\n                 except:\n                     print("Dead")\n \n-                ret, buffer = cv2.imencode(\'.jpg\', frame)\n+                ret, buffer = cv2.imencode(".jpg", frame)\n                 frame = buffer.tobytes()\n-                yield (b\'--frame\\r\\n\'\n-                       b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + frame + b\'\\r\\n\')\n+                yield (\n+                    b"--frame\\r\\n" b"Content-Type: image/jpeg\\r\\n\\r\\n" + frame + b"\\r\\n"\n+                )\n \n \n-@app.route(\'/video_feed\')\n+@app.route("/video_feed")\n def video_feed():\n     """Stream the video frames to the web page"""\n-    return Response(generate_frames(), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n+    return Response(\n+        generate_frames(), mimetype="multipart/x-mixed-replace; boundary=frame"\n+    )\n \n \n-@app.route(\'/signin\')\n+@app.route("/signin")\n def signin():\n     """Render the signin.html template"""\n-    return render_template(\'signin.html\')\n+    return render_template("signin.html")\n \n \n-@app.route(\'/signup\')\n+@app.route("/signup")\n def signup():\n     """Render the signup.html template"""\n-    return render_template(\'signup.html\')\n+    return render_template("signup.html")\n \n \n-@app.route(\'/signup_image\')\n+@app.route("/signup_image")\n def signup_image():\n     """Render the signup_image.html template"""\n-    return render_template(\'signup_image.html\')\n+    return render_template("signup_image.html")\n \n \n def generate_frames_signup():\n@@ -217,32 +263,42 @@ def generate_frames_signup():\n     with tf.Graph().as_default():\n \n         # Cai dat GPU neu co\n-        gpu_options = tf.compat.v1.GPUOptions(\n-            per_process_gpu_memory_fraction=0.6)\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n-            gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n+        sess = tf.compat.v1.Session(\n+            config=tf.compat.v1.ConfigProto(\n+                gpu_options=gpu_options, log_device_placement=False\n+            )\n+        )\n \n         with sess.as_default():\n \n-            pnet, rnet, onet = detect_face.create_mtcnn(\n-                sess, "model/src/align")\n+            pnet, rnet, onet = detect_face.create_mtcnn(sess, "model/src/align")\n \n             people_detected = set()\n             person_detected = collections.Counter()\n \n-            while (True):\n+            while True:\n                 frame = video.read()\n \n                 frame = cv2.flip(frame, 1)\n \n                 bounding_boxes, _ = detect_face.detect_face(\n-                    frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n+                    frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR\n+                )\n \n                 faces_found = bounding_boxes.shape[0]\n                 try:\n                     if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n+                        cv2.putText(\n+                            frame,\n+                            "Only one face",\n+                            (0, 100),\n+                            cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                            1,\n+                            (255, 255, 255),\n+                            thickness=1,\n+                            lineType=2,\n+                        )\n                     elif faces_found > 0:\n                         det = bounding_boxes[:, 0:4]\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\n@@ -251,54 +307,73 @@ def generate_frames_signup():\n                             bb[i][1] = det[i][1]\n                             bb[i][2] = det[i][2]\n                             bb[i][3] = det[i][3]\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0] > 0.25:\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\n                                 cv2.rectangle(\n-                                    frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                                    frame,\n+                                    (bb[i][0], bb[i][1]),\n+                                    (bb[i][2], bb[i][3]),\n+                                    (0, 255, 0),\n+                                    2,\n+                                )\n \n                 except:\n                     print("Dead")\n \n-                ret, buffer = cv2.imencode(\'.jpg\', frame)\n+                ret, buffer = cv2.imencode(".jpg", frame)\n                 frame = buffer.tobytes()\n-                yield (b\'--frame\\r\\n\'\n-                       b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + frame + b\'\\r\\n\')\n+                yield (\n+                    b"--frame\\r\\n" b"Content-Type: image/jpeg\\r\\n\\r\\n" + frame + b"\\r\\n"\n+                )\n \n \n-@app.route(\'/video_feed_signup\')\n+@app.route("/video_feed_signup")\n def video_feed_signup():\n     """Stream the video frames to the web page"""\n-    return Response(generate_frames_signup(), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n+    return Response(\n+        generate_frames_signup(), mimetype="multipart/x-mixed-replace; boundary=frame"\n+    )\n \n \n-@app.route(\'/capture_signup\', methods=[\'POST\'])\n+@app.route("/capture_signup", methods=["POST"])\n def capture_image_signup():\n     """Capture the current frame and save it to a file"""\n     global image_counter\n \n     if image_counter >= 10:\n-        return jsonify({\'success\': False})\n+        return jsonify({"success": False})\n \n     # Read the next video frame\n     frame = video.read()\n \n     # Cai dat GPU neu co\n     gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(\n-        gpu_options=gpu_options, log_device_placement=False))\n+    sess = tf.compat.v1.Session(\n+        config=tf.compat.v1.ConfigProto(\n+            gpu_options=gpu_options, log_device_placement=False\n+        )\n+    )\n     tf.compat.v1.disable_eager_execution()\n \n     with sess.as_default():\n-        pnet, rnet, onet = detect_face.create_mtcnn(\n-            sess, "model/src/align")\n+        pnet, rnet, onet = detect_face.create_mtcnn(sess, "model/src/align")\n \n         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n         bounding_boxes, _ = detect_face.detect_face(\n-            frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n+            frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR\n+        )\n \n         faces = bounding_boxes.shape[0]\n         if faces > 1:\n-            cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                        1, (255, 255, 255), thickness=1, lineType=2)\n+            cv2.putText(\n+                frame,\n+                "Only one face",\n+                (0, 100),\n+                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n+                1,\n+                (255, 255, 255),\n+                thickness=1,\n+                lineType=2,\n+            )\n         elif faces > 0:\n             det = bounding_boxes[:, 0:4]\n             bb = np.zeros((faces, 4), dtype=np.int32)\n@@ -307,14 +382,19 @@ def capture_image_signup():\n                 bb[i][1] = det[i][1]\n                 bb[i][2] = det[i][2]\n                 bb[i][3] = det[i][3]\n-                if (bb[i][3]-bb[i][1])/frame.shape[0] > 0.25:\n+                if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\n                     cv2.rectangle(\n-                        frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n+                        frame,\n+                        (bb[i][0], bb[i][1]),\n+                        (bb[i][2], bb[i][3]),\n+                        (0, 255, 0),\n+                        2,\n+                    )\n \n-                    cropped_frame = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2]]\n+                    cropped_frame = frame[bb[i][1] : bb[i][3], bb[i][0] : bb[i][2]]\n \n                     data = request.get_json()\n-                    name = data[\'name\']\n+                    name = data["name"]\n                     num_docs = collection.count_documents({})\n                     new_id = f"user{num_docs + 1}"\n                     print(new_id)\n@@ -326,143 +406,178 @@ def capture_image_signup():\n                         if not os.path.exists("model/Dataset/Facedata"):\n                             os.makedirs("model/Dataset/Facedata")\n                             if not os.path.exists("model/Dataset/Facedata/raw"):\n-                                os.makedirs(\n-                                    "model/Dataset/Facedata/raw")\n-                    user_folder = os.path.join(\n-                        "model/Dataset/Facedata/raw", new_id)\n+                                os.makedirs("model/Dataset/Facedata/raw")\n+                    user_folder = os.path.join("model/Dataset/Facedata/raw", new_id)\n                     if not os.path.exists(user_folder):\n                         os.mkdir(user_folder)\n                     print(user_folder)\n \n                     cv2.imwrite(\n-                        f"{user_folder}/capture_image{image_counter+1}.png", frame)\n+                        f"{user_folder}/capture_image{image_counter+1}.png", frame\n+                    )\n                     print(f"Image {image_counter+1} saved successfully.")\n                     image_counter += 1\n \n-    return jsonify({\'success\': True})\n-@app.route(\'/save_data\', methods=[\'POST\'])\n+    return jsonify({"success": True})\n+\n+\n+@app.route("/save_data", methods=["POST"])\n def save_data():\n     data = request.json\n-    name = data[\'name\']\n-    position = data[\'position\']\n-    password = data[\'password\']\n-    gender = data[\'gender\']\n-    today = datetime.today().strftime(\'%d/%m/%Y\')\n+    name = data["name"]\n+    position = data["position"]\n+    password = data["password"]\n+    gender = data["gender"]\n+    today = datetime.today().strftime("%d/%m/%Y")\n     num_docs = collection.count_documents({})\n     new_id = f"user{num_docs + 1}"\n-    train_images = os.path.join(\n-        "model", "Dataset", "Facedata", "raw", new_id)\n+    train_images = os.path.join("model", "Dataset", "Facedata", "raw", new_id)\n \n     # Create the folder and print the folder ID\n     folder_name = new_id\n     file_metadata = {\n-        \'name\': folder_name,\n-        \'mimeType\': \'application/vnd.google-apps.folder\'\n+        "name": folder_name,\n+        "mimeType": "application/vnd.google-apps.folder",\n     }\n-    folder = service.files().create(body=file_metadata, fields=\'id\').execute()\n-    folder_id = folder.get(\'id\')\n-    print(f\'Folder created with ID: {folder_id}\')\n+    folder = service.files().create(body=file_metadata, fields="id").execute()\n+    folder_id = folder.get("id")\n+    print(f"Folder created with ID: {folder_id}")\n \n     # Traverse the directory structure of the local folder and upload each file to Google Drive\n     for dirpath, dirnames, filenames in os.walk(train_images):\n         for filename in filenames:\n             file_path = os.path.join(dirpath, filename)\n-            file_metadata = {\n-                \'name\': filename,\n-                \'parents\': [folder_id]\n-            }\n+            file_metadata = {"name": filename, "parents": [folder_id]}\n             media = MediaFileUpload(file_path)\n-            file = service.files().create(body=file_metadata,\n-                                          media_body=media, fields=\'id\').execute()\n+            file = (\n+                service.files()\n+                .create(body=file_metadata, media_body=media, fields="id")\n+                .execute()\n+            )\n             print(f\'File ID: {file.get("id")}\')\n \n     # Search for the folder by ID and print its name\n-    folder = service.files().get(fileId=folder_id, fields=\'id, name\').execute()\n+    folder = service.files().get(fileId=folder_id, fields="id, name").execute()\n     print(f\'Folder name: {folder.get("name")}\')\n \n     # Search for the folder by name and print its ID\n     query = f"\'{folder_id}\' in parents and mimeType contains \'image/\'"\n-    results = service.files().list(\n-        q=query, fields="nextPageToken, files(id, name)").execute()\n-    items = results.get(\'files\', [])\n+    results = (\n+        service.files().list(q=query, fields="nextPageToken, files(id, name)").execute()\n+    )\n+    items = results.get("files", [])\n     if items:\n-        first_image_id = items[0][\'id\']\n-        print(f\'Folder found with ID: {first_image_id}\')\n+        first_image_id = items[0]["id"]\n+        print(f"Folder found with ID: {first_image_id}")\n     else:\n-        print(\'Folder not found\')\n+        print("Folder not found")\n \n     align_dataset_mtcnn.preprocess()\n     classifier.train_classify()\n     # Save the data to MongoDB\n-    doc = {"_id": new_id, "name": name, "position": position,\n-           "password": password, "gender": gender, "type": "user",\n-           "date_in": today, "train_images": folder_id, "profile_image": first_image_id, }\n+    doc = {\n+        "_id": new_id,\n+        "name": name,\n+        "position": position,\n+        "password": password,\n+        "gender": gender,\n+        "type": "user",\n+        "date_in": today,\n+        "train_images": folder_id,\n+        "profile_image": first_image_id,\n+    }\n     collection.insert_one(doc)\n \n     print("re-train model\\n")\n     return "Data saved to MongoDB"\n \n \n-@app.route(\'/signin_manual\')\n+@app.route("/signin_manual")\n def signin_manual():\n     """Render the signup_image.html template"""\n-    return render_template(\'signin_manual.html\')\n+    return render_template("signin_manual.html")\n \n \n-@app.route(\'/authenticate\', methods=[\'GET\', \'POST\'])\n+@app.route("/authenticate", methods=["GET", "POST"])\n def authenticate():\n-    username = request.json[\'username\']\n-    password = request.json[\'password\']\n-    user = collection.find_one({\'_id\': username, \'password\': password})\n+    username = request.json["username"]\n+    password = request.json["password"]\n+    user = collection.find_one({"_id": username, "password": password})\n     if user:\n-        user_name = user.get(\'name\')\n+        user_name = user.get("name")\n         today = datetime.now()\n         day = today.day\n         month = today.month\n         year = today.year\n         login_data = logindata.find_one(\n-            {\'userId\': username, \'day\': day, \'month\': month, \'year\': year})\n+            {"userId": username, "day": day, "month": month, "year": year}\n+        )\n \n         if login_data:\n             time_out = datetime.now().strftime("%H:%M:%S")\n-            logindata.update_one({\'userId\': username, \'day\': day, \'month\': month, \'year\': year}, {\n-                                 \'$set\': {\'time_out\': time_out}})\n+            logindata.update_one(\n+                {"userId": username, "day": day, "month": month, "year": year},\n+                {"$set": {"time_out": time_out}},\n+            )\n         else:\n             time_in = datetime.now().strftime("%H:%M:%S")\n-            logindata.insert_one({\'userId\': username, \'name\': user_name, \'day\': day,\n-                                 \'month\': month, \'year\': year, \'time_in\': time_in, \'time_out\': time_in})\n-        return jsonify({\'success\': True, \'redirect\': \'/welcome\', \'username\': username, \'password\': password})\n+            logindata.insert_one(\n+                {\n+                    "userId": username,\n+                    "name": user_name,\n+                    "day": day,\n+                    "month": month,\n+                    "year": year,\n+                    "time_in": time_in,\n+                    "time_out": time_in,\n+                }\n+            )\n+        return jsonify(\n+            {\n+                "success": True,\n+                "redirect": "/welcome",\n+                "username": username,\n+                "password": password,\n+            }\n+        )\n     else:\n-        return jsonify({\'success\': False, \'redirect\': \'/signup\'})\n+        return jsonify({"success": False, "redirect": "/signup"})\n \n \n-@app.route(\'/welcome\')\n+@app.route("/welcome")\n def welcome():\n     """Render the signup_image.html template"""\n-    username = request.args.get(\'username\')\n-    password = request.args.get(\'password\')\n+    username = request.args.get("username")\n+    password = request.args.get("password")\n \n-    user = collection.find_one({\'_id\': username, \'password\': password})\n+    user = collection.find_one({"_id": username, "password": password})\n     if user:\n-        user_name = user.get(\'name\')\n-        user_position = user.get(\'position\')\n+        user_name = user.get("name")\n+        user_position = user.get("position")\n \n     today = datetime.now()\n     day = today.day\n     month = today.month\n     year = today.year\n     login_data = logindata.find_one(\n-        {\'userId\': username, \'day\': day, \'month\': month, \'year\': year})\n+        {"userId": username, "day": day, "month": month, "year": year}\n+    )\n \n     if login_data is not None:\n-        user_timeout = login_data.get(\'time_out\')\n-        user_timein = login_data.get(\'time_in\')\n+        user_timeout = login_data.get("time_out")\n+        user_timein = login_data.get("time_in")\n \n-    formatted_date = today.strftime(\'%d/%m/%Y\')\n+    formatted_date = today.strftime("%d/%m/%Y")\n \n-    return render_template(\'success.html\', user_name=user_name, user_position=user_position,\n-                           user_timein=user_timein, user_timeout=user_timeout, date=formatted_date)\n+    return render_template(\n+        "success.html",\n+        user_name=user_name,\n+        user_position=user_position,\n+        user_timein=user_timein,\n+        user_timeout=user_timeout,\n+        date=formatted_date,\n+    )\n \n \n-if __name__ == \'__main__\':\n-    app.run()\n\\ No newline at end of file\n+if __name__ == "__main__":\n+    app.run()\ndiff --git a/model/requirements.txt b/model/requirements.txt\nindex 9c19e2c..32ebfaa 100644\n--- a/model/requirements.txt\n+++ b/model/requirements.txt\n@@ -12,4 +12,6 @@ google-api-python-client\n imutils\n flask\n pymongo\n-pydrive\n\\ No newline at end of file\n+pydrive\n+isort\n+git+https://github.com/psf/black\n\\ No newline at end of file\ndiff --git a/model/src/__pycache__/align_dataset_mtcnn.cpython-37.pyc b/model/src/__pycache__/align_dataset_mtcnn.cpython-37.pyc\nindex 494ef14..c0d716a 100644\nBinary files a/model/src/__pycache__/align_dataset_mtcnn.cpython-37.pyc and b/model/src/__pycache__/align_dataset_mtcnn.cpython-37.pyc differ\ndiff --git a/model/src/__pycache__/classifier.cpython-37.pyc b/model/src/__pycache__/classifier.cpython-37.pyc\nindex c5f7805..ce83ab2 100644\nBinary files a/model/src/__pycache__/classifier.cpython-37.pyc and b/model/src/__pycache__/classifier.cpython-37.pyc differ\ndiff --git a/model/src/__pycache__/facenet_config.cpython-37.pyc b/model/src/__pycache__/facenet_config.cpython-37.pyc\nindex c562caf..3eb549e 100644\nBinary files a/model/src/__pycache__/facenet_config.cpython-37.pyc and b/model/src/__pycache__/facenet_config.cpython-37.pyc differ\ndiff --git a/model/src/align/__pycache__/detect_face.cpython-37.pyc b/model/src/align/__pycache__/detect_face.cpython-37.pyc\nindex c92884a..92fa4e7 100644\nBinary files a/model/src/align/__pycache__/detect_face.cpython-37.pyc and b/model/src/align/__pycache__/detect_face.cpython-37.pyc differ\ndiff --git a/model/src/align/detect_face.py b/model/src/align/detect_face.py\nindex 2300ff3..2fc46aa 100644\n--- a/model/src/align/detect_face.py\n+++ b/model/src/align/detect_face.py\n@@ -2,19 +2,19 @@\n https://github.com/kpzhang93/MTCNN_face_detection_alignment\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,26 +23,26 @@ https://github.com/kpzhang93/MTCNN_face_detection_alignment\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-from six import string_types, iteritems\n+from __future__ import absolute_import, division, print_function\n \n+import os\n+\n+# from math import floor\n+import cv2\n import numpy as np\n import tensorflow as tf\n-#from math import floor\n-import cv2\n-import os\n+from six import iteritems, string_types\n+\n \n def layer(op):\n     """Decorator for composable network layers."""\n \n     def layer_decorated(self, *args, **kwargs):\n         # Automatically set a name if not provided.\n-        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n+        name = kwargs.setdefault("name", self.get_unique_name(op.__name__))\n         # Figure out the layer inputs.\n         if len(self.terminals) == 0:\n-            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n+            raise RuntimeError("No input variables found for layer %s." % name)\n         elif len(self.terminals) == 1:\n             layer_input = self.terminals[0]\n         else:\n@@ -58,8 +58,8 @@ def layer(op):\n \n     return layer_decorated\n \n-class Network(object):\n \n+class Network(object):\n     def __init__(self, inputs, trainable=True):\n         # The input nodes for this network\n         self.inputs = inputs\n@@ -74,7 +74,7 @@ class Network(object):\n \n     def setup(self):\n         """Construct the network. """\n-        raise NotImplementedError(\'Must be implemented by the subclass.\')\n+        raise NotImplementedError("Must be implemented by the subclass.")\n \n     def load(self, data_path, session, ignore_missing=False):\n         """Load network weights.\n@@ -82,7 +82,9 @@ class Network(object):\n         session: The current TensorFlow session\n         ignore_missing: If true, serialized weights for missing layers are ignored.\n         """\n-        data_dict = np.load(data_path, encoding=\'latin1\',allow_pickle=True).item() #pylint: disable=no-member\n+        data_dict = np.load(\n+            data_path, encoding="latin1", allow_pickle=True\n+        ).item()  # pylint: disable=no-member\n \n         for op_name in data_dict:\n             with tf.compat.v1.variable_scope(op_name, reuse=True):\n@@ -105,7 +107,7 @@ class Network(object):\n                 try:\n                     fed_layer = self.layers[fed_layer]\n                 except KeyError:\n-                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n+                    raise KeyError("Unknown layer name fed: %s" % fed_layer)\n             self.terminals.append(fed_layer)\n         return self\n \n@@ -118,7 +120,7 @@ class Network(object):\n         This is used for auto-generating layer names based on the type-prefix.\n         """\n         ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n-        return \'%s_%d\' % (prefix, ident)\n+        return "%s_%d" % (prefix, ident)\n \n     def make_var(self, name, shape):\n         """Creates a new TensorFlow variable."""\n@@ -126,21 +128,23 @@ class Network(object):\n \n     def validate_padding(self, padding):\n         """Verifies that the padding is one of the supported ones."""\n-        assert padding in (\'SAME\', \'VALID\')\n+        assert padding in ("SAME", "VALID")\n \n     @layer\n-    def conv(self,\n-             inp,\n-             k_h,\n-             k_w,\n-             c_o,\n-             s_h,\n-             s_w,\n-             name,\n-             relu=True,\n-             padding=\'SAME\',\n-             group=1,\n-             biased=True):\n+    def conv(\n+        self,\n+        inp,\n+        k_h,\n+        k_w,\n+        c_o,\n+        s_h,\n+        s_w,\n+        name,\n+        relu=True,\n+        padding="SAME",\n+        group=1,\n+        biased=True,\n+    ):\n         # Verify that the padding is acceptable\n         self.validate_padding(padding)\n         # Get the number of channels in the input\n@@ -151,12 +155,12 @@ class Network(object):\n         # Convolution for a given input and kernel\n         convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n         with tf.compat.v1.variable_scope(name) as scope:\n-            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n+            kernel = self.make_var("weights", shape=[k_h, k_w, c_i // group, c_o])\n             # This is the common-case. Convolve the input without any further complications.\n             output = convolve(inp, kernel)\n             # Add the biases\n             if biased:\n-                biases = self.make_var(\'biases\', [c_o])\n+                biases = self.make_var("biases", [c_o])\n                 output = tf.nn.bias_add(output, biases)\n             if relu:\n                 # ReLU non-linearity\n@@ -167,18 +171,20 @@ class Network(object):\n     def prelu(self, inp, name):\n         with tf.compat.v1.variable_scope(name):\n             i = int(inp.get_shape()[-1])\n-            alpha = self.make_var(\'alpha\', shape=(i,))\n+            alpha = self.make_var("alpha", shape=(i,))\n             output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n         return output\n \n     @layer\n-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n+    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding="SAME"):\n         self.validate_padding(padding)\n-        return tf.nn.max_pool(inp,\n-                              ksize=[1, k_h, k_w, 1],\n-                              strides=[1, s_h, s_w, 1],\n-                              padding=padding,\n-                              name=name)\n+        return tf.nn.max_pool(\n+            inp,\n+            ksize=[1, k_h, k_w, 1],\n+            strides=[1, s_h, s_w, 1],\n+            padding=padding,\n+            name=name,\n+        )\n \n     @layer\n     def fc(self, inp, num_out, name, relu=True):\n@@ -192,109 +198,139 @@ class Network(object):\n                 feed_in = tf.reshape(inp, [-1, dim])\n             else:\n                 feed_in, dim = (inp, input_shape[-1])\n-            weights = self.make_var(\'weights\', shape=[dim, num_out])\n-            biases = self.make_var(\'biases\', [num_out])\n+            weights = self.make_var("weights", shape=[dim, num_out])\n+            biases = self.make_var("biases", [num_out])\n             op = tf.compat.v1.nn.relu_layer if relu else tf.compat.v1.nn.xw_plus_b\n             fc = op(feed_in, weights, biases, name=name)\n             return fc\n \n-\n     """\n     Multi dimensional softmax,\n     refer to https://github.com/tensorflow/tensorflow/issues/210\n     compute softmax along the dimension of target\n     the native softmax only supports batch_size x dimension\n     """\n+\n     @layer\n     def softmax(self, target, axis, name=None):\n         max_axis = tf.reduce_max(target, axis, keepdims=True)\n-        target_exp = tf.exp(target-max_axis)\n+        target_exp = tf.exp(target - max_axis)\n         normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n         softmax = tf.compat.v1.div(target_exp, normalize, name)\n         return softmax\n-    \n+\n+\n class PNet(Network):\n     def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'PReLU1\')\n-             .max_pool(2, 2, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'PReLU2\')\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'PReLU3\')\n-             .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n-             .softmax(3,name=\'prob1\'))\n-\n-        (self.feed(\'PReLU3\') #pylint: disable=no-value-for-parameter\n-             .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n-        \n+        (\n+            self.feed("data")  # pylint: disable=no-value-for-parameter, no-member\n+            .conv(3, 3, 10, 1, 1, padding="VALID", relu=False, name="conv1")\n+            .prelu(name="PReLU1")\n+            .max_pool(2, 2, 2, 2, name="pool1")\n+            .conv(3, 3, 16, 1, 1, padding="VALID", relu=False, name="conv2")\n+            .prelu(name="PReLU2")\n+            .conv(3, 3, 32, 1, 1, padding="VALID", relu=False, name="conv3")\n+            .prelu(name="PReLU3")\n+            .conv(1, 1, 2, 1, 1, relu=False, name="conv4-1")\n+            .softmax(3, name="prob1")\n+        )\n+\n+        (\n+            self.feed("PReLU3").conv(  # pylint: disable=no-value-for-parameter\n+                1, 1, 4, 1, 1, relu=False, name="conv4-2"\n+            )\n+        )\n+\n+\n class RNet(Network):\n     def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .fc(128, relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(2, relu=False, name=\'conv5-1\')\n-             .softmax(1,name=\'prob1\'))\n-\n-        (self.feed(\'prelu4\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv5-2\'))\n+        (\n+            self.feed("data")  # pylint: disable=no-value-for-parameter, no-member\n+            .conv(3, 3, 28, 1, 1, padding="VALID", relu=False, name="conv1")\n+            .prelu(name="prelu1")\n+            .max_pool(3, 3, 2, 2, name="pool1")\n+            .conv(3, 3, 48, 1, 1, padding="VALID", relu=False, name="conv2")\n+            .prelu(name="prelu2")\n+            .max_pool(3, 3, 2, 2, padding="VALID", name="pool2")\n+            .conv(2, 2, 64, 1, 1, padding="VALID", relu=False, name="conv3")\n+            .prelu(name="prelu3")\n+            .fc(128, relu=False, name="conv4")\n+            .prelu(name="prelu4")\n+            .fc(2, relu=False, name="conv5-1")\n+            .softmax(1, name="prob1")\n+        )\n+\n+        (\n+            self.feed("prelu4").fc(  # pylint: disable=no-value-for-parameter\n+                4, relu=False, name="conv5-2"\n+            )\n+        )\n+\n \n class ONet(Network):\n     def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .max_pool(2, 2, 2, 2, name=\'pool3\')\n-             .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(256, relu=False, name=\'conv5\')\n-             .prelu(name=\'prelu5\')\n-             .fc(2, relu=False, name=\'conv6-1\')\n-             .softmax(1, name=\'prob1\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv6-2\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(10, relu=False, name=\'conv6-3\'))\n+        (\n+            self.feed("data")  # pylint: disable=no-value-for-parameter, no-member\n+            .conv(3, 3, 32, 1, 1, padding="VALID", relu=False, name="conv1")\n+            .prelu(name="prelu1")\n+            .max_pool(3, 3, 2, 2, name="pool1")\n+            .conv(3, 3, 64, 1, 1, padding="VALID", relu=False, name="conv2")\n+            .prelu(name="prelu2")\n+            .max_pool(3, 3, 2, 2, padding="VALID", name="pool2")\n+            .conv(3, 3, 64, 1, 1, padding="VALID", relu=False, name="conv3")\n+            .prelu(name="prelu3")\n+            .max_pool(2, 2, 2, 2, name="pool3")\n+            .conv(2, 2, 128, 1, 1, padding="VALID", relu=False, name="conv4")\n+            .prelu(name="prelu4")\n+            .fc(256, relu=False, name="conv5")\n+            .prelu(name="prelu5")\n+            .fc(2, relu=False, name="conv6-1")\n+            .softmax(1, name="prob1")\n+        )\n+\n+        (\n+            self.feed("prelu5").fc(  # pylint: disable=no-value-for-parameter\n+                4, relu=False, name="conv6-2"\n+            )\n+        )\n+\n+        (\n+            self.feed("prelu5").fc(  # pylint: disable=no-value-for-parameter\n+                10, relu=False, name="conv6-3"\n+            )\n+        )\n+\n \n def create_mtcnn(sess, model_path):\n     if not model_path:\n-        model_path,_ = os.path.split(os.path.realpath(__file__))\n-\n-    with tf.compat.v1.variable_scope(\'pnet\'):\n-        data = tf.compat.v1.placeholder(tf.float32, (None,None,None,3), \'input\')\n-        pnet = PNet({\'data\':data})\n-        pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n-    with tf.compat.v1.variable_scope(\'rnet\'):\n-        data = tf.compat.v1.placeholder(tf.float32, (None,24,24,3), \'input\')\n-        rnet = RNet({\'data\':data})\n-        rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n-    with tf.compat.v1.variable_scope(\'onet\'):\n-        data = tf.compat.v1.placeholder(tf.float32, (None,48,48,3), \'input\')\n-        onet = ONet({\'data\':data})\n-        onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n-        \n-    pnet_fun = lambda img : sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\':img})\n-    rnet_fun = lambda img : sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\':img})\n-    onet_fun = lambda img : sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'), feed_dict={\'onet/input:0\':img})\n+        model_path, _ = os.path.split(os.path.realpath(__file__))\n+\n+    with tf.compat.v1.variable_scope("pnet"):\n+        data = tf.compat.v1.placeholder(tf.float32, (None, None, None, 3), "input")\n+        pnet = PNet({"data": data})\n+        pnet.load(os.path.join(model_path, "det1.npy"), sess)\n+    with tf.compat.v1.variable_scope("rnet"):\n+        data = tf.compat.v1.placeholder(tf.float32, (None, 24, 24, 3), "input")\n+        rnet = RNet({"data": data})\n+        rnet.load(os.path.join(model_path, "det2.npy"), sess)\n+    with tf.compat.v1.variable_scope("onet"):\n+        data = tf.compat.v1.placeholder(tf.float32, (None, 48, 48, 3), "input")\n+        onet = ONet({"data": data})\n+        onet.load(os.path.join(model_path, "det3.npy"), sess)\n+\n+    pnet_fun = lambda img: sess.run(\n+        ("pnet/conv4-2/BiasAdd:0", "pnet/prob1:0"), feed_dict={"pnet/input:0": img}\n+    )\n+    rnet_fun = lambda img: sess.run(\n+        ("rnet/conv5-2/conv5-2:0", "rnet/prob1:0"), feed_dict={"rnet/input:0": img}\n+    )\n+    onet_fun = lambda img: sess.run(\n+        ("onet/conv6-2/conv6-2:0", "onet/conv6-3/conv6-3:0", "onet/prob1:0"),\n+        feed_dict={"onet/input:0": img},\n+    )\n     return pnet_fun, rnet_fun, onet_fun\n \n+\n def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n     """Detects faces in an image, and returns bounding boxes and points for them.\n     img: input image\n@@ -303,122 +339,150 @@ def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n     threshold: threshold=[th1, th2, th3], th1-3 are three steps\'s threshold\n     factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n     """\n-    factor_count=0\n-    total_boxes=np.empty((0,9))\n-    points=np.empty(0)\n-    h=img.shape[0]\n-    w=img.shape[1]\n-    minl=np.amin([h, w])\n-    m=12.0/minsize\n-    minl=minl*m\n+    factor_count = 0\n+    total_boxes = np.empty((0, 9))\n+    points = np.empty(0)\n+    h = img.shape[0]\n+    w = img.shape[1]\n+    minl = np.amin([h, w])\n+    m = 12.0 / minsize\n+    minl = minl * m\n     # create scale pyramid\n-    scales=[]\n-    while minl>=12:\n-        scales += [m*np.power(factor, factor_count)]\n-        minl = minl*factor\n+    scales = []\n+    while minl >= 12:\n+        scales += [m * np.power(factor, factor_count)]\n+        minl = minl * factor\n         factor_count += 1\n \n     # first stage\n     for scale in scales:\n-        hs=int(np.ceil(h*scale))\n-        ws=int(np.ceil(w*scale))\n+        hs = int(np.ceil(h * scale))\n+        ws = int(np.ceil(w * scale))\n         im_data = imresample(img, (hs, ws))\n-        im_data = (im_data-127.5)*0.0078125\n+        im_data = (im_data - 127.5) * 0.0078125\n         img_x = np.expand_dims(im_data, 0)\n-        img_y = np.transpose(img_x, (0,2,1,3))\n+        img_y = np.transpose(img_x, (0, 2, 1, 3))\n         out = pnet(img_y)\n-        out0 = np.transpose(out[0], (0,2,1,3))\n-        out1 = np.transpose(out[1], (0,2,1,3))\n-        \n-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n-        \n+        out0 = np.transpose(out[0], (0, 2, 1, 3))\n+        out1 = np.transpose(out[1], (0, 2, 1, 3))\n+\n+        boxes, _ = generateBoundingBox(\n+            out1[0, :, :, 1].copy(), out0[0, :, :, :].copy(), scale, threshold[0]\n+        )\n+\n         # inter-scale nms\n-        pick = nms(boxes.copy(), 0.5, \'Union\')\n-        if boxes.size>0 and pick.size>0:\n-            boxes = boxes[pick,:]\n+        pick = nms(boxes.copy(), 0.5, "Union")\n+        if boxes.size > 0 and pick.size > 0:\n+            boxes = boxes[pick, :]\n             total_boxes = np.append(total_boxes, boxes, axis=0)\n \n     numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n-        total_boxes = total_boxes[pick,:]\n-        regw = total_boxes[:,2]-total_boxes[:,0]\n-        regh = total_boxes[:,3]-total_boxes[:,1]\n-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n+    if numbox > 0:\n+        pick = nms(total_boxes.copy(), 0.7, "Union")\n+        total_boxes = total_boxes[pick, :]\n+        regw = total_boxes[:, 2] - total_boxes[:, 0]\n+        regh = total_boxes[:, 3] - total_boxes[:, 1]\n+        qq1 = total_boxes[:, 0] + total_boxes[:, 5] * regw\n+        qq2 = total_boxes[:, 1] + total_boxes[:, 6] * regh\n+        qq3 = total_boxes[:, 2] + total_boxes[:, 7] * regw\n+        qq4 = total_boxes[:, 3] + total_boxes[:, 8] * regh\n+        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:, 4]]))\n         total_boxes = rerec(total_boxes.copy())\n-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n+        total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)\n         dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n \n     numbox = total_boxes.shape[0]\n-    if numbox>0:\n+    if numbox > 0:\n         # second stage\n-        tempimg = np.zeros((24,24,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n+        tempimg = np.zeros((24, 24, 3, numbox))\n+        for k in range(0, numbox):\n+            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n+            tmp[dy[k] - 1 : edy[k], dx[k] - 1 : edx[k], :] = img[\n+                y[k] - 1 : ey[k], x[k] - 1 : ex[k], :\n+            ]\n+            if (\n+                tmp.shape[0] > 0\n+                and tmp.shape[1] > 0\n+                or tmp.shape[0] == 0\n+                and tmp.shape[1] == 0\n+            ):\n+                tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n             else:\n                 return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n+        tempimg = (tempimg - 127.5) * 0.0078125\n+        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n         out = rnet(tempimg1)\n         out0 = np.transpose(out[0])\n         out1 = np.transpose(out[1])\n-        score = out1[1,:]\n-        ipass = np.where(score>threshold[1])\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-        if total_boxes.shape[0]>0:\n-            pick = nms(total_boxes, 0.7, \'Union\')\n-            total_boxes = total_boxes[pick,:]\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n+        score = out1[1, :]\n+        ipass = np.where(score > threshold[1])\n+        total_boxes = np.hstack(\n+            [total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)]\n+        )\n+        mv = out0[:, ipass[0]]\n+        if total_boxes.shape[0] > 0:\n+            pick = nms(total_boxes, 0.7, "Union")\n+            total_boxes = total_boxes[pick, :]\n+            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:, pick]))\n             total_boxes = rerec(total_boxes.copy())\n \n     numbox = total_boxes.shape[0]\n-    if numbox>0:\n+    if numbox > 0:\n         # third stage\n         total_boxes = np.fix(total_boxes).astype(np.int32)\n         dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n-        tempimg = np.zeros((48,48,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n+        tempimg = np.zeros((48, 48, 3, numbox))\n+        for k in range(0, numbox):\n+            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n+            tmp[dy[k] - 1 : edy[k], dx[k] - 1 : edx[k], :] = img[\n+                y[k] - 1 : ey[k], x[k] - 1 : ex[k], :\n+            ]\n+            if (\n+                tmp.shape[0] > 0\n+                and tmp.shape[1] > 0\n+                or tmp.shape[0] == 0\n+                and tmp.shape[1] == 0\n+            ):\n+                tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n             else:\n                 return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n+        tempimg = (tempimg - 127.5) * 0.0078125\n+        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))\n         out = onet(tempimg1)\n         out0 = np.transpose(out[0])\n         out1 = np.transpose(out[1])\n         out2 = np.transpose(out[2])\n-        score = out2[1,:]\n+        score = out2[1, :]\n         points = out1\n-        ipass = np.where(score>threshold[2])\n-        points = points[:,ipass[0]]\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-\n-        w = total_boxes[:,2]-total_boxes[:,0]+1\n-        h = total_boxes[:,3]-total_boxes[:,1]+1\n-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n-        if total_boxes.shape[0]>0:\n+        ipass = np.where(score > threshold[2])\n+        points = points[:, ipass[0]]\n+        total_boxes = np.hstack(\n+            [total_boxes[ipass[0], 0:4].copy(), np.expand_dims(score[ipass].copy(), 1)]\n+        )\n+        mv = out0[:, ipass[0]]\n+\n+        w = total_boxes[:, 2] - total_boxes[:, 0] + 1\n+        h = total_boxes[:, 3] - total_boxes[:, 1] + 1\n+        points[0:5, :] = (\n+            np.tile(w, (5, 1)) * points[0:5, :] + np.tile(total_boxes[:, 0], (5, 1)) - 1\n+        )\n+        points[5:10, :] = (\n+            np.tile(h, (5, 1)) * points[5:10, :]\n+            + np.tile(total_boxes[:, 1], (5, 1))\n+            - 1\n+        )\n+        if total_boxes.shape[0] > 0:\n             total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n-            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n-            total_boxes = total_boxes[pick,:]\n-            points = points[:,pick]\n-                \n+            pick = nms(total_boxes.copy(), 0.7, "Min")\n+            total_boxes = total_boxes[pick, :]\n+            points = points[:, pick]\n+\n     return total_boxes, points\n \n \n-def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n+def bulk_detect_face(\n+    images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor\n+):\n     """Detects faces in a list of images\n     images: list containing input images\n     detection_window_size_ratio: ratio of minimum face size to smallest image dimension\n@@ -430,7 +494,7 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n     images_with_boxes = [None] * len(images)\n \n     for i in range(len(images)):\n-        images_with_boxes[i] = {\'total_boxes\': np.empty((0, 9))}\n+        images_with_boxes[i] = {"total_boxes": np.empty((0, 9))}\n \n     # create scale pyramid\n     for index, img in enumerate(images):\n@@ -471,61 +535,82 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n \n             im_data = imresample(images[index], (hs, ws))\n             im_data = (im_data - 127.5) * 0.0078125\n-            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n-            images_obj_per_resolution[(ws, hs)].append({\'scale\': scale, \'image\': img_y, \'index\': index})\n+            img_y = np.transpose(\n+                im_data, (1, 0, 2)\n+            )  # caffe uses different dimensions ordering\n+            images_obj_per_resolution[(ws, hs)].append(\n+                {"scale": scale, "image": img_y, "index": index}\n+            )\n \n     for resolution in images_obj_per_resolution:\n-        images_per_resolution = [i[\'image\'] for i in images_obj_per_resolution[resolution]]\n+        images_per_resolution = [\n+            i["image"] for i in images_obj_per_resolution[resolution]\n+        ]\n         outs = pnet(images_per_resolution)\n \n         for index in range(len(outs[0])):\n-            scale = images_obj_per_resolution[resolution][index][\'scale\']\n-            image_index = images_obj_per_resolution[resolution][index][\'index\']\n+            scale = images_obj_per_resolution[resolution][index]["scale"]\n+            image_index = images_obj_per_resolution[resolution][index]["index"]\n             out0 = np.transpose(outs[0][index], (1, 0, 2))\n             out1 = np.transpose(outs[1][index], (1, 0, 2))\n \n-            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n+            boxes, _ = generateBoundingBox(\n+                out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0]\n+            )\n \n             # inter-scale nms\n-            pick = nms(boxes.copy(), 0.5, \'Union\')\n+            pick = nms(boxes.copy(), 0.5, "Union")\n             if boxes.size > 0 and pick.size > 0:\n                 boxes = boxes[pick, :]\n-                images_with_boxes[image_index][\'total_boxes\'] = np.append(images_with_boxes[image_index][\'total_boxes\'],\n-                                                                          boxes,\n-                                                                          axis=0)\n+                images_with_boxes[image_index]["total_boxes"] = np.append(\n+                    images_with_boxes[image_index]["total_boxes"], boxes, axis=0\n+                )\n \n     for index, image_obj in enumerate(images_with_boxes):\n-        numbox = image_obj[\'total_boxes\'].shape[0]\n+        numbox = image_obj["total_boxes"].shape[0]\n         if numbox > 0:\n             h = images[index].shape[0]\n             w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            regw = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0]\n-            regh = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1]\n-            qq1 = image_obj[\'total_boxes\'][:, 0] + image_obj[\'total_boxes\'][:, 5] * regw\n-            qq2 = image_obj[\'total_boxes\'][:, 1] + image_obj[\'total_boxes\'][:, 6] * regh\n-            qq3 = image_obj[\'total_boxes\'][:, 2] + image_obj[\'total_boxes\'][:, 7] * regw\n-            qq4 = image_obj[\'total_boxes\'][:, 3] + image_obj[\'total_boxes\'][:, 8] * regh\n-            image_obj[\'total_boxes\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\'total_boxes\'][:, 4]]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n-            image_obj[\'total_boxes\'][:, 0:4] = np.fix(image_obj[\'total_boxes\'][:, 0:4]).astype(np.int32)\n-            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n-\n-            numbox = image_obj[\'total_boxes\'].shape[0]\n+            pick = nms(image_obj["total_boxes"].copy(), 0.7, "Union")\n+            image_obj["total_boxes"] = image_obj["total_boxes"][pick, :]\n+            regw = image_obj["total_boxes"][:, 2] - image_obj["total_boxes"][:, 0]\n+            regh = image_obj["total_boxes"][:, 3] - image_obj["total_boxes"][:, 1]\n+            qq1 = image_obj["total_boxes"][:, 0] + image_obj["total_boxes"][:, 5] * regw\n+            qq2 = image_obj["total_boxes"][:, 1] + image_obj["total_boxes"][:, 6] * regh\n+            qq3 = image_obj["total_boxes"][:, 2] + image_obj["total_boxes"][:, 7] * regw\n+            qq4 = image_obj["total_boxes"][:, 3] + image_obj["total_boxes"][:, 8] * regh\n+            image_obj["total_boxes"] = np.transpose(\n+                np.vstack([qq1, qq2, qq3, qq4, image_obj["total_boxes"][:, 4]])\n+            )\n+            image_obj["total_boxes"] = rerec(image_obj["total_boxes"].copy())\n+            image_obj["total_boxes"][:, 0:4] = np.fix(\n+                image_obj["total_boxes"][:, 0:4]\n+            ).astype(np.int32)\n+            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(\n+                image_obj["total_boxes"].copy(), w, h\n+            )\n+\n+            numbox = image_obj["total_boxes"].shape[0]\n             tempimg = np.zeros((24, 24, 3, numbox))\n \n             if numbox > 0:\n                 for k in range(0, numbox):\n                     tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n+                    tmp[dy[k] - 1 : edy[k], dx[k] - 1 : edx[k], :] = images[index][\n+                        y[k] - 1 : ey[k], x[k] - 1 : ex[k], :\n+                    ]\n+                    if (\n+                        tmp.shape[0] > 0\n+                        and tmp.shape[1] > 0\n+                        or tmp.shape[0] == 0\n+                        and tmp.shape[1] == 0\n+                    ):\n                         tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n                     else:\n                         return np.empty()\n \n                 tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'rnet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n+                image_obj["rnet_input"] = np.transpose(tempimg, (3, 1, 0, 2))\n \n     # # # # # # # # # # # # #\n     # second stage - refinement of face candidates with rnet\n@@ -533,8 +618,10 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n \n     bulk_rnet_input = np.empty((0, 24, 24, 3))\n     for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' in image_obj:\n-            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\'rnet_input\'], axis=0)\n+        if "rnet_input" in image_obj:\n+            bulk_rnet_input = np.append(\n+                bulk_rnet_input, image_obj["rnet_input"], axis=0\n+            )\n \n     out = rnet(bulk_rnet_input)\n     out0 = np.transpose(out[0])\n@@ -543,43 +630,60 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n \n     i = 0\n     for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' not in image_obj:\n+        if "rnet_input" not in image_obj:\n             continue\n \n-        rnet_input_count = image_obj[\'rnet_input\'].shape[0]\n-        score_per_image = score[i:i + rnet_input_count]\n-        out0_per_image = out0[:, i:i + rnet_input_count]\n+        rnet_input_count = image_obj["rnet_input"].shape[0]\n+        score_per_image = score[i : i + rnet_input_count]\n+        out0_per_image = out0[:, i : i + rnet_input_count]\n \n         ipass = np.where(score_per_image > threshold[1])\n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n+        image_obj["total_boxes"] = np.hstack(\n+            [\n+                image_obj["total_boxes"][ipass[0], 0:4].copy(),\n+                np.expand_dims(score_per_image[ipass].copy(), 1),\n+            ]\n+        )\n \n         mv = out0_per_image[:, ipass[0]]\n \n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n+        if image_obj["total_boxes"].shape[0] > 0:\n             h = images[index].shape[0]\n             w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'], 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv[:, pick]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n+            pick = nms(image_obj["total_boxes"], 0.7, "Union")\n+            image_obj["total_boxes"] = image_obj["total_boxes"][pick, :]\n+            image_obj["total_boxes"] = bbreg(\n+                image_obj["total_boxes"].copy(), np.transpose(mv[:, pick])\n+            )\n+            image_obj["total_boxes"] = rerec(image_obj["total_boxes"].copy())\n \n-            numbox = image_obj[\'total_boxes\'].shape[0]\n+            numbox = image_obj["total_boxes"].shape[0]\n \n             if numbox > 0:\n                 tempimg = np.zeros((48, 48, 3, numbox))\n-                image_obj[\'total_boxes\'] = np.fix(image_obj[\'total_boxes\']).astype(np.int32)\n-                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n+                image_obj["total_boxes"] = np.fix(image_obj["total_boxes"]).astype(\n+                    np.int32\n+                )\n+                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(\n+                    image_obj["total_boxes"].copy(), w, h\n+                )\n \n                 for k in range(0, numbox):\n                     tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n+                    tmp[dy[k] - 1 : edy[k], dx[k] - 1 : edx[k], :] = images[index][\n+                        y[k] - 1 : ey[k], x[k] - 1 : ex[k], :\n+                    ]\n+                    if (\n+                        tmp.shape[0] > 0\n+                        and tmp.shape[1] > 0\n+                        or tmp.shape[0] == 0\n+                        and tmp.shape[1] == 0\n+                    ):\n                         tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n                     else:\n                         return np.empty()\n                 tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'onet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n+                image_obj["onet_input"] = np.transpose(tempimg, (3, 1, 0, 2))\n \n         i += rnet_input_count\n \n@@ -589,8 +693,10 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n \n     bulk_onet_input = np.empty((0, 48, 48, 3))\n     for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' in image_obj:\n-            bulk_onet_input = np.append(bulk_onet_input, image_obj[\'onet_input\'], axis=0)\n+        if "onet_input" in image_obj:\n+            bulk_onet_input = np.append(\n+                bulk_onet_input, image_obj["onet_input"], axis=0\n+            )\n \n     out = onet(bulk_onet_input)\n \n@@ -603,37 +709,49 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n     i = 0\n     ret = []\n     for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' not in image_obj:\n+        if "onet_input" not in image_obj:\n             ret.append(None)\n             continue\n \n-        onet_input_count = image_obj[\'onet_input\'].shape[0]\n+        onet_input_count = image_obj["onet_input"].shape[0]\n \n-        out0_per_image = out0[:, i:i + onet_input_count]\n-        score_per_image = score[i:i + onet_input_count]\n-        points_per_image = points[:, i:i + onet_input_count]\n+        out0_per_image = out0[:, i : i + onet_input_count]\n+        score_per_image = score[i : i + onet_input_count]\n+        points_per_image = points[:, i : i + onet_input_count]\n \n         ipass = np.where(score_per_image > threshold[2])\n         points_per_image = points_per_image[:, ipass[0]]\n \n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n+        image_obj["total_boxes"] = np.hstack(\n+            [\n+                image_obj["total_boxes"][ipass[0], 0:4].copy(),\n+                np.expand_dims(score_per_image[ipass].copy(), 1),\n+            ]\n+        )\n         mv = out0_per_image[:, ipass[0]]\n \n-        w = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0] + 1\n-        h = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1] + 1\n-        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 0], (5, 1)) - 1\n-        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 1], (5, 1)) - 1\n-\n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv))\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Min\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n+        w = image_obj["total_boxes"][:, 2] - image_obj["total_boxes"][:, 0] + 1\n+        h = image_obj["total_boxes"][:, 3] - image_obj["total_boxes"][:, 1] + 1\n+        points_per_image[0:5, :] = (\n+            np.tile(w, (5, 1)) * points_per_image[0:5, :]\n+            + np.tile(image_obj["total_boxes"][:, 0], (5, 1))\n+            - 1\n+        )\n+        points_per_image[5:10, :] = (\n+            np.tile(h, (5, 1)) * points_per_image[5:10, :]\n+            + np.tile(image_obj["total_boxes"][:, 1], (5, 1))\n+            - 1\n+        )\n+\n+        if image_obj["total_boxes"].shape[0] > 0:\n+            image_obj["total_boxes"] = bbreg(\n+                image_obj["total_boxes"].copy(), np.transpose(mv)\n+            )\n+            pick = nms(image_obj["total_boxes"].copy(), 0.7, "Min")\n+            image_obj["total_boxes"] = image_obj["total_boxes"][pick, :]\n             points_per_image = points_per_image[:, pick]\n \n-            ret.append((image_obj[\'total_boxes\'], points_per_image))\n+            ret.append((image_obj["total_boxes"], points_per_image))\n         else:\n             ret.append(None)\n \n@@ -643,60 +761,62 @@ def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, thre\n \n \n # function [boundingbox] = bbreg(boundingbox,reg)\n-def bbreg(boundingbox,reg):\n+def bbreg(boundingbox, reg):\n     """Calibrate bounding boxes"""\n-    if reg.shape[1]==1:\n+    if reg.shape[1] == 1:\n         reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n \n-    w = boundingbox[:,2]-boundingbox[:,0]+1\n-    h = boundingbox[:,3]-boundingbox[:,1]+1\n-    b1 = boundingbox[:,0]+reg[:,0]*w\n-    b2 = boundingbox[:,1]+reg[:,1]*h\n-    b3 = boundingbox[:,2]+reg[:,2]*w\n-    b4 = boundingbox[:,3]+reg[:,3]*h\n-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n+    w = boundingbox[:, 2] - boundingbox[:, 0] + 1\n+    h = boundingbox[:, 3] - boundingbox[:, 1] + 1\n+    b1 = boundingbox[:, 0] + reg[:, 0] * w\n+    b2 = boundingbox[:, 1] + reg[:, 1] * h\n+    b3 = boundingbox[:, 2] + reg[:, 2] * w\n+    b4 = boundingbox[:, 3] + reg[:, 3] * h\n+    boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4]))\n     return boundingbox\n- \n+\n+\n def generateBoundingBox(imap, reg, scale, t):\n     """Use heatmap to generate bounding boxes"""\n-    stride=2\n-    cellsize=12\n+    stride = 2\n+    cellsize = 12\n \n     imap = np.transpose(imap)\n-    dx1 = np.transpose(reg[:,:,0])\n-    dy1 = np.transpose(reg[:,:,1])\n-    dx2 = np.transpose(reg[:,:,2])\n-    dy2 = np.transpose(reg[:,:,3])\n+    dx1 = np.transpose(reg[:, :, 0])\n+    dy1 = np.transpose(reg[:, :, 1])\n+    dx2 = np.transpose(reg[:, :, 2])\n+    dy2 = np.transpose(reg[:, :, 3])\n     y, x = np.where(imap >= t)\n-    if y.shape[0]==1:\n+    if y.shape[0] == 1:\n         dx1 = np.flipud(dx1)\n         dy1 = np.flipud(dy1)\n         dx2 = np.flipud(dx2)\n         dy2 = np.flipud(dy2)\n-    score = imap[(y,x)]\n-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n-    if reg.size==0:\n-        reg = np.empty((0,3))\n-    bb = np.transpose(np.vstack([y,x]))\n-    q1 = np.fix((stride*bb+1)/scale)\n-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n+    score = imap[(y, x)]\n+    reg = np.transpose(np.vstack([dx1[(y, x)], dy1[(y, x)], dx2[(y, x)], dy2[(y, x)]]))\n+    if reg.size == 0:\n+        reg = np.empty((0, 3))\n+    bb = np.transpose(np.vstack([y, x]))\n+    q1 = np.fix((stride * bb + 1) / scale)\n+    q2 = np.fix((stride * bb + cellsize - 1 + 1) / scale)\n+    boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg])\n     return boundingbox, reg\n- \n+\n+\n # function pick = nms(boxes,threshold,type)\n def nms(boxes, threshold, method):\n-    if boxes.size==0:\n-        return np.empty((0,3))\n-    x1 = boxes[:,0]\n-    y1 = boxes[:,1]\n-    x2 = boxes[:,2]\n-    y2 = boxes[:,3]\n-    s = boxes[:,4]\n-    area = (x2-x1+1) * (y2-y1+1)\n+    if boxes.size == 0:\n+        return np.empty((0, 3))\n+    x1 = boxes[:, 0]\n+    y1 = boxes[:, 1]\n+    x2 = boxes[:, 2]\n+    y2 = boxes[:, 3]\n+    s = boxes[:, 4]\n+    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n     I = np.argsort(s)\n     pick = np.zeros_like(s, dtype=np.int16)\n     counter = 0\n-    while I.size>0:\n+    while I.size > 0:\n         i = I[-1]\n         pick[counter] = i\n         counter += 1\n@@ -705,22 +825,23 @@ def nms(boxes, threshold, method):\n         yy1 = np.maximum(y1[i], y1[idx])\n         xx2 = np.minimum(x2[i], x2[idx])\n         yy2 = np.minimum(y2[i], y2[idx])\n-        w = np.maximum(0.0, xx2-xx1+1)\n-        h = np.maximum(0.0, yy2-yy1+1)\n+        w = np.maximum(0.0, xx2 - xx1 + 1)\n+        h = np.maximum(0.0, yy2 - yy1 + 1)\n         inter = w * h\n-        if method is \'Min\':\n+        if method is "Min":\n             o = inter / np.minimum(area[i], area[idx])\n         else:\n             o = inter / (area[i] + area[idx] - inter)\n-        I = I[np.where(o<=threshold)]\n+        I = I[np.where(o <= threshold)]\n     pick = pick[0:counter]\n     return pick\n \n+\n # function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\n def pad(total_boxes, w, h):\n     """Compute the padding coordinates (pad the bounding boxes to square)"""\n-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n+    tmpw = (total_boxes[:, 2] - total_boxes[:, 0] + 1).astype(np.int32)\n+    tmph = (total_boxes[:, 3] - total_boxes[:, 1] + 1).astype(np.int32)\n     numbox = total_boxes.shape[0]\n \n     dx = np.ones((numbox), dtype=np.int32)\n@@ -728,45 +849,51 @@ def pad(total_boxes, w, h):\n     edx = tmpw.copy().astype(np.int32)\n     edy = tmph.copy().astype(np.int32)\n \n-    x = total_boxes[:,0].copy().astype(np.int32)\n-    y = total_boxes[:,1].copy().astype(np.int32)\n-    ex = total_boxes[:,2].copy().astype(np.int32)\n-    ey = total_boxes[:,3].copy().astype(np.int32)\n+    x = total_boxes[:, 0].copy().astype(np.int32)\n+    y = total_boxes[:, 1].copy().astype(np.int32)\n+    ex = total_boxes[:, 2].copy().astype(np.int32)\n+    ey = total_boxes[:, 3].copy().astype(np.int32)\n \n-    tmp = np.where(ex>w)\n-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n+    tmp = np.where(ex > w)\n+    edx.flat[tmp] = np.expand_dims(-ex[tmp] + w + tmpw[tmp], 1)\n     ex[tmp] = w\n-    \n-    tmp = np.where(ey>h)\n-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n+\n+    tmp = np.where(ey > h)\n+    edy.flat[tmp] = np.expand_dims(-ey[tmp] + h + tmph[tmp], 1)\n     ey[tmp] = h\n \n-    tmp = np.where(x<1)\n-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n+    tmp = np.where(x < 1)\n+    dx.flat[tmp] = np.expand_dims(2 - x[tmp], 1)\n     x[tmp] = 1\n \n-    tmp = np.where(y<1)\n-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n+    tmp = np.where(y < 1)\n+    dy.flat[tmp] = np.expand_dims(2 - y[tmp], 1)\n     y[tmp] = 1\n-    \n+\n     return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n \n+\n # function [bboxA] = rerec(bboxA)\n def rerec(bboxA):\n     """Convert bboxA to square."""\n-    h = bboxA[:,3]-bboxA[:,1]\n-    w = bboxA[:,2]-bboxA[:,0]\n+    h = bboxA[:, 3] - bboxA[:, 1]\n+    w = bboxA[:, 2] - bboxA[:, 0]\n     l = np.maximum(w, h)\n-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n+    bboxA[:, 0] = bboxA[:, 0] + w * 0.5 - l * 0.5\n+    bboxA[:, 1] = bboxA[:, 1] + h * 0.5 - l * 0.5\n+    bboxA[:, 2:4] = bboxA[:, 0:2] + np.transpose(np.tile(l, (2, 1)))\n     return bboxA\n \n+\n def imresample(img, sz):\n-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n+    im_data = cv2.resize(\n+        img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA\n+    )  # @UndefinedVariable\n     return im_data\n \n     # This method is kept for debugging purpose\n+\n+\n #     h=img.shape[0]\n #     w=img.shape[1]\n #     hs, ws = sz\n@@ -778,4 +905,3 @@ def imresample(img, sz):\n #             for a3 in range(0,3):\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n #     return im_data\n-\ndiff --git a/model/src/align_dataset_mtcnn.py b/model/src/align_dataset_mtcnn.py\nindex 64b85fe..ff7de49 100644\n--- a/model/src/align_dataset_mtcnn.py\n+++ b/model/src/align_dataset_mtcnn.py\n@@ -1,18 +1,18 @@\n """Performs face alignment and stores face thumbnails in the output directory."""\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -21,48 +21,56 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from scipy import misc\n-import sys\n import os\n-import tensorflow as tf\n-import numpy as np\n-from model.src.align import detect_face \n import random\n+import sys\n from time import sleep\n \n-from model.src import facenet_config as facenet\n+import numpy as np\n+import tensorflow as tf\n+from scipy import misc\n \n+from model.src import facenet_config as facenet\n+from model.src.align import detect_face\n \n \n-def face_alignment(input_dir, output_dir, image_size=182, margin=44, random_order=False, gpu_memory_fraction=1.0, detect_multiple_faces=False):\n+def face_alignment(\n+    input_dir,\n+    output_dir,\n+    image_size=182,\n+    margin=44,\n+    random_order=False,\n+    gpu_memory_fraction=1.0,\n+    detect_multiple_faces=False,\n+):\n     sleep(random.random())\n     output_dir = os.path.expanduser(output_dir)\n     if not os.path.exists(output_dir):\n         os.makedirs(output_dir)\n     # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n+    src_path, _ = os.path.split(os.path.realpath(__file__))\n+    facenet.store_revision_info(src_path, output_dir, " ".join(sys.argv))\n     dataset = facenet.get_dataset(input_dir)\n-    \n-    print(\'Creating networks and loading parameters\')\n-    \n+\n+    print("Creating networks and loading parameters")\n+\n     with tf.Graph().as_default():\n         sess = tf.compat.v1.Session()\n         with sess.as_default():\n             pnet, rnet, onet = detect_face.create_mtcnn(sess, None)\n-    \n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\' threshold\n-    factor = 0.709 # scale factor\n+\n+    minsize = 20  # minimum size of face\n+    threshold = [0.6, 0.7, 0.7]  # three steps\' threshold\n+    factor = 0.709  # scale factor\n \n     # Add a random key to the filename to allow alignment using multiple processes\n     random_key = np.random.randint(0, high=99999)\n-    bounding_boxes_filename = os.path.join(output_dir, \'bounding_boxes_%05d.txt\' % random_key)\n-    \n+    bounding_boxes_filename = os.path.join(\n+        output_dir, "bounding_boxes_%05d.txt" % random_key\n+    )\n+\n     with open(bounding_boxes_filename, "w") as text_file:\n         nrof_images_total = 0\n         nrof_successfully_aligned = 0\n@@ -77,28 +85,31 @@ def face_alignment(input_dir, output_dir, image_size=182, margin=44, random_orde\n             for image_path in cls.image_paths:\n                 nrof_images_total += 1\n                 filename = os.path.splitext(os.path.split(image_path)[1])[0]\n-                output_filename = os.path.join(output_class_dir, filename+\'.png\')\n+                output_filename = os.path.join(output_class_dir, filename + ".png")\n                 print(image_path)\n                 if not os.path.exists(output_filename):\n                     try:\n                         import imageio\n+\n                         img = imageio.imread(image_path)\n                     except (IOError, ValueError, IndexError) as e:\n-                        errorMessage = \'{}: {}\'.format(image_path, e)\n+                        errorMessage = "{}: {}".format(image_path, e)\n                         print(errorMessage)\n                     else:\n                         if img.ndim < 2:\n                             print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n+                            text_file.write("%s\\n" % (output_filename))\n                             continue\n                         if img.ndim == 2:\n                             img = facenet.to_rgb(img)\n-                        img = img[:,:,0:3]\n-    \n-                        bounding_boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n+                        img = img[:, :, 0:3]\n+\n+                        bounding_boxes, _ = detect_face.detect_face(\n+                            img, minsize, pnet, rnet, onet, threshold, factor\n+                        )\n                         nrof_faces = bounding_boxes.shape[0]\n                         if nrof_faces > 0:\n-                            det = bounding_boxes[:,0:4]\n+                            det = bounding_boxes[:, 0:4]\n                             det_arr = []\n                             img_size = np.asarray(img.shape)[0:2]\n                             if nrof_faces > 1:\n@@ -106,45 +117,78 @@ def face_alignment(input_dir, output_dir, image_size=182, margin=44, random_orde\n                                     for i in range(nrof_faces):\n                                         det_arr.append(np.squeeze(det[i]))\n                                 else:\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n+                                    bounding_box_size = (det[:, 2] - det[:, 0]) * (\n+                                        det[:, 3] - det[:, 1]\n+                                    )\n                                     img_center = img_size / 2\n-                                    offsets = np.vstack([(det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0]])\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n-                                    det_arr.append(det[index,:])\n+                                    offsets = np.vstack(\n+                                        [\n+                                            (det[:, 0] + det[:, 2]) / 2 - img_center[1],\n+                                            (det[:, 1] + det[:, 3]) / 2 - img_center[0],\n+                                        ]\n+                                    )\n+                                    offset_dist_squared = np.sum(\n+                                        np.power(offsets, 2.0), 0\n+                                    )\n+                                    index = np.argmax(\n+                                        bounding_box_size - offset_dist_squared * 2.0\n+                                    )  # some extra weight on the centering\n+                                    det_arr.append(det[index, :])\n                             else:\n                                 det_arr.append(np.squeeze(det))\n \n                             for i, det in enumerate(det_arr):\n                                 det = np.squeeze(det)\n                                 bb = np.zeros(4, dtype=np.int32)\n-                                bb[0] = np.maximum(det[0]-margin/2, 0)\n-                                bb[1] = np.maximum(det[1]-margin/2, 0)\n-                                bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n-                                bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n+                                bb[0] = np.maximum(det[0] - margin / 2, 0)\n+                                bb[1] = np.maximum(det[1] - margin / 2, 0)\n+                                bb[2] = np.minimum(det[2] + margin / 2, img_size[1])\n+                                bb[3] = np.minimum(det[3] + margin / 2, img_size[0])\n+                                cropped = img[bb[1] : bb[3], bb[0] : bb[2], :]\n                                 from PIL import Image\n+\n                                 cropped = Image.fromarray(cropped)\n-                                scaled = cropped.resize((image_size, image_size), Image.BILINEAR)\n+                                scaled = cropped.resize(\n+                                    (image_size, image_size), Image.BILINEAR\n+                                )\n                                 nrof_successfully_aligned += 1\n-                                filename_base, file_extension = os.path.splitext(output_filename)\n+                                filename_base, file_extension = os.path.splitext(\n+                                    output_filename\n+                                )\n                                 if detect_multiple_faces:\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\n+                                    output_filename_n = "{}_{}{}".format(\n+                                        filename_base, i, file_extension\n+                                    )\n                                 else:\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\n+                                    output_filename_n = "{}{}".format(\n+                                        filename_base, file_extension\n+                                    )\n                                 imageio.imwrite(output_filename_n, scaled)\n-                                text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n+                                text_file.write(\n+                                    "%s %d %d %d %d\\n"\n+                                    % (output_filename_n, bb[0], bb[1], bb[2], bb[3])\n+                                )\n                         else:\n                             print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            \n-    print(\'Total number of images: %d\' % nrof_images_total)\n-    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n+                            text_file.write("%s\\n" % (output_filename))\n+\n+    print("Total number of images: %d" % nrof_images_total)\n+    print("Number of successfully aligned images: %d" % nrof_successfully_aligned)\n+\n \n def preprocess():\n-    input_dir = \'model/Dataset/FaceData/raw\'\n-    output_dir = \'model/Dataset/FaceData/processed\'\n-    face_alignment(input_dir, output_dir, image_size=160, margin=32, random_order=True, gpu_memory_fraction=0.25, detect_multiple_faces=False)\n+    input_dir = "model/Dataset/FaceData/raw"\n+    output_dir = "model/Dataset/FaceData/processed"\n+    face_alignment(\n+        input_dir,\n+        output_dir,\n+        image_size=160,\n+        margin=32,\n+        random_order=True,\n+        gpu_memory_fraction=0.25,\n+        detect_multiple_faces=False,\n+    )\n+\n \n-if __name__ == \'__main__\':\n-    preprocess()\n\\ No newline at end of file\n+if __name__ == "__main__":\n+    preprocess()\ndiff --git a/model/src/calculate_filtering_metrics.py b/model/src/calculate_filtering_metrics.py\nindex b6bc79e..9a5f786 100644\n--- a/model/src/calculate_filtering_metrics.py\n+++ b/model/src/calculate_filtering_metrics.py\n@@ -1,19 +1,19 @@\n """Calculate filtering metrics for a dataset and store in a .hdf file.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -22,59 +22,68 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-import numpy as np\n import argparse\n-from model.src import facenet_config as facenet\n-\n+import math\n import os\n import sys\n import time\n+\n import h5py\n-import math\n-from tensorflow.python.platform import gfile\n+import numpy as np\n+import tensorflow as tf\n from six import iteritems\n+from tensorflow.python.platform import gfile\n+\n+from model.src import facenet_config as facenet\n+\n \n def main(args):\n     dataset = facenet.get_dataset(args.dataset_dir)\n-  \n+\n     with tf.Graph().as_default():\n-      \n+\n         # Get a list of image paths and their labels\n         image_list, label_list = facenet.get_image_paths_and_labels(dataset)\n         nrof_images = len(image_list)\n         image_indices = range(nrof_images)\n \n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\n-            image_indices, args.image_size, args.batch_size, None, \n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\n-        \n+        image_batch, label_batch = facenet.read_and_augment_data(\n+            image_list,\n+            image_indices,\n+            args.image_size,\n+            args.batch_size,\n+            None,\n+            False,\n+            False,\n+            False,\n+            nrof_preprocess_threads=4,\n+            shuffle=False,\n+        )\n+\n         model_exp = os.path.expanduser(args.model_file)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n+        with gfile.FastGFile(model_exp, "rb") as f:\n             graph_def = tf.GraphDef()\n             graph_def.ParseFromString(f.read())\n-            input_map={\'input\':image_batch, \'phase_train\':False}\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'net\')\n-        \n+            input_map = {"input": image_batch, "phase_train": False}\n+            tf.import_graph_def(graph_def, input_map=input_map, name="net")\n+\n         embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\n \n         with tf.Session() as sess:\n             tf.train.start_queue_runners(sess=sess)\n-                \n+\n             embedding_size = int(embeddings.get_shape()[1])\n             nrof_batches = int(math.ceil(nrof_images / args.batch_size))\n             nrof_classes = len(dataset)\n             label_array = np.array(label_list)\n             class_names = [cls.name for cls in dataset]\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\n+            nrof_examples_per_class = [len(cls.image_paths) for cls in dataset]\n             class_variance = np.zeros((nrof_classes,))\n-            class_center = np.zeros((nrof_classes,embedding_size))\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\n-            emb_array = np.zeros((0,embedding_size))\n+            class_center = np.zeros((nrof_classes, embedding_size))\n+            distance_to_center = np.ones((len(label_list),)) * np.NaN\n+            emb_array = np.zeros((0, embedding_size))\n             idx_array = np.zeros((0,), dtype=np.int32)\n             lab_array = np.zeros((0,), dtype=np.int32)\n             index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\n@@ -85,45 +94,65 @@ def main(args):\n                 idx_array = np.append(idx_array, idx, axis=0)\n                 lab_array = np.append(lab_array, label_array[idx], axis=0)\n                 for cls in set(lab_array):\n-                    cls_idx = np.where(lab_array==cls)[0]\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\n+                    cls_idx = np.where(lab_array == cls)[0]\n+                    if cls_idx.shape[0] == nrof_examples_per_class[cls]:\n                         # We have calculated all the embeddings for this class\n                         i2 = np.argsort(idx_array[cls_idx])\n-                        emb_class = emb_array[cls_idx,:]\n-                        emb_sort = emb_class[i2,:]\n+                        emb_class = emb_array[cls_idx, :]\n+                        emb_sort = emb_class[i2, :]\n                         center = np.mean(emb_sort, axis=0)\n                         diffs = emb_sort - center\n                         dists_sqr = np.sum(np.square(diffs), axis=1)\n                         class_variance[cls] = np.mean(dists_sqr)\n-                        class_center[cls,:] = center\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\n+                        class_center[cls, :] = center\n+                        distance_to_center[\n+                            index_arr[cls] : index_arr[cls + 1]\n+                        ] = np.sqrt(dists_sqr)\n                         emb_array = np.delete(emb_array, cls_idx, axis=0)\n                         idx_array = np.delete(idx_array, cls_idx, axis=0)\n                         lab_array = np.delete(lab_array, cls_idx, axis=0)\n \n-                        \n-                print(\'Batch %d in %.3f seconds\' % (i, time.time()-t))\n-                \n-            print(\'Writing filtering data to %s\' % args.data_file_name)\n-            mdict = {\'class_names\':class_names, \'image_list\':image_list, \'label_list\':label_list, \'distance_to_center\':distance_to_center }\n-            with h5py.File(args.data_file_name, \'w\') as f:\n+                print("Batch %d in %.3f seconds" % (i, time.time() - t))\n+\n+            print("Writing filtering data to %s" % args.data_file_name)\n+            mdict = {\n+                "class_names": class_names,\n+                "image_list": image_list,\n+                "label_list": label_list,\n+                "distance_to_center": distance_to_center,\n+            }\n+            with h5py.File(args.data_file_name, "w") as f:\n                 for key, value in iteritems(mdict):\n                     f.create_dataset(key, data=value)\n-                        \n+\n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'dataset_dir\', type=str,\n-        help=\'Path to the directory containing aligned dataset.\')\n-    parser.add_argument(\'model_file\', type=str,\n-        help=\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\')\n-    parser.add_argument(\'data_file_name\', type=str,\n-        help=\'The name of the file to store filtering data in.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size.\', default=160)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n+\n+    parser.add_argument(\n+        "dataset_dir",\n+        type=str,\n+        help="Path to the directory containing aligned dataset.",\n+    )\n+    parser.add_argument(\n+        "model_file",\n+        type=str,\n+        help="File containing the frozen model in protobuf (.pb) format to use for feature extraction.",\n+    )\n+    parser.add_argument(\n+        "data_file_name",\n+        type=str,\n+        help="The name of the file to store filtering data in.",\n+    )\n+    parser.add_argument("--image_size", type=int, help="Image size.", default=160)\n+    parser.add_argument(\n+        "--batch_size",\n+        type=int,\n+        help="Number of images to process in a batch.",\n+        default=90,\n+    )\n     return parser.parse_args(argv)\n \n-if __name__ == \'__main__\':\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/classifier.py b/model/src/classifier.py\nindex b5ec195..47f2a08 100644\n--- a/model/src/classifier.py\n+++ b/model/src/classifier.py\n@@ -1,48 +1,72 @@\n-import tensorflow as tf\n-import numpy as np\n-from model.src import facenet_config as facenet\n-\n-import os\n import math\n+import os\n import pickle\n+\n+import numpy as np\n+import tensorflow as tf\n from sklearn.svm import SVC\n \n-def train_or_classify(mode, data_dir, model, classifier_filename, use_split_dataset=False, test_data_dir=None, batch_size=90, image_size=160, seed=666, min_nrof_images_per_class=20, nrof_train_images_per_class=10):\n+from model.src import facenet_config as facenet\n+\n+\n+def train_or_classify(\n+    mode,\n+    data_dir,\n+    model,\n+    classifier_filename,\n+    use_split_dataset=False,\n+    test_data_dir=None,\n+    batch_size=90,\n+    image_size=160,\n+    seed=666,\n+    min_nrof_images_per_class=20,\n+    nrof_train_images_per_class=10,\n+):\n     with tf.Graph().as_default():\n         with tf.compat.v1.Session() as sess:\n             np.random.seed(seed=seed)\n \n             if use_split_dataset:\n                 dataset_tmp = facenet.get_dataset(data_dir)\n-                train_set, test_set = split_dataset(dataset_tmp, min_nrof_images_per_class, nrof_train_images_per_class)\n-                if mode == \'TRAIN\':\n+                train_set, test_set = split_dataset(\n+                    dataset_tmp, min_nrof_images_per_class, nrof_train_images_per_class\n+                )\n+                if mode == "TRAIN":\n                     dataset = train_set\n-                elif mode == \'CLASSIFY\':\n+                elif mode == "CLASSIFY":\n                     dataset = test_set\n             else:\n                 dataset = facenet.get_dataset(data_dir)\n \n             # Check that there are at least one training image per class\n             for cls in dataset:\n-                assert len(cls.image_paths) > 0, \'There must be at least one image for each class in the dataset\'\n+                assert (\n+                    len(cls.image_paths) > 0\n+                ), "There must be at least one image for each class in the dataset"\n \n             paths, labels = facenet.get_image_paths_and_labels(dataset)\n \n-            print(\'Number of classes: %d\' % len(dataset))\n-            print(\'Number of images: %d\' % len(paths))\n+            print("Number of classes: %d" % len(dataset))\n+            print("Number of images: %d" % len(paths))\n \n             # Load the model\n-            print(\'Loading feature extraction model\')\n+            print("Loading feature extraction model")\n             facenet.load_model(model)\n \n             # Get input and output tensors\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "input:0"\n+            )\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "embeddings:0"\n+            )\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\n+                "phase_train:0"\n+            )\n             embedding_size = embeddings.get_shape()[1]\n \n             # Run forward pass to calculate embeddings\n-            print(\'Calculating features for images\')\n+            print("Calculating features for images")\n             nrof_images = len(paths)\n             nrof_batches_per_epoch = int(math.ceil(1.0 * nrof_images / batch_size))\n             emb_array = np.zeros((nrof_images, embedding_size))\n@@ -52,41 +76,54 @@ def train_or_classify(mode, data_dir, model, classifier_filename, use_split_data\n                 paths_batch = paths[start_index:end_index]\n                 images = facenet.load_data(paths_batch, False, False, image_size)\n                 feed_dict = {images_placeholder: images, phase_train_placeholder: False}\n-                emb_array[start_index:end_index, :] = sess.run(embeddings, feed_dict=feed_dict)\n+                emb_array[start_index:end_index, :] = sess.run(\n+                    embeddings, feed_dict=feed_dict\n+                )\n \n             classifier_filename_exp = os.path.expanduser(classifier_filename)\n \n-            if mode == \'TRAIN\':\n+            if mode == "TRAIN":\n                 # Train classifier\n-                print(\'Training classifier\')\n-                model = SVC(kernel=\'linear\', probability=True)\n+                print("Training classifier")\n+                model = SVC(kernel="linear", probability=True)\n                 model.fit(emb_array, labels)\n \n                 # Create a list of class names\n-                class_names = [cls.name.replace(\'_\', \' \') for cls in dataset]\n+                class_names = [cls.name.replace("_", " ") for cls in dataset]\n \n                 # Saving classifier model\n-                with open(classifier_filename_exp, \'wb\') as outfile:\n+                with open(classifier_filename_exp, "wb") as outfile:\n                     pickle.dump((model, class_names), outfile)\n                 print(\'Saved classifier model to file "%s"\' % classifier_filename_exp)\n \n-            elif mode == \'CLASSIFY\':\n+            elif mode == "CLASSIFY":\n                 # Classify images\n-                print(\'Testing classifier\')\n-                with open(classifier_filename_exp, \'rb\') as infile:\n+                print("Testing classifier")\n+                with open(classifier_filename_exp, "rb") as infile:\n                     (model, class_names) = pickle.load(infile)\n \n-                print(\'Loaded classifier model from file "%s"\' % classifier_filename_exp)\n+                print(\n+                    \'Loaded classifier model from file "%s"\' % classifier_filename_exp\n+                )\n \n                 predictions = model.predict_proba(emb_array)\n                 best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n+                best_class_probabilities = predictions[\n+                    np.arange(len(best_class_indices)), best_class_indices\n+                ]\n \n                 for i in range(len(best_class_indices)):\n-                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n+                    print(\n+                        "%4d  %s: %.3f"\n+                        % (\n+                            i,\n+                            class_names[best_class_indices[i]],\n+                            best_class_probabilities[i],\n+                        )\n+                    )\n \n                 accuracy = np.mean(np.equal(best_class_indices, labels))\n-                print(\'Accuracy: %.3f\' % accuracy)\n+                print("Accuracy: %.3f" % accuracy)\n \n \n def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n@@ -97,22 +134,30 @@ def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_clas\n         # Remove classes with less than min_nrof_images_per_class\n         if len(paths) >= min_nrof_images_per_class:\n             np.random.shuffle(paths)\n-            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n-            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n+            train_set.append(\n+                facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class])\n+            )\n+            test_set.append(\n+                facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:])\n+            )\n     return train_set, test_set\n \n \n def train_classify():\n-    mode = \'TRAIN\'  # Set mode to \'TRAIN\' or \'CLASSIFY\' based on your requirements\n-    data_dir = \'model/Dataset/FaceData/processed\'  # Path to the data directory containing aligned LFW face patches\n-    model = \'model/Models/20180402-114759.pb\'  # Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\n-    classifier_filename = \'model/Models/facemodel.pkl\'  # Classifier model file name as a pickle (.pkl) file\n-    use_split_dataset = False  # Set to True if the dataset should be split into a training and test set\n+    mode = "TRAIN"  # Set mode to \'TRAIN\' or \'CLASSIFY\' based on your requirements\n+    data_dir = "model/Dataset/FaceData/processed"  # Path to the data directory containing aligned LFW face patches\n+    model = "model/pretrained/20180402-114759.pb"  # Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\n+    classifier_filename = "model/pretrained/facemodel.pkl"  # Classifier model file name as a pickle (.pkl) file\n+    use_split_dataset = (\n+        False  # Set to True if the dataset should be split into a training and test set\n+    )\n     test_data_dir = None  # Path to the test data directory (optional)\n     batch_size = 1000  # Number of images to process in a batch\n     image_size = 160  # Image size (height, width) in pixels\n     seed = 666  # Random seed\n-    min_nrof_images_per_class = 20  # Only include classes with at least this number of images in the dataset\n+    min_nrof_images_per_class = (\n+        20  # Only include classes with at least this number of images in the dataset\n+    )\n     nrof_train_images_per_class = 10  # Use this number of images from each class for training and the rest for testing\n \n     train_or_classify(\n@@ -126,9 +171,9 @@ def train_classify():\n         image_size=image_size,\n         seed=seed,\n         min_nrof_images_per_class=min_nrof_images_per_class,\n-        nrof_train_images_per_class=nrof_train_images_per_class\n+        nrof_train_images_per_class=nrof_train_images_per_class,\n     )\n \n \n-if __name__ == \'__main__\':\n+if __name__ == "__main__":\n     train_classify()\ndiff --git a/model/src/compare.py b/model/src/compare.py\nindex e90f7d9..506cd4d 100644\n--- a/model/src/compare.py\n+++ b/model/src/compare.py\n@@ -22,19 +22,18 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from scipy import misc\n-import tensorflow as tf\n-import numpy as np\n-import sys\n-import os\n-import copy\n import argparse\n-from model.src import facenet_config as facenet\n+import copy\n+import os\n+import sys\n \n+import numpy as np\n+import tensorflow as tf\n+from scipy import misc\n+\n+from model.src import facenet_config as facenet\n from model.src.align import detect_face as align.detect_face\n \n \ndiff --git a/model/src/decode_msceleb_dataset.py b/model/src/decode_msceleb_dataset.py\nindex 4588769..108eb4e 100644\n--- a/model/src/decode_msceleb_dataset.py\n+++ b/model/src/decode_msceleb_dataset.py\n@@ -2,19 +2,19 @@\n https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,20 +23,18 @@ https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizi\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from scipy import misc\n-import numpy as np\n+import argparse\n import base64\n-import sys\n import os\n-import cv2\n-import argparse\n-from model.src import facenet_config as facenet\n+import sys\n \n+import cv2\n+import numpy as np\n+from scipy import misc\n \n+from model.src import facenet_config as facenet\n \n # File format: text files, each line is an image record containing 6 columns, delimited by TAB.\n # Column1: Freebase MID\n@@ -46,43 +44,59 @@ from model.src import facenet_config as facenet\n # Column5: PageURL\n # Column6: ImageData_Base64Encoded\n \n+\n def main(args):\n     output_dir = os.path.expanduser(args.output_dir)\n-  \n+\n     if not os.path.exists(output_dir):\n         os.mkdir(output_dir)\n-  \n+\n     # Store some git revision info in a text file in the output directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    \n+    src_path, _ = os.path.split(os.path.realpath(__file__))\n+    facenet.store_revision_info(src_path, output_dir, " ".join(sys.argv))\n+\n     i = 0\n     for f in args.tsv_files:\n         for line in f:\n-            fields = line.split(\'\\t\')\n+            fields = line.split("\\t")\n             class_dir = fields[0]\n-            img_name = fields[1] + \'-\' + fields[4] + \'.\' + args.output_format\n+            img_name = fields[1] + "-" + fields[4] + "." + args.output_format\n             img_string = fields[5]\n             img_dec_string = base64.b64decode(img_string)\n             img_data = np.fromstring(img_dec_string, dtype=np.uint8)\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\n+            img = cv2.imdecode(\n+                img_data, cv2.IMREAD_COLOR\n+            )  # pylint: disable=maybe-no-member\n             if args.size:\n-                img = misc.imresize(img, (args.size, args.size), interp=\'bilinear\')\n+                img = misc.imresize(img, (args.size, args.size), interp="bilinear")\n             full_class_dir = os.path.join(output_dir, class_dir)\n             if not os.path.exists(full_class_dir):\n                 os.mkdir(full_class_dir)\n-            full_path = os.path.join(full_class_dir, img_name.replace(\'/\',\'_\'))\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\n-            print(\'%8d: %s\' % (i, full_path))\n+            full_path = os.path.join(full_class_dir, img_name.replace("/", "_"))\n+            cv2.imwrite(full_path, img)  # pylint: disable=maybe-no-member\n+            print("%8d: %s" % (i, full_path))\n             i += 1\n-  \n-if __name__ == \'__main__\':\n+\n+\n+if __name__ == "__main__":\n     parser = argparse.ArgumentParser()\n \n-    parser.add_argument(\'output_dir\', type=str, help=\'Output base directory for the image dataset\')\n-    parser.add_argument(\'tsv_files\', type=argparse.FileType(\'r\'), nargs=\'+\', help=\'Input TSV file name(s)\')\n-    parser.add_argument(\'--size\', type=int, help=\'Images are resized to the given size\')\n-    parser.add_argument(\'--output_format\', type=str, help=\'Format of the output images\', default=\'png\', choices=[\'png\', \'jpg\'])\n+    parser.add_argument(\n+        "output_dir", type=str, help="Output base directory for the image dataset"\n+    )\n+    parser.add_argument(\n+        "tsv_files",\n+        type=argparse.FileType("r"),\n+        nargs="+",\n+        help="Input TSV file name(s)",\n+    )\n+    parser.add_argument("--size", type=int, help="Images are resized to the given size")\n+    parser.add_argument(\n+        "--output_format",\n+        type=str,\n+        help="Format of the output images",\n+        default="png",\n+        choices=["png", "jpg"],\n+    )\n \n     main(parser.parse_args())\n-\ndiff --git a/model/src/download_and_extract.py b/model/src/download_and_extract.py\nindex a835ac2..950ea99 100644\n--- a/model/src/download_and_extract.py\n+++ b/model/src/download_and_extract.py\n@@ -1,51 +1,56 @@\n-import requests\n-import zipfile\n import os\n+import zipfile\n+\n+import requests\n \n model_dict = {\n-    \'lfw-subset\':      \'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\', \n-    \'20170131-234652\': \'0B5MzpY9kBtDVSGM0RmVET2EwVEk\',\n-    \'20170216-091149\': \'0B5MzpY9kBtDVTGZjcWkzT3pldDA\',\n-    \'20170512-110547\': \'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\',\n-    \'20180402-114759\': \'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\'\n-    }\n+    "lfw-subset": "1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi",\n+    "20170131-234652": "0B5MzpY9kBtDVSGM0RmVET2EwVEk",\n+    "20170216-091149": "0B5MzpY9kBtDVTGZjcWkzT3pldDA",\n+    "20170512-110547": "0B5MzpY9kBtDVZ2RpVDYwWmxoSUk",\n+    "20180402-114759": "1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-",\n+}\n+\n \n def download_and_extract_file(model_name, data_dir):\n     file_id = model_dict[model_name]\n-    destination = os.path.join(data_dir, model_name + \'.zip\')\n+    destination = os.path.join(data_dir, model_name + ".zip")\n     if not os.path.exists(destination):\n-        print(\'Downloading file to %s\' % destination)\n+        print("Downloading file to %s" % destination)\n         download_file_from_google_drive(file_id, destination)\n-        with zipfile.ZipFile(destination, \'r\') as zip_ref:\n-            print(\'Extracting file to %s\' % data_dir)\n+        with zipfile.ZipFile(destination, "r") as zip_ref:\n+            print("Extracting file to %s" % data_dir)\n             zip_ref.extractall(data_dir)\n \n+\n def download_file_from_google_drive(file_id, destination):\n-    \n-        URL = "https://drive.google.com/uc?export=download"\n-    \n-        session = requests.Session()\n-    \n-        response = session.get(URL, params = { \'id\' : file_id }, stream = True)\n-        token = get_confirm_token(response)\n-    \n-        if token:\n-            params = { \'id\' : file_id, \'confirm\' : token }\n-            response = session.get(URL, params = params, stream = True)\n-    \n-        save_response_content(response, destination)    \n+\n+    URL = "https://drive.google.com/uc?export=download"\n+\n+    session = requests.Session()\n+\n+    response = session.get(URL, params={"id": file_id}, stream=True)\n+    token = get_confirm_token(response)\n+\n+    if token:\n+        params = {"id": file_id, "confirm": token}\n+        response = session.get(URL, params=params, stream=True)\n+\n+    save_response_content(response, destination)\n+\n \n def get_confirm_token(response):\n     for key, value in response.cookies.items():\n-        if key.startswith(\'download_warning\'):\n+        if key.startswith("download_warning"):\n             return value\n \n     return None\n \n+\n def save_response_content(response, destination):\n     CHUNK_SIZE = 32768\n \n     with open(destination, "wb") as f:\n         for chunk in response.iter_content(CHUNK_SIZE):\n-            if chunk: # filter out keep-alive new chunks\n+            if chunk:  # filter out keep-alive new chunks\n                 f.write(chunk)\ndiff --git a/model/src/face_rec.py b/model/src/face_rec.py\nindex e155f35..1042411 100644\n--- a/model/src/face_rec.py\n+++ b/model/src/face_rec.py\n@@ -1,22 +1,20 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n import argparse\n-from model.src import facenet_config as facenet\n-\n-import os\n-import sys\n+import collections\n import math\n+import os\n import pickle\n-from model.src.align import detect_face as align.detect_face\n+import sys\n \n-import numpy as np\n import cv2\n-import collections\n+import numpy as np\n+import tensorflow as tf\n from sklearn.svm import SVC\n \n+from model.src import facenet_config as facenet\n+from model.src.align import detect_face as align.detect_face\n+\n \n def main():\n     parser = argparse.ArgumentParser()\n@@ -29,9 +27,9 @@ def main():\n     FACTOR = 0.709\n     IMAGE_SIZE = 182\n     INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n+    CLASSIFIER_PATH = \'pretrained/facemodel.pkl\'\n     VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n+    FACENET_MODEL_PATH = \'pretrained/20180402-114759.pb\'\n \n     # Load model da train de nhan dien khuon mat - thuc chat la classifier\n     with open(CLASSIFIER_PATH, \'rb\') as file:\ndiff --git a/model/src/face_rec_cam.py b/model/src/face_rec_cam.py\ndeleted file mode 100644\nindex 5359fd4..0000000\n--- a/model/src/face_rec_cam.py\n+++ /dev/null\n@@ -1,137 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-\n-from imutils.video import VideoStream\n-\n-\n-import argparse\n-from model.src import facenet_config as facenet\n-\n-import imutils\n-import os\n-import sys\n-import math\n-import pickle\n-from model.src.align import detect_face as align.detect_face\n-\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n-    args = parser.parse_args()\n-\n-    MINSIZE = 20\n-    THRESHOLD = [0.6, 0.7, 0.7]\n-    FACTOR = 0.709\n-    IMAGE_SIZE = 182\n-    INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n-    VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n-\n-    # Load The Custom Classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n-        model, class_names = pickle.load(file)\n-    print("Custom Classifier, Successfully loaded")\n-\n-    with tf.Graph().as_default():\n-\n-        # Cai dat GPU neu co\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-        with sess.as_default():\n-\n-            # Load the model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(FACENET_MODEL_PATH)\n-\n-            # Get input and output tensors\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n-\n-            people_detected = set()\n-            person_detected = collections.Counter()\n-\n-            cap  = VideoStream(src=0).start()\n-\n-            while (True):\n-                frame = cap.read()\n-                frame = imutils.resize(frame, width=600)\n-                frame = cv2.flip(frame, 1)\n-\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-                faces_found = bounding_boxes.shape[0]\n-                try:\n-                    if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n-                    elif faces_found > 0:\n-                        det = bounding_boxes[:, 0:4]\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\n-                        for i in range(faces_found):\n-                            bb[i][0] = det[i][0]\n-                            bb[i][1] = det[i][1]\n-                            bb[i][2] = det[i][2]\n-                            bb[i][3] = det[i][3]\n-                            print(bb[i][3]-bb[i][1])\n-                            print(frame.shape[0])\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                    interpolation=cv2.INTER_CUBIC)\n-                                scaled = facenet.prewhiten(scaled)\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-\n-                                predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(predictions, axis=1)\n-                                best_class_probabilities = predictions[\n-                                    np.arange(len(best_class_indices)), best_class_indices]\n-                                best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-\n-\n-                                if best_class_probabilities > 0.8:\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                                    text_x = bb[i][0]\n-                                    text_y = bb[i][3] + 20\n-\n-                                    name = class_names[best_class_indices[0]]\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    person_detected[best_name] += 1\n-                                else:\n-                                    name = "Unknown"\n-\n-                except:\n-                    pass\n-\n-                cv2.imshow(\'Face Recognition\', frame)\n-                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                    break\n-\n-            cap.release()\n-            cv2.destroyAllWindows()\n-\n-\n-main()\ndiff --git a/model/src/face_rec_flask.py b/model/src/face_rec_flask.py\nindex e9e77b1..bc6b03c 100644\n--- a/model/src/face_rec_flask.py\n+++ b/model/src/face_rec_flask.py\n@@ -1,33 +1,30 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from flask import Flask\n-from flask import render_template , request\n-from flask_cors import CORS, cross_origin\n-import tensorflow as tf\n import argparse\n-from model.src import facenet_config as facenet\n-\n-import os\n-import sys\n+import base64\n+import collections\n import math\n+import os\n import pickle\n-from model.src.align import detect_face as align.detect_face\n+import sys\n \n-import numpy as np\n import cv2\n-import collections\n+import numpy as np\n+import tensorflow as tf\n+from flask import Flask, render_template, request\n+from flask_cors import CORS, cross_origin\n from sklearn.svm import SVC\n-import base64\n+\n+from model.src import facenet_config as facenet\n+from model.src.align import detect_face as align.detect_face\n \n MINSIZE = 20\n THRESHOLD = [0.6, 0.7, 0.7]\n FACTOR = 0.709\n IMAGE_SIZE = 182\n INPUT_IMAGE_SIZE = 160\n-CLASSIFIER_PATH = \'../Models/facemodel.pkl\'\n-FACENET_MODEL_PATH = \'../Models/20180402-114759.pb\'\n+CLASSIFIER_PATH = \'../pretrained/facemodel.pkl\'\n+FACENET_MODEL_PATH = \'../pretrained/20180402-114759.pb\'\n \n # Load The Custom Classifier\n with open(CLASSIFIER_PATH, \'rb\') as file:\ndiff --git a/model/src/facenet_config.py b/model/src/facenet_config.py\nindex 26a4e3d..e1cb30f 100644\n--- a/model/src/facenet_config.py\n+++ b/model/src/facenet_config.py\n@@ -1,19 +1,19 @@\n """Functions for building the face recognition network.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.p\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,23 +23,22 @@\n # SOFTWARE.\n \n # pylint: disable=missing-docstring\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n+import math\n import os\n-from subprocess import Popen, PIPE\n-import tensorflow as tf\n-import numpy as np\n-from scipy import misc\n-from sklearn.model_selection import KFold\n-from scipy import interpolate\n-from tensorflow.python.training import training\n import random\n import re\n-from tensorflow.python.platform import gfile\n-import math\n+from subprocess import PIPE, Popen\n+\n+import numpy as np\n+import tensorflow as tf\n+from scipy import interpolate, misc\n from six import iteritems\n+from sklearn.model_selection import KFold\n+from tensorflow.python.platform import gfile\n+from tensorflow.python.training import training\n+\n \n def triplet_loss(anchor, positive, negative, alpha):\n     """Calculate the triplet loss according to the FaceNet paper\n@@ -52,22 +51,28 @@ def triplet_loss(anchor, positive, negative, alpha):\n     Returns:\n       the triplet loss according to the FaceNet paper as a float tensor.\n     """\n-    with tf.variable_scope(\'triplet_loss\'):\n+    with tf.variable_scope("triplet_loss"):\n         pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n         neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n+\n+        basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n         loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n+\n     return loss\n-  \n+\n+\n def center_loss(features, label, alfa, nrof_classes):\n     """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n        (http://ydwen.github.io/papers/WenECCV16.pdf)\n     """\n     nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n+    centers = tf.get_variable(\n+        "centers",\n+        [nrof_classes, nrof_features],\n+        dtype=tf.float32,\n+        initializer=tf.constant_initializer(0),\n+        trainable=False,\n+    )\n     label = tf.reshape(label, [-1])\n     centers_batch = tf.gather(centers, label)\n     diff = (1 - alfa) * (centers_batch - features)\n@@ -76,6 +81,7 @@ def center_loss(features, label, alfa, nrof_classes):\n         loss = tf.reduce_mean(tf.square(features - centers_batch))\n     return loss, centers\n \n+\n def get_image_paths_and_labels(dataset):\n     image_paths_flat = []\n     labels_flat = []\n@@ -84,23 +90,30 @@ def get_image_paths_and_labels(dataset):\n         labels_flat += [i] * len(dataset[i].image_paths)\n     return image_paths_flat, labels_flat\n \n+\n def shuffle_examples(image_paths, labels):\n     shuffle_list = list(zip(image_paths, labels))\n     random.shuffle(shuffle_list)\n     image_paths_shuff, labels_shuff = zip(*shuffle_list)\n     return image_paths_shuff, labels_shuff\n \n+\n def random_rotate_image(image):\n     angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n-  \n+    return misc.imrotate(image, angle, "bicubic")\n+\n+\n # 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n RANDOM_ROTATE = 1\n RANDOM_CROP = 2\n RANDOM_FLIP = 4\n FIXED_STANDARDIZATION = 8\n FLIP = 16\n-def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n+\n+\n+def create_input_pipeline(\n+    input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder\n+):\n     images_and_labels_list = []\n     for _ in range(nrof_preprocess_threads):\n         filenames, label, control = input_queue.dequeue()\n@@ -108,37 +121,54 @@ def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batc\n         for filename in tf.unstack(filenames):\n             file_contents = tf.read_file(filename)\n             image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            #pylint: disable=no-member\n+            image = tf.cond(\n+                get_control_flag(control[0], RANDOM_ROTATE),\n+                lambda: tf.py_func(random_rotate_image, [image], tf.uint8),\n+                lambda: tf.identity(image),\n+            )\n+            image = tf.cond(\n+                get_control_flag(control[0], RANDOM_CROP),\n+                lambda: tf.random_crop(image, image_size + (3,)),\n+                lambda: tf.image.resize_image_with_crop_or_pad(\n+                    image, image_size[0], image_size[1]\n+                ),\n+            )\n+            image = tf.cond(\n+                get_control_flag(control[0], RANDOM_FLIP),\n+                lambda: tf.image.random_flip_left_right(image),\n+                lambda: tf.identity(image),\n+            )\n+            image = tf.cond(\n+                get_control_flag(control[0], FIXED_STANDARDIZATION),\n+                lambda: (tf.cast(image, tf.float32) - 127.5) / 128.0,\n+                lambda: tf.image.per_image_standardization(image),\n+            )\n+            image = tf.cond(\n+                get_control_flag(control[0], FLIP),\n+                lambda: tf.image.flip_left_right(image),\n+                lambda: tf.identity(image),\n+            )\n+            # pylint: disable=no-member\n             image.set_shape(image_size + (3,))\n             images.append(image)\n         images_and_labels_list.append([images, label])\n \n     image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n-        shapes=[image_size + (3,), ()], enqueue_many=True,\n+        images_and_labels_list,\n+        batch_size=batch_size_placeholder,\n+        shapes=[image_size + (3,), ()],\n+        enqueue_many=True,\n         capacity=4 * nrof_preprocess_threads * 100,\n-        allow_smaller_final_batch=True)\n-    \n+        allow_smaller_final_batch=True,\n+    )\n+\n     return image_batch, label_batch\n \n+\n def get_control_flag(control, field):\n     return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n-  \n+\n+\n def _add_loss_summaries(total_loss):\n     """Add summaries for losses.\n   \n@@ -151,100 +181,127 @@ def _add_loss_summaries(total_loss):\n       loss_averages_op: op for generating moving averages of losses.\n     """\n     # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n+    loss_averages = tf.train.ExponentialMovingAverage(0.9, name="avg")\n+    losses = tf.get_collection("losses")\n     loss_averages_op = loss_averages.apply(losses + [total_loss])\n-  \n+\n     # Attach a scalar summmary to all individual losses and the total loss; do the\n     # same for the averaged version of the losses.\n     for l in losses + [total_loss]:\n         # Name each loss as \'(raw)\' and name the moving average version of the loss\n         # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n+        tf.summary.scalar(l.op.name + " (raw)", l)\n         tf.summary.scalar(l.op.name, loss_averages.average(l))\n-  \n+\n     return loss_averages_op\n \n-def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n+\n+def train(\n+    total_loss,\n+    global_step,\n+    optimizer,\n+    learning_rate,\n+    moving_average_decay,\n+    update_gradient_vars,\n+    log_histograms=True,\n+):\n     # Generate moving averages of all losses and associated summaries.\n     loss_averages_op = _add_loss_summaries(total_loss)\n \n     # Compute gradients.\n     with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n+        if optimizer == "ADAGRAD":\n             opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n+        elif optimizer == "ADADELTA":\n             opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n+        elif optimizer == "ADAM":\n+            opt = tf.train.AdamOptimizer(\n+                learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1\n+            )\n+        elif optimizer == "RMSPROP":\n+            opt = tf.train.RMSPropOptimizer(\n+                learning_rate, decay=0.9, momentum=0.9, epsilon=1.0\n+            )\n+        elif optimizer == "MOM":\n             opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n         else:\n-            raise ValueError(\'Invalid optimization algorithm\')\n-    \n+            raise ValueError("Invalid optimization algorithm")\n+\n         grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n+\n     # Apply gradients.\n     apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n-  \n+\n     # Add histograms for trainable variables.\n     if log_histograms:\n         for var in tf.trainable_variables():\n             tf.summary.histogram(var.op.name, var)\n-   \n+\n     # Add histograms for gradients.\n     if log_histograms:\n         for grad, var in grads:\n             if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n-  \n+                tf.summary.histogram(var.op.name + "/gradients", grad)\n+\n     # Track the moving averages of all trainable variables.\n     variable_averages = tf.train.ExponentialMovingAverage(\n-        moving_average_decay, global_step)\n+        moving_average_decay, global_step\n+    )\n     variables_averages_op = variable_averages.apply(tf.trainable_variables())\n-  \n+\n     with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n-  \n+        train_op = tf.no_op(name="train")\n+\n     return train_op\n \n+\n def prewhiten(x):\n     mean = np.mean(x)\n     std = np.std(x)\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n+    std_adj = np.maximum(std, 1.0 / np.sqrt(x.size))\n+    y = np.multiply(np.subtract(x, mean), 1 / std_adj)\n+    return y\n+\n \n def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n-        sz1 = int(image.shape[1]//2)\n-        sz2 = int(image_size//2)\n+    if image.shape[1] > image_size:\n+        sz1 = int(image.shape[1] // 2)\n+        sz2 = int(image_size // 2)\n         if random_crop:\n-            diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n+            diff = sz1 - sz2\n+            (h, v) = (\n+                np.random.randint(-diff, diff + 1),\n+                np.random.randint(-diff, diff + 1),\n+            )\n         else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n+            (h, v) = (0, 0)\n+        image = image[\n+            (sz1 - sz2 + v) : (sz1 + sz2 + v), (sz1 - sz2 + h) : (sz1 + sz2 + h), :\n+        ]\n     return image\n-  \n+\n+\n def flip(image, random_flip):\n     if random_flip and np.random.choice([True, False]):\n         image = np.fliplr(image)\n     return image\n \n+\n def to_rgb(img):\n     w, h = img.shape\n     ret = np.empty((w, h, 3), dtype=np.uint8)\n     ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n     return ret\n-  \n-def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n+\n+\n+def load_data(\n+    image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True\n+):\n     nrof_samples = len(image_paths)\n     images = np.zeros((nrof_samples, image_size, image_size, 3))\n     for i in range(nrof_samples):\n         import imageio\n+\n         img = imageio.imread(image_paths[i])\n         if img.ndim == 2:\n             img = to_rgb(img)\n@@ -252,49 +309,53 @@ def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhi\n             img = prewhiten(img)\n         img = crop(img, do_random_crop, image_size)\n         img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n+        images[i, :, :, :] = img\n     return images\n \n+\n def get_label_batch(label_data, batch_size, batch_index):\n     nrof_examples = np.size(label_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = label_data[j:j+batch_size]\n+    j = batch_index * batch_size % nrof_examples\n+    if j + batch_size <= nrof_examples:\n+        batch = label_data[j : j + batch_size]\n     else:\n         x1 = label_data[j:nrof_examples]\n-        x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n+        x2 = label_data[0 : nrof_examples - j]\n+        batch = np.vstack([x1, x2])\n     batch_int = batch.astype(np.int64)\n     return batch_int\n \n+\n def get_batch(image_data, batch_size, batch_index):\n     nrof_examples = np.size(image_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n+    j = batch_index * batch_size % nrof_examples\n+    if j + batch_size <= nrof_examples:\n+        batch = image_data[j : j + batch_size, :, :, :]\n     else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n+        x1 = image_data[j:nrof_examples, :, :, :]\n+        x2 = image_data[0 : nrof_examples - j, :, :, :]\n+        batch = np.vstack([x1, x2])\n     batch_float = batch.astype(np.float32)\n     return batch_float\n \n+\n def get_triplet_batch(triplets, batch_index, batch_size):\n     ax, px, nx = triplets\n-    a = get_batch(ax, int(batch_size/3), batch_index)\n-    p = get_batch(px, int(batch_size/3), batch_index)\n-    n = get_batch(nx, int(batch_size/3), batch_index)\n+    a = get_batch(ax, int(batch_size / 3), batch_index)\n+    p = get_batch(px, int(batch_size / 3), batch_index)\n+    n = get_batch(nx, int(batch_size / 3), batch_index)\n     batch = np.vstack([a, p, n])\n     return batch\n \n+\n def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n+    with open(filename, "r") as f:\n         for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n+            line = line.split("#", 1)[0]\n             if line:\n-                par = line.strip().split(\':\')\n+                par = line.strip().split(":")\n                 e = int(par[0])\n-                if par[1]==\'-\':\n+                if par[1] == "-":\n                     lr = -1\n                 else:\n                     lr = float(par[1])\n@@ -303,23 +364,29 @@ def get_learning_rate_from_file(filename, epoch):\n                 else:\n                     return learning_rate\n \n-class ImageClass():\n+\n+class ImageClass:\n     "Stores the paths to images for a given class"\n+\n     def __init__(self, name, image_paths):\n         self.name = name\n         self.image_paths = image_paths\n-  \n+\n     def __str__(self):\n-        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n-  \n+        return self.name + ", " + str(len(self.image_paths)) + " images"\n+\n     def __len__(self):\n         return len(self.image_paths)\n-  \n+\n+\n def get_dataset(path, has_class_directories=True):\n     dataset = []\n     path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n+    classes = [\n+        path\n+        for path in os.listdir(path_exp)\n+        if os.path.isdir(os.path.join(path_exp, path))\n+    ]\n     classes.sort()\n     nrof_classes = len(classes)\n     for i in range(nrof_classes):\n@@ -327,179 +394,225 @@ def get_dataset(path, has_class_directories=True):\n         facedir = os.path.join(path_exp, class_name)\n         image_paths = get_image_paths(facedir)\n         dataset.append(ImageClass(class_name, image_paths))\n-  \n+\n     return dataset\n \n+\n def get_image_paths(facedir):\n     image_paths = []\n     if os.path.isdir(facedir):\n         images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n+        image_paths = [os.path.join(facedir, img) for img in images]\n     return image_paths\n-  \n+\n+\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n+    if mode == "SPLIT_CLASSES":\n         nrof_classes = len(dataset)\n         class_indices = np.arange(nrof_classes)\n         np.random.shuffle(class_indices)\n-        split = int(round(nrof_classes*(1-split_ratio)))\n+        split = int(round(nrof_classes * (1 - split_ratio)))\n         train_set = [dataset[i] for i in class_indices[0:split]]\n         test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n+    elif mode == "SPLIT_IMAGES":\n         train_set = []\n         test_set = []\n         for cls in dataset:\n             paths = cls.image_paths\n             np.random.shuffle(paths)\n             nrof_images_in_class = len(paths)\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n-                split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n+            split = int(math.floor(nrof_images_in_class * (1 - split_ratio)))\n+            if split == nrof_images_in_class:\n+                split = nrof_images_in_class - 1\n+            if split >= min_nrof_images_per_class and nrof_images_in_class - split >= 1:\n                 train_set.append(ImageClass(cls.name, paths[:split]))\n                 test_set.append(ImageClass(cls.name, paths[split:]))\n     else:\n         raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n     return train_set, test_set\n \n+\n def load_model(model, input_map=None):\n     # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n     #  or if it is a protobuf file with a frozen graph\n     model_exp = os.path.expanduser(model)\n-    if (os.path.isfile(model_exp)):\n-        print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n+    if os.path.isfile(model_exp):\n+        print("Model filename: %s" % model_exp)\n+        with gfile.FastGFile(model_exp, "rb") as f:\n             graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n+            tf.import_graph_def(graph_def, input_map=input_map, name="")\n     else:\n-        print(\'Model directory: %s\' % model_exp)\n+        print("Model directory: %s" % model_exp)\n         meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n-        print(\'Metagraph file: %s\' % meta_file)\n-        print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n+\n+        print("Metagraph file: %s" % meta_file)\n+        print("Checkpoint file: %s" % ckpt_file)\n+\n+        saver = tf.train.import_meta_graph(\n+            os.path.join(model_exp, meta_file), input_map=input_map\n+        )\n         saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n+\n+\n def get_model_filenames(model_dir):\n     files = os.listdir(model_dir)\n-    meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n+    meta_files = [s for s in files if s.endswith(".meta")]\n+    if len(meta_files) == 0:\n+        raise ValueError("No meta file found in the model directory (%s)" % model_dir)\n+    elif len(meta_files) > 1:\n+        raise ValueError(\n+            "There should not be more than one meta file in the model directory (%s)"\n+            % model_dir\n+        )\n     meta_file = meta_files[0]\n     ckpt = tf.train.get_checkpoint_state(model_dir)\n     if ckpt and ckpt.model_checkpoint_path:\n         ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n         return meta_file, ckpt_file\n \n-    meta_files = [s for s in files if \'.ckpt\' in s]\n+    meta_files = [s for s in files if ".ckpt" in s]\n     max_step = -1\n     for f in files:\n-        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n+        step_str = re.match(r"(^model-[\\w\\- ]+.ckpt-(\\d+))", f)\n+        if step_str is not None and len(step_str.groups()) >= 2:\n             step = int(step_str.groups()[1])\n             if step > max_step:\n                 max_step = step\n                 ckpt_file = step_str.groups()[0]\n     return meta_file, ckpt_file\n-  \n+\n+\n def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n+    if distance_metric == 0:\n         # Euclidian distance\n         diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n+        dist = np.sum(np.square(diff), 1)\n+    elif distance_metric == 1:\n         # Distance based on cosine similarity\n         dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n         norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n         similarity = dot / norm\n         dist = np.arccos(similarity) / math.pi\n     else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n+        raise "Undefined distance metric %d" % distance_metric\n+\n     return dist\n \n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n+\n+def calculate_roc(\n+    thresholds,\n+    embeddings1,\n+    embeddings2,\n+    actual_issame,\n+    nrof_folds=10,\n+    distance_metric=0,\n+    subtract_mean=False,\n+):\n+    assert embeddings1.shape[0] == embeddings2.shape[0]\n+    assert embeddings1.shape[1] == embeddings2.shape[1]\n     nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n     nrof_thresholds = len(thresholds)\n     k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n+\n+    tprs = np.zeros((nrof_folds, nrof_thresholds))\n+    fprs = np.zeros((nrof_folds, nrof_thresholds))\n     accuracy = np.zeros((nrof_folds))\n-    \n+\n     indices = np.arange(nrof_pairs)\n-    \n+\n     for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n         if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+            mean = np.mean(\n+                np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0\n+            )\n         else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n+            mean = 0.0\n+        dist = distance(embeddings1 - mean, embeddings2 - mean, distance_metric)\n+\n         # Find the best threshold for the fold\n         acc_train = np.zeros((nrof_thresholds))\n         for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n+            _, _, acc_train[threshold_idx] = calculate_accuracy(\n+                threshold, dist[train_set], actual_issame[train_set]\n+            )\n         best_threshold_index = np.argmax(acc_train)\n         for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n+            (\n+                tprs[fold_idx, threshold_idx],\n+                fprs[fold_idx, threshold_idx],\n+                _,\n+            ) = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n+        _, _, accuracy[fold_idx] = calculate_accuracy(\n+            thresholds[best_threshold_index], dist[test_set], actual_issame[test_set]\n+        )\n+\n+        tpr = np.mean(tprs, 0)\n+        fpr = np.mean(fprs, 0)\n     return tpr, fpr, accuracy\n \n+\n def calculate_accuracy(threshold, dist, actual_issame):\n     predict_issame = np.less(dist, threshold)\n     tp = np.sum(np.logical_and(predict_issame, actual_issame))\n     fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n+    tn = np.sum(\n+        np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame))\n+    )\n     fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n-  \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n-    acc = float(tp+tn)/dist.size\n+\n+    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n+    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n+    acc = float(tp + tn) / dist.size\n     return tpr, fpr, acc\n \n \n-  \n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n+def calculate_val(\n+    thresholds,\n+    embeddings1,\n+    embeddings2,\n+    actual_issame,\n+    far_target,\n+    nrof_folds=10,\n+    distance_metric=0,\n+    subtract_mean=False,\n+):\n+    assert embeddings1.shape[0] == embeddings2.shape[0]\n+    assert embeddings1.shape[1] == embeddings2.shape[1]\n     nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n     nrof_thresholds = len(thresholds)\n     k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n+\n     val = np.zeros(nrof_folds)\n     far = np.zeros(nrof_folds)\n-    \n+\n     indices = np.arange(nrof_pairs)\n-    \n+\n     for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n         if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n+            mean = np.mean(\n+                np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0\n+            )\n         else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n+            mean = 0.0\n+        dist = distance(embeddings1 - mean, embeddings2 - mean, distance_metric)\n+\n         # Find the threshold that gives FAR = far_target\n         far_train = np.zeros(nrof_thresholds)\n         for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n+            _, far_train[threshold_idx] = calculate_val_far(\n+                threshold, dist[train_set], actual_issame[train_set]\n+            )\n+        if np.max(far_train) >= far_target:\n+            f = interpolate.interp1d(far_train, thresholds, kind="slinear")\n             threshold = f(far_target)\n         else:\n             threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n-  \n+\n+        val[fold_idx], far[fold_idx] = calculate_val_far(\n+            threshold, dist[test_set], actual_issame[test_set]\n+        )\n+\n     val_mean = np.mean(val)\n     far_mean = np.mean(far)\n     val_std = np.std(val)\n@@ -516,32 +629,36 @@ def calculate_val_far(threshold, dist, actual_issame):\n     far = float(false_accept) / float(n_diff)\n     return val, far\n \n+\n def store_revision_info(src_path, output_dir, arg_string):\n     try:\n         # Get git hash\n-        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+        cmd = ["git", "rev-parse", "HEAD"]\n+        gitproc = Popen(cmd, stdout=PIPE, cwd=src_path)\n         (stdout, _) = gitproc.communicate()\n         git_hash = stdout.strip()\n     except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n-  \n+        git_hash = " ".join(cmd) + ": " + e.strerror\n+\n     try:\n         # Get local changes\n-        cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n+        cmd = ["git", "diff", "HEAD"]\n+        gitproc = Popen(cmd, stdout=PIPE, cwd=src_path)\n         (stdout, _) = gitproc.communicate()\n         git_diff = stdout.strip()\n     except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n+        git_diff = " ".join(cmd) + ": " + e.strerror\n+\n     # Store a text file in the log directory\n-    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n+    rev_info_filename = os.path.join(output_dir, "revision_info.txt")\n     with open(rev_info_filename, "w") as text_file:\n-        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n-        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n-        text_file.write(\'%s\' % git_diff)\n+        text_file.write("arguments: %s\\n--------------------\\n" % arg_string)\n+        text_file.write(\n+            "tensorflow version: %s\\n--------------------\\n" % tf.__version__\n+        )  # @UndefinedVariable\n+        text_file.write("git hash: %s\\n--------------------\\n" % git_hash)\n+        text_file.write("%s" % git_diff)\n+\n \n def list_variables(filename):\n     reader = training.NewCheckpointReader(filename)\n@@ -549,24 +666,31 @@ def list_variables(filename):\n     names = sorted(variable_map.keys())\n     return names\n \n-def put_images_on_grid(images, shape=(16,8)):\n+\n+def put_images_on_grid(images, shape=(16, 8)):\n     nrof_images = images.shape[0]\n     img_size = images.shape[1]\n     bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n+    img = np.zeros(\n+        (shape[1] * (img_size + bw) + bw, shape[0] * (img_size + bw) + bw, 3),\n+        np.float32,\n+    )\n     for i in range(shape[1]):\n-        x_start = i*(img_size+bw)+bw\n+        x_start = i * (img_size + bw) + bw\n         for j in range(shape[0]):\n-            img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n+            img_index = i * shape[0] + j\n+            if img_index >= nrof_images:\n                 break\n-            y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n+            y_start = j * (img_size + bw) + bw\n+            img[x_start : x_start + img_size, y_start : y_start + img_size, :] = images[\n+                img_index, :, :, :\n+            ]\n+        if img_index >= nrof_images:\n             break\n     return img\n \n+\n def write_arguments_to_file(args, filename):\n-    with open(filename, \'w\') as f:\n+    with open(filename, "w") as f:\n         for key, value in iteritems(vars(args)):\n-            f.write(\'%s: %s\\n\' % (key, str(value)))\n+            f.write("%s: %s\\n" % (key, str(value)))\ndiff --git a/model/src/freeze_graph.py b/model/src/freeze_graph.py\nindex f167e44..d87f4da 100644\n--- a/model/src/freeze_graph.py\n+++ b/model/src/freeze_graph.py\n@@ -2,19 +2,19 @@\n and exports the model as a graphdef protobuf\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,82 +23,112 @@ and exports the model as a graphdef protobuf\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from tensorflow.python.framework import graph_util\n-import tensorflow as tf\n import argparse\n import os\n import sys\n-from model.src import facenet_config as facenet\n \n+import tensorflow as tf\n from six.moves import xrange  # @UnresolvedImport\n+from tensorflow.python.framework import graph_util\n+\n+from model.src import facenet_config as facenet\n+\n \n def main(args):\n     with tf.Graph().as_default():\n         with tf.Session() as sess:\n             # Load the model metagraph and checkpoint\n-            print(\'Model directory: %s\' % args.model_dir)\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n-            \n-            print(\'Metagraph file: %s\' % meta_file)\n-            print(\'Checkpoint file: %s\' % ckpt_file)\n+            print("Model directory: %s" % args.model_dir)\n+            meta_file, ckpt_file = facenet.get_model_filenames(\n+                os.path.expanduser(args.model_dir)\n+            )\n+\n+            print("Metagraph file: %s" % meta_file)\n+            print("Checkpoint file: %s" % ckpt_file)\n \n             model_dir_exp = os.path.expanduser(args.model_dir)\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n+            saver = tf.train.import_meta_graph(\n+                os.path.join(model_dir_exp, meta_file), clear_devices=True\n+            )\n             tf.get_default_session().run(tf.global_variables_initializer())\n             tf.get_default_session().run(tf.local_variables_initializer())\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n-            \n+            saver.restore(\n+                tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file)\n+            )\n+\n             # Retrieve the protobuf graph definition and fix the batch norm nodes\n             input_graph_def = sess.graph.as_graph_def()\n-            \n+\n             # Freeze the graph def\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\n+            output_graph_def = freeze_graph_def(\n+                sess, input_graph_def, "embeddings,label_batch"\n+            )\n \n         # Serialize and dump the output graph to the filesystem\n-        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n+        with tf.gfile.GFile(args.output_file, "wb") as f:\n             f.write(output_graph_def.SerializeToString())\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\n-        \n+        print(\n+            "%d ops in the final graph: %s"\n+            % (len(output_graph_def.node), args.output_file)\n+        )\n+\n+\n def freeze_graph_def(sess, input_graph_def, output_node_names):\n     for node in input_graph_def.node:\n-        if node.op == \'RefSwitch\':\n-            node.op = \'Switch\'\n+        if node.op == "RefSwitch":\n+            node.op = "Switch"\n             for index in xrange(len(node.input)):\n-                if \'moving_\' in node.input[index]:\n-                    node.input[index] = node.input[index] + \'/read\'\n-        elif node.op == \'AssignSub\':\n-            node.op = \'Sub\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-        elif node.op == \'AssignAdd\':\n-            node.op = \'Add\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-    \n+                if "moving_" in node.input[index]:\n+                    node.input[index] = node.input[index] + "/read"\n+        elif node.op == "AssignSub":\n+            node.op = "Sub"\n+            if "use_locking" in node.attr:\n+                del node.attr["use_locking"]\n+        elif node.op == "AssignAdd":\n+            node.op = "Add"\n+            if "use_locking" in node.attr:\n+                del node.attr["use_locking"]\n+\n     # Get the list of important nodes\n     whitelist_names = []\n     for node in input_graph_def.node:\n-        if (node.name.startswith(\'InceptionResnet\') or node.name.startswith(\'embeddings\') or \n-                node.name.startswith(\'image_batch\') or node.name.startswith(\'label_batch\') or\n-                node.name.startswith(\'phase_train\') or node.name.startswith(\'Logits\')):\n+        if (\n+            node.name.startswith("InceptionResnet")\n+            or node.name.startswith("embeddings")\n+            or node.name.startswith("image_batch")\n+            or node.name.startswith("label_batch")\n+            or node.name.startswith("phase_train")\n+            or node.name.startswith("Logits")\n+        ):\n             whitelist_names.append(node.name)\n \n     # Replace all the variables in the graph with constants of the same values\n     output_graph_def = graph_util.convert_variables_to_constants(\n-        sess, input_graph_def, output_node_names.split(","),\n-        variable_names_whitelist=whitelist_names)\n+        sess,\n+        input_graph_def,\n+        output_node_names.split(","),\n+        variable_names_whitelist=whitelist_names,\n+    )\n     return output_graph_def\n-  \n+\n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model_dir\', type=str, \n-        help=\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\')\n-    parser.add_argument(\'output_file\', type=str, \n-        help=\'Filename for the exported graphdef protobuf (.pb)\')\n+\n+    parser.add_argument(\n+        "model_dir",\n+        type=str,\n+        help="Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters",\n+    )\n+    parser.add_argument(\n+        "output_file",\n+        type=str,\n+        help="Filename for the exported graphdef protobuf (.pb)",\n+    )\n     return parser.parse_args(argv)\n \n-if __name__ == \'__main__\':\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/generative/calculate_attribute_vectors.py b/model/src/generative/calculate_attribute_vectors.py\nindex 358f65e..dae0da6 100644\n--- a/model/src/generative/calculate_attribute_vectors.py\n+++ b/model/src/generative/calculate_attribute_vectors.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,54 +23,56 @@\n """Calculate average latent variables (here called attribute vectors) \n for the different attributes in CelebA\n """\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-import sys\n import argparse\n import importlib\n-from model.src import facenet_config as facenet\n-\n-import os\n-import numpy as np\n import math\n+import os\n+import sys\n import time\n+\n import h5py\n+import numpy as np\n+import tensorflow as tf\n from six import iteritems\n \n+from model.src import facenet_config as facenet\n+\n+\n def main(args):\n-  \n+\n     img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n     img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-    \n+\n     vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n-    \n+\n     fields, attribs_dict = read_annotations(args.annotations_filename)\n-    \n+\n     vae_def = importlib.import_module(args.vae_def)\n     vae = vae_def.Vae(args.latent_var_size)\n     gen_image_size = vae.get_image_size()\n \n     with tf.Graph().as_default():\n         tf.set_random_seed(args.seed)\n-        \n+\n         image_list = facenet.get_image_paths(os.path.expanduser(args.data_dir))\n-        \n+\n         # Get attributes for images\n         nrof_attributes = len(fields)\n         attribs_list = []\n         for img in image_list:\n-            key = os.path.split(img)[1].split(\'.\')[0]\n+            key = os.path.split(img)[1].split(".")[0]\n             attr = attribs_dict[key]\n-            assert len(attr)==nrof_attributes\n+            assert len(attr) == nrof_attributes\n             attribs_list.append(attr)\n-            \n+\n         # Create the input queue\n         index_list = range(len(image_list))\n-        input_queue = tf.train.slice_input_producer([image_list, attribs_list, index_list], num_epochs=1, shuffle=False)        \n-        \n+        input_queue = tf.train.slice_input_producer(\n+            [image_list, attribs_list, index_list], num_epochs=1, shuffle=False\n+        )\n+\n         nrof_preprocess_threads = 4\n         image_per_thread = []\n         for _ in range(nrof_preprocess_threads):\n@@ -78,124 +80,168 @@ def main(args):\n             file_contents = tf.read_file(filename)\n             image = tf.image.decode_image(file_contents, channels=3)\n             image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n-            #image = tf.image.resize_images(image, (64,64))\n+            # image = tf.image.resize_images(image, (64,64))\n             image.set_shape((args.image_size, args.image_size, 3))\n             attrib = input_queue[1]\n             attrib.set_shape((nrof_attributes,))\n             image = tf.cast(image, tf.float32)\n             image_per_thread.append([image, attrib, input_queue[2]])\n-    \n+\n         images, attribs, indices = tf.train.batch_join(\n-            image_per_thread, batch_size=args.batch_size, \n-            shapes=[(args.image_size, args.image_size, 3), (nrof_attributes,), ()], enqueue_many=False,\n+            image_per_thread,\n+            batch_size=args.batch_size,\n+            shapes=[(args.image_size, args.image_size, 3), (nrof_attributes,), ()],\n+            enqueue_many=False,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        \n+            allow_smaller_final_batch=True,\n+        )\n+\n         # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n+        images_norm = (images - img_mean) / img_stddev\n+\n+        # Resize to appropriate size for the encoder\n+        images_norm_resize = tf.image.resize_images(\n+            images_norm, (gen_image_size, gen_image_size)\n+        )\n \n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n+\n         epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n+        std = tf.exp(log_variance / 2)\n         latent_var = mean + epsilon * std\n-        \n+\n         # Create a saver\n         saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n+\n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess = tf.Session(\n+            config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n+        )\n         sess.run(tf.global_variables_initializer())\n         sess.run(tf.local_variables_initializer())\n         coord = tf.train.Coordinator()\n         tf.train.start_queue_runners(coord=coord, sess=sess)\n-        \n \n         with sess.as_default():\n-          \n+\n             if vae_checkpoint:\n-                print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n+                print("Restoring VAE checkpoint: %s" % vae_checkpoint)\n                 saver.restore(sess, vae_checkpoint)\n-           \n+\n             nrof_images = len(image_list)\n             nrof_batches = int(math.ceil(len(image_list) / args.batch_size))\n             latent_vars = np.zeros((nrof_images, args.latent_var_size))\n             attributes = np.zeros((nrof_images, nrof_attributes))\n             for i in range(nrof_batches):\n                 start_time = time.time()\n-                latent_var_, attribs_, indices_ = sess.run([latent_var, attribs, indices])\n-                latent_vars[indices_,:] = latent_var_\n-                attributes[indices_,:] = attribs_\n+                latent_var_, attribs_, indices_ = sess.run(\n+                    [latent_var, attribs, indices]\n+                )\n+                latent_vars[indices_, :] = latent_var_\n+                attributes[indices_, :] = attribs_\n                 duration = time.time() - start_time\n-                print(\'Batch %d/%d: %.3f seconds\' % (i+1, nrof_batches, duration))\n+                print("Batch %d/%d: %.3f seconds" % (i + 1, nrof_batches, duration))\n             # NOTE: This will print the \'Out of range\' warning if the last batch is not full,\n             #  as described by https://github.com/tensorflow/tensorflow/issues/8330\n-             \n+\n             # Calculate average change in the latent variable when each attribute changes\n-            attribute_vectors = np.zeros((nrof_attributes, args.latent_var_size), np.float32)\n+            attribute_vectors = np.zeros(\n+                (nrof_attributes, args.latent_var_size), np.float32\n+            )\n             for i in range(nrof_attributes):\n-                pos_idx = np.argwhere(attributes[:,i]==1)[:,0]\n-                neg_idx = np.argwhere(attributes[:,i]==-1)[:,0]\n-                pos_avg = np.mean(latent_vars[pos_idx,:], 0)\n-                neg_avg = np.mean(latent_vars[neg_idx,:], 0)\n-                attribute_vectors[i,:] = pos_avg - neg_avg\n-            \n+                pos_idx = np.argwhere(attributes[:, i] == 1)[:, 0]\n+                neg_idx = np.argwhere(attributes[:, i] == -1)[:, 0]\n+                pos_avg = np.mean(latent_vars[pos_idx, :], 0)\n+                neg_avg = np.mean(latent_vars[neg_idx, :], 0)\n+                attribute_vectors[i, :] = pos_avg - neg_avg\n+\n             filename = os.path.expanduser(args.output_filename)\n-            print(\'Writing attribute vectors, latent variables and attributes to %s\' % filename)\n-            mdict = {\'latent_vars\':latent_vars, \'attributes\':attributes, \n-                     \'fields\':fields, \'attribute_vectors\':attribute_vectors }\n-            with h5py.File(filename, \'w\') as f:\n+            print(\n+                "Writing attribute vectors, latent variables and attributes to %s"\n+                % filename\n+            )\n+            mdict = {\n+                "latent_vars": latent_vars,\n+                "attributes": attributes,\n+                "fields": fields,\n+                "attribute_vectors": attribute_vectors,\n+            }\n+            with h5py.File(filename, "w") as f:\n                 for key, value in iteritems(mdict):\n                     f.create_dataset(key, data=value)\n-                    \n-                    \n+\n+\n def read_annotations(filename):\n-    attribs = {}    \n-    with open(filename, \'r\') as f:\n+    attribs = {}\n+    with open(filename, "r") as f:\n         for i, line in enumerate(f.readlines()):\n-            if i==0:\n+            if i == 0:\n                 continue  # First line is the number of entries in the file\n-            elif i==1:\n-                fields = line.strip().split() # Second line is the field names\n+            elif i == 1:\n+                fields = line.strip().split()  # Second line is the field names\n             else:\n                 line = line.split()\n-                img_name = line[0].split(\'.\')[0]\n+                img_name = line[0].split(".")[0]\n                 img_attribs = map(int, line[1:])\n                 attribs[img_name] = img_attribs\n     return fields, attribs\n \n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\', \n-        default=\'src.generative.models.dfc_vae\')\n-    parser.add_argument(\'vae_checkpoint\', type=str,\n-        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the directory containing aligned face patches for the CelebA dataset.\')\n-    parser.add_argument(\'annotations_filename\', type=str,\n-        help=\'Path to the annotations file\',\n-        default=\'/media/deep/datasets/CelebA/Anno/list_attr_celeba.txt\')\n-    parser.add_argument(\'output_filename\', type=str,\n-        help=\'Filename to use for the file containing the attribute vectors.\')\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=128)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=64)\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n+\n+    parser.add_argument(\n+        "vae_def",\n+        type=str,\n+        help="Model definition for the variational autoencoder. Points to a module containing the definition.",\n+        default="src.generative.models.dfc_vae",\n+    )\n+    parser.add_argument(\n+        "vae_checkpoint",\n+        type=str,\n+        help="Checkpoint file of a pre-trained variational autoencoder.",\n+    )\n+    parser.add_argument(\n+        "data_dir",\n+        type=str,\n+        help="Path to the directory containing aligned face patches for the CelebA dataset.",\n+    )\n+    parser.add_argument(\n+        "annotations_filename",\n+        type=str,\n+        help="Path to the annotations file",\n+        default="/media/deep/datasets/CelebA/Anno/list_attr_celeba.txt",\n+    )\n+    parser.add_argument(\n+        "output_filename",\n+        type=str,\n+        help="Filename to use for the file containing the attribute vectors.",\n+    )\n+    parser.add_argument(\n+        "--batch_size",\n+        type=int,\n+        help="Number of images to process in a batch.",\n+        default=128,\n+    )\n+    parser.add_argument(\n+        "--image_size",\n+        type=int,\n+        help="Image size (height, width) in pixels.",\n+        default=64,\n+    )\n+    parser.add_argument(\n+        "--latent_var_size",\n+        type=int,\n+        help="Dimensionality of the latent variable.",\n+        default=100,\n+    )\n+    parser.add_argument("--seed", type=int, help="Random seed.", default=666)\n \n     return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n+\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/generative/models/dfc_vae.py b/model/src/generative/models/dfc_vae.py\nindex b4450f2..7e92419 100644\n--- a/model/src/generative/models/dfc_vae.py\n+++ b/model/src/generative/models/dfc_vae.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -25,68 +25,144 @@\n (https://arxiv.org/pdf/1610.00291.pdf)\n """\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n+import generative.models.vae_base  # @UnresolvedImport\n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n \n \n class Vae(generative.models.vae_base.Vae):\n-  \n     def __init__(self, latent_variable_dim):\n         super(Vae, self).__init__(latent_variable_dim, 64)\n-  \n+\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n+        with tf.variable_scope("encoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n+                    net = slim.conv2d(\n+                        images,\n+                        32,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        256,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4",\n+                    )\n                     net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n+                    fc1 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    fc2 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_2",\n+                    )\n         return fc1, fc2\n-      \n+\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4\')\n-                \n+        weight_decay = 0.0\n+        with tf.variable_scope("decoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n+                    net = slim.fully_connected(\n+                        latent_var,\n+                        4096,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    net = tf.reshape(net, [-1, 4, 4, 256], name="Reshape")\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(8, 8), name="Upsample_1"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(16, 16), name="Upsample_2"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(32, 32), name="Upsample_3"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        32,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(64, 64), name="Upsample_4"\n+                    )\n+                    net = slim.conv2d(\n+                        net, 3, [3, 3], 1, activation_fn=None, scope="Conv2d_4"\n+                    )\n+\n         return net\n-      \n+\n+\n def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\n+    return tf.maximum(0.1 * x, x)\ndiff --git a/model/src/generative/models/dfc_vae_large.py b/model/src/generative/models/dfc_vae_large.py\nindex aa8e8b7..9e06843 100644\n--- a/model/src/generative/models/dfc_vae_large.py\n+++ b/model/src/generative/models/dfc_vae_large.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -25,71 +25,158 @@\n (https://arxiv.org/pdf/1610.00291.pdf) but with a larger image size (128x128 pixels)\n """\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n+import generative.models.vae_base  # @UnresolvedImport\n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n \n \n class Vae(generative.models.vae_base.Vae):\n-  \n     def __init__(self, latent_variable_dim):\n         super(Vae, self).__init__(latent_variable_dim, 128)\n-        \n-      \n+\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n-                    net = slim.conv2d(net, 512, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_5\')\n+        with tf.variable_scope("encoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n+                    net = slim.conv2d(\n+                        images,\n+                        32,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        256,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4",\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        512,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_5",\n+                    )\n                     net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n+                    fc1 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    fc2 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_2",\n+                    )\n         return fc1, fc2\n-      \n+\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4\')\n-                \n-                    net = tf.image.resize_nearest_neighbor(net, size=(128,128), name=\'Upsample_5\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_5\')\n+        weight_decay = 0.0\n+        with tf.variable_scope("decoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n+                    net = slim.fully_connected(\n+                        latent_var,\n+                        4096,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    net = tf.reshape(net, [-1, 4, 4, 256], name="Reshape")\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(8, 8), name="Upsample_1"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(16, 16), name="Upsample_2"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(32, 32), name="Upsample_3"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        32,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(64, 64), name="Upsample_4"\n+                    )\n+                    net = slim.conv2d(\n+                        net, 3, [3, 3], 1, activation_fn=activation_fn, scope="Conv2d_4"\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(128, 128), name="Upsample_5"\n+                    )\n+                    net = slim.conv2d(\n+                        net, 3, [3, 3], 1, activation_fn=None, scope="Conv2d_5"\n+                    )\n         return net\n \n+\n def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)  \n+    return tf.maximum(0.1 * x, x)\ndiff --git a/model/src/generative/models/dfc_vae_resnet.py b/model/src/generative/models/dfc_vae_resnet.py\nindex 7c2f52c..c9635ec 100644\n--- a/model/src/generative/models/dfc_vae_resnet.py\n+++ b/model/src/generative/models/dfc_vae_resnet.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -25,86 +25,250 @@\n (https://arxiv.org/pdf/1610.00291.pdf)\n """\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n+import generative.models.vae_base  # @UnresolvedImport\n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n \n \n class Vae(generative.models.vae_base.Vae):\n-  \n     def __init__(self, latent_variable_dim):\n         super(Vae, self).__init__(latent_variable_dim, 64)\n-  \n+\n     def encoder(self, images, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n         weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n+        with tf.variable_scope("encoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n                     net = images\n-                    \n-                    net = slim.conv2d(net, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n-                    \n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n-\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n-\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 256, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n-                    \n+\n+                    net = slim.conv2d(\n+                        net,\n+                        32,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        32,\n+                        [4, 4],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1b",\n+                    )\n+\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        64,\n+                        [4, 4],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2b",\n+                    )\n+\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        128,\n+                        [4, 4],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3b",\n+                    )\n+\n+                    net = slim.conv2d(\n+                        net,\n+                        256,\n+                        [4, 4],\n+                        2,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        256,\n+                        [4, 4],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4b",\n+                    )\n+\n                     net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n+                    fc1 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    fc2 = slim.fully_connected(\n+                        net,\n+                        self.latent_variable_dim,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_2",\n+                    )\n         return fc1, fc2\n-      \n+\n     def decoder(self, latent_var, is_training):\n         activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4c\')\n-                \n+        weight_decay = 0.0\n+        with tf.variable_scope("decoder"):\n+            with slim.arg_scope([slim.batch_norm], is_training=is_training):\n+                with slim.arg_scope(\n+                    [slim.conv2d, slim.fully_connected],\n+                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+                    weights_regularizer=slim.l2_regularizer(weight_decay),\n+                    normalizer_fn=slim.batch_norm,\n+                    normalizer_params=self.batch_norm_params,\n+                ):\n+                    net = slim.fully_connected(\n+                        latent_var,\n+                        4096,\n+                        activation_fn=None,\n+                        normalizer_fn=None,\n+                        scope="Fc_1",\n+                    )\n+                    net = tf.reshape(net, [-1, 4, 4, 256], name="Reshape")\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(8, 8), name="Upsample_1"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        128,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        128,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_1b",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(16, 16), name="Upsample_2"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        64,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        64,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_2b",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(32, 32), name="Upsample_3"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        32,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        32,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_3b",\n+                    )\n+\n+                    net = tf.image.resize_nearest_neighbor(\n+                        net, size=(64, 64), name="Upsample_4"\n+                    )\n+                    net = slim.conv2d(\n+                        net,\n+                        3,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4a",\n+                    )\n+                    net = slim.repeat(\n+                        net,\n+                        3,\n+                        conv2d_block,\n+                        0.1,\n+                        3,\n+                        [3, 3],\n+                        1,\n+                        activation_fn=activation_fn,\n+                        scope="Conv2d_4b",\n+                    )\n+                    net = slim.conv2d(\n+                        net, 3, [3, 3], 1, activation_fn=None, scope="Conv2d_4c"\n+                    )\n+\n         return net\n-      \n+\n+\n def conv2d_block(inp, scale, *args, **kwargs):\n     return inp + slim.conv2d(inp, *args, **kwargs) * scale\n \n+\n def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\n+    return tf.maximum(0.1 * x, x)\ndiff --git a/model/src/generative/models/vae_base.py b/model/src/generative/models/vae_base.py\nindex 7437251..aca83ee 100644\n--- a/model/src/generative/models/vae_base.py\n+++ b/model/src/generative/models/vae_base.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,36 +23,33 @@\n """Base class for variational autoencoders containing an encoder and a decoder\n """\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n import tensorflow as tf\n \n+\n class Vae(object):\n-  \n     def __init__(self, latent_variable_dim, image_size):\n         self.latent_variable_dim = latent_variable_dim\n         self.image_size = image_size\n         self.batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-  \n+            # Decay for the moving averages.\n+            "decay": 0.995,\n+            # epsilon to prevent 0s in variance.\n+            "epsilon": 0.001,\n+            # force in-place updates of mean and variance estimates\n+            "updates_collections": None,\n+            # Moving averages ends up in the trainable variables collection\n+            "variables_collections": [tf.GraphKeys.TRAINABLE_VARIABLES],\n+        }\n+\n     def encoder(self, images, is_training):\n         # Must be overridden in implementation classes\n         raise NotImplementedError\n-      \n+\n     def decoder(self, latent_var, is_training):\n         # Must be overridden in implementation classes\n         raise NotImplementedError\n \n     def get_image_size(self):\n         return self.image_size\n-        \n\\ No newline at end of file\ndiff --git a/model/src/generative/modify_attribute.py b/model/src/generative/modify_attribute.py\nindex a83a97a..94af4ab 100644\n--- a/model/src/generative/modify_attribute.py\n+++ b/model/src/generative/modify_attribute.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -24,120 +24,165 @@\n \'calculate_attribute_vectors.py\'. Images are generated from latent variables of\n the CelebA dataset.\n """\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-import sys\n import argparse\n import importlib\n-from model.src import facenet_config as facenet\n-\n+import math\n import os\n-import numpy as np\n+import sys\n+\n import h5py\n-import math\n+import numpy as np\n+import tensorflow as tf\n from scipy import misc\n \n+from model.src import facenet_config as facenet\n+\n+\n def main(args):\n-  \n+\n     img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n     img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-    \n+\n     vae_def = importlib.import_module(args.vae_def)\n     vae = vae_def.Vae(args.latent_var_size)\n     gen_image_size = vae.get_image_size()\n \n     with tf.Graph().as_default():\n         tf.set_random_seed(args.seed)\n-        \n-        images = tf.placeholder(tf.float32, shape=(None,gen_image_size,gen_image_size,3), name=\'input\')\n-        \n+\n+        images = tf.placeholder(\n+            tf.float32, shape=(None, gen_image_size, gen_image_size, 3), name="input"\n+        )\n+\n         # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n+        images_norm = (images - img_mean) / img_stddev\n+\n+        # Resize to appropriate size for the encoder\n+        images_norm_resize = tf.image.resize_images(\n+            images_norm, (gen_image_size, gen_image_size)\n+        )\n \n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n+\n         epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n+        std = tf.exp(log_variance / 2)\n         latent_var = mean + epsilon * std\n-        \n+\n         # Create decoder\n         reconstructed_norm = vae.decoder(latent_var, False)\n-        \n+\n         # Un-normalize\n-        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n+        reconstructed = (reconstructed_norm * img_stddev) + img_mean\n \n         # Create a saver\n         saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n+\n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess = tf.Session(\n+            config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n+        )\n         sess.run(tf.global_variables_initializer())\n         sess.run(tf.local_variables_initializer())\n         coord = tf.train.Coordinator()\n         tf.train.start_queue_runners(coord=coord, sess=sess)\n-        \n \n         with sess.as_default():\n-          \n+\n             vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n-            print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n+            print("Restoring VAE checkpoint: %s" % vae_checkpoint)\n             saver.restore(sess, vae_checkpoint)\n-           \n+\n             filename = os.path.expanduser(args.attributes_filename)\n-            with h5py.File(filename,\'r\') as f:\n-                latent_vars = np.array(f.get(\'latent_vars\'))\n-                attributes = np.array(f.get(\'attributes\'))\n-                #fields = np.array(f.get(\'fields\'))\n-                attribute_vectors = np.array(f.get(\'attribute_vectors\'))\n+            with h5py.File(filename, "r") as f:\n+                latent_vars = np.array(f.get("latent_vars"))\n+                attributes = np.array(f.get("attributes"))\n+                # fields = np.array(f.get(\'fields\'))\n+                attribute_vectors = np.array(f.get("attribute_vectors"))\n \n             # Reconstruct faces while adding varying amount of the selected attribute vector\n-            attribute_index = 31 # 31: \'Smiling\'\n-            image_indices = [8,11,13,18,19,26,31,39,47,54,56,57,58,59,60,73]\n+            attribute_index = 31  # 31: \'Smiling\'\n+            image_indices = [\n+                8,\n+                11,\n+                13,\n+                18,\n+                19,\n+                26,\n+                31,\n+                39,\n+                47,\n+                54,\n+                56,\n+                57,\n+                58,\n+                59,\n+                60,\n+                73,\n+            ]\n             nrof_images = len(image_indices)\n             nrof_interp_steps = 10\n-            sweep_latent_var = np.zeros((nrof_interp_steps*nrof_images, args.latent_var_size), np.float32)\n+            sweep_latent_var = np.zeros(\n+                (nrof_interp_steps * nrof_images, args.latent_var_size), np.float32\n+            )\n             for j in range(nrof_images):\n                 image_index = image_indices[j]\n-                idx = np.argwhere(attributes[:,attribute_index]==-1)[image_index,0]\n+                idx = np.argwhere(attributes[:, attribute_index] == -1)[image_index, 0]\n                 for i in range(nrof_interp_steps):\n-                    sweep_latent_var[i+nrof_interp_steps*j,:] = latent_vars[idx,:] + 5.0*i/nrof_interp_steps*attribute_vectors[attribute_index,:]\n-                \n-            recon = sess.run(reconstructed, feed_dict={latent_var:sweep_latent_var})\n-            \n-            img = facenet.put_images_on_grid(recon, shape=(nrof_interp_steps*2,int(math.ceil(nrof_images/2))))\n-            \n+                    sweep_latent_var[i + nrof_interp_steps * j, :] = (\n+                        latent_vars[idx, :]\n+                        + 5.0\n+                        * i\n+                        / nrof_interp_steps\n+                        * attribute_vectors[attribute_index, :]\n+                    )\n+\n+            recon = sess.run(reconstructed, feed_dict={latent_var: sweep_latent_var})\n+\n+            img = facenet.put_images_on_grid(\n+                recon, shape=(nrof_interp_steps * 2, int(math.ceil(nrof_images / 2)))\n+            )\n+\n             image_filename = os.path.expanduser(args.output_image_filename)\n-            print(\'Writing generated image to %s\' % image_filename)\n+            print("Writing generated image to %s" % image_filename)\n             misc.imsave(image_filename, img)\n \n-                    \n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n-    parser.add_argument(\'vae_checkpoint\', type=str,\n-        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n-    parser.add_argument(\'attributes_filename\', type=str,\n-        help=\'The file containing the attribute vectors, as generated by calculate_attribute_vectors.py.\')\n-    parser.add_argument(\'output_image_filename\', type=str,\n-        help=\'File to write the generated image to.\')\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n+\n+    parser.add_argument(\n+        "vae_def",\n+        type=str,\n+        help="Model definition for the variational autoencoder. Points to a module containing the definition.",\n+    )\n+    parser.add_argument(\n+        "vae_checkpoint",\n+        type=str,\n+        help="Checkpoint file of a pre-trained variational autoencoder.",\n+    )\n+    parser.add_argument(\n+        "attributes_filename",\n+        type=str,\n+        help="The file containing the attribute vectors, as generated by calculate_attribute_vectors.py.",\n+    )\n+    parser.add_argument(\n+        "output_image_filename", type=str, help="File to write the generated image to."\n+    )\n+    parser.add_argument(\n+        "--latent_var_size",\n+        type=int,\n+        help="Dimensionality of the latent variable.",\n+        default=100,\n+    )\n+    parser.add_argument("--seed", type=int, help="Random seed.", default=666)\n \n     return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n+\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/generative/train_vae.py b/model/src/generative/train_vae.py\nindex 00e81e7..9bc214b 100644\n--- a/model/src/generative/train_vae.py\n+++ b/model/src/generative/train_vae.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2017 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -22,264 +22,402 @@\n \n """Train a Variational Autoencoder\n """\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n+import argparse\n+import importlib\n+import os\n import sys\n import time\n-import importlib\n-import argparse\n-from model.src import facenet_config as facenet\n+from datetime import datetime\n \n-import numpy as np\n import h5py\n-import os\n-from datetime import datetime\n+import numpy as np\n+import tensorflow as tf\n+import tensorflow.contrib.slim as slim\n from scipy import misc\n from six import iteritems\n \n+from model.src import facenet_config as facenet\n+\n+\n def main(args):\n-  \n+\n     img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n     img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-  \n+\n     vae_def = importlib.import_module(args.vae_def)\n     vae = vae_def.Vae(args.latent_var_size)\n     gen_image_size = vae.get_image_size()\n \n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n+    subdir = datetime.strftime(datetime.now(), "%Y%m%d-%H%M%S")\n     model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n     if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n         os.makedirs(model_dir)\n-    log_file_name = os.path.join(model_dir, \'logs.h5\')\n-    \n+    log_file_name = os.path.join(model_dir, "logs.h5")\n+\n     # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(model_dir, \'arguments.txt\'))\n-        \n+    facenet.write_arguments_to_file(args, os.path.join(model_dir, "arguments.txt"))\n+\n     # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, model_dir, \' \'.join(sys.argv))\n-    \n+    src_path, _ = os.path.split(os.path.realpath(__file__))\n+    facenet.store_revision_info(src_path, model_dir, " ".join(sys.argv))\n+\n     with tf.Graph().as_default():\n         tf.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n-        \n+\n         train_set = facenet.get_dataset(args.data_dir)\n         image_list, _ = facenet.get_image_paths_and_labels(train_set)\n-        \n+\n         # Create the input queue\n         input_queue = tf.train.string_input_producer(image_list, shuffle=True)\n-    \n+\n         nrof_preprocess_threads = 4\n         image_per_thread = []\n         for _ in range(nrof_preprocess_threads):\n             file_contents = tf.read_file(input_queue.dequeue())\n             image = tf.image.decode_image(file_contents, channels=3)\n-            image = tf.image.resize_image_with_crop_or_pad(image, args.input_image_size, args.input_image_size)\n+            image = tf.image.resize_image_with_crop_or_pad(\n+                image, args.input_image_size, args.input_image_size\n+            )\n             image.set_shape((args.input_image_size, args.input_image_size, 3))\n             image = tf.cast(image, tf.float32)\n-            #pylint: disable=no-member\n+            # pylint: disable=no-member\n             image_per_thread.append([image])\n-    \n+\n         images = tf.train.batch_join(\n-            image_per_thread, batch_size=args.batch_size,\n+            image_per_thread,\n+            batch_size=args.batch_size,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=False)\n-        \n+            allow_smaller_final_batch=False,\n+        )\n+\n         # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n+        images_norm = (images - img_mean) / img_stddev\n+\n+        # Resize to appropriate size for the encoder\n+        images_norm_resize = tf.image.resize_images(\n+            images_norm, (gen_image_size, gen_image_size)\n+        )\n \n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n         # Create encoder network\n         mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n+\n         epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n+        std = tf.exp(log_variance / 2)\n         latent_var = mean + epsilon * std\n-        \n+\n         # Create decoder network\n         reconstructed_norm = vae.decoder(latent_var, True)\n-        \n+\n         # Un-normalize\n-        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n-        \n+        reconstructed = (reconstructed_norm * img_stddev) + img_mean\n+\n         # Create reconstruction loss\n-        if args.reconstruction_loss_type==\'PLAIN\':\n-            images_resize = tf.image.resize_images(images, (gen_image_size,gen_image_size))\n-            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(images_resize - reconstructed,2)))\n-        elif args.reconstruction_loss_type==\'PERCEPTUAL\':\n+        if args.reconstruction_loss_type == "PLAIN":\n+            images_resize = tf.image.resize_images(\n+                images, (gen_image_size, gen_image_size)\n+            )\n+            reconstruction_loss = tf.reduce_mean(\n+                tf.reduce_sum(tf.pow(images_resize - reconstructed, 2))\n+            )\n+        elif args.reconstruction_loss_type == "PERCEPTUAL":\n             network = importlib.import_module(args.model_def)\n \n-            reconstructed_norm_resize = tf.image.resize_images(reconstructed_norm, (args.input_image_size,args.input_image_size))\n+            reconstructed_norm_resize = tf.image.resize_images(\n+                reconstructed_norm, (args.input_image_size, args.input_image_size)\n+            )\n \n-            # Stack images from both the input batch and the reconstructed batch in a new tensor \n+            # Stack images from both the input batch and the reconstructed batch in a new tensor\n             shp = [-1] + images_norm.get_shape().as_list()[1:]\n-            input_images = tf.reshape(tf.stack([images_norm, reconstructed_norm_resize], axis=0), shp)\n-            _, end_points = network.inference(input_images, 1.0, \n-                phase_train=False, bottleneck_layer_size=128, weight_decay=0.0)\n+            input_images = tf.reshape(\n+                tf.stack([images_norm, reconstructed_norm_resize], axis=0), shp\n+            )\n+            _, end_points = network.inference(\n+                input_images,\n+                1.0,\n+                phase_train=False,\n+                bottleneck_layer_size=128,\n+                weight_decay=0.0,\n+            )\n \n             # Get a list of feature names to use for loss terms\n-            feature_names = args.loss_features.replace(\' \', \'\').split(\',\')\n+            feature_names = args.loss_features.replace(" ", "").split(",")\n \n             # Calculate L2 loss between original and reconstructed images in feature space\n             reconstruction_loss_list = []\n             for feature_name in feature_names:\n                 feature_flat = slim.flatten(end_points[feature_name])\n-                image_feature, reconstructed_feature = tf.unstack(tf.reshape(feature_flat, [2,args.batch_size,-1]), num=2, axis=0)\n-                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(image_feature-reconstructed_feature, 2)), name=feature_name+\'_loss\')\n+                image_feature, reconstructed_feature = tf.unstack(\n+                    tf.reshape(feature_flat, [2, args.batch_size, -1]), num=2, axis=0\n+                )\n+                reconstruction_loss = tf.reduce_mean(\n+                    tf.reduce_sum(tf.pow(image_feature - reconstructed_feature, 2)),\n+                    name=feature_name + "_loss",\n+                )\n                 reconstruction_loss_list.append(reconstruction_loss)\n             # Sum up the losses in for the different features\n-            reconstruction_loss = tf.add_n(reconstruction_loss_list, \'reconstruction_loss\')\n+            reconstruction_loss = tf.add_n(\n+                reconstruction_loss_list, "reconstruction_loss"\n+            )\n         else:\n             pass\n-        \n+\n         # Create KL divergence loss\n         kl_loss = kl_divergence_loss(mean, log_variance)\n         kl_loss_mean = tf.reduce_mean(kl_loss)\n-        \n-        total_loss = args.alfa*kl_loss_mean + args.beta*reconstruction_loss\n-        \n-        learning_rate = tf.train.exponential_decay(args.initial_learning_rate, global_step,\n-            args.learning_rate_decay_steps, args.learning_rate_decay_factor, staircase=True)\n-        \n+\n+        total_loss = args.alfa * kl_loss_mean + args.beta * reconstruction_loss\n+\n+        learning_rate = tf.train.exponential_decay(\n+            args.initial_learning_rate,\n+            global_step,\n+            args.learning_rate_decay_steps,\n+            args.learning_rate_decay_factor,\n+            staircase=True,\n+        )\n+\n         # Calculate gradients and make sure not to include parameters for the perceptual loss model\n         opt = tf.train.AdamOptimizer(learning_rate)\n         grads = opt.compute_gradients(total_loss, var_list=get_variables_to_train())\n-        \n+\n         # Apply gradients\n         apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n         with tf.control_dependencies([apply_gradient_op]):\n-            train_op = tf.no_op(name=\'train\')\n+            train_op = tf.no_op(name="train")\n \n         # Create a saver\n         saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n+\n         facenet_saver = tf.train.Saver(get_facenet_variables_to_restore())\n \n         # Start running operations on the Graph\n         gpu_memory_fraction = 1.0\n         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        sess = tf.Session(\n+            config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n+        )\n         sess.run(tf.global_variables_initializer())\n         sess.run(tf.local_variables_initializer())\n         coord = tf.train.Coordinator()\n         tf.train.start_queue_runners(coord=coord, sess=sess)\n \n         with sess.as_default():\n-            \n-            if args.reconstruction_loss_type==\'PERCEPTUAL\':\n+\n+            if args.reconstruction_loss_type == "PERCEPTUAL":\n                 if not args.pretrained_model:\n-                    raise ValueError(\'A pretrained model must be specified when using perceptual loss\')\n+                    raise ValueError(\n+                        "A pretrained model must be specified when using perceptual loss"\n+                    )\n                 pretrained_model_exp = os.path.expanduser(args.pretrained_model)\n-                print(\'Restoring pretrained model: %s\' % pretrained_model_exp)\n+                print("Restoring pretrained model: %s" % pretrained_model_exp)\n                 facenet_saver.restore(sess, pretrained_model_exp)\n-          \n+\n             log = {\n-                \'total_loss\': np.zeros((0,), np.float),\n-                \'reconstruction_loss\': np.zeros((0,), np.float),\n-                \'kl_loss\': np.zeros((0,), np.float),\n-                \'learning_rate\': np.zeros((0,), np.float),\n-                }\n-            \n+                "total_loss": np.zeros((0,), np.float),\n+                "reconstruction_loss": np.zeros((0,), np.float),\n+                "kl_loss": np.zeros((0,), np.float),\n+                "learning_rate": np.zeros((0,), np.float),\n+            }\n+\n             step = 0\n-            print(\'Running training\')\n+            print("Running training")\n             while step < args.max_nrof_steps:\n                 start_time = time.time()\n                 step += 1\n-                save_state = step>0 and (step % args.save_every_n_steps==0 or step==args.max_nrof_steps)\n+                save_state = step > 0 and (\n+                    step % args.save_every_n_steps == 0 or step == args.max_nrof_steps\n+                )\n                 if save_state:\n-                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_, rec_ = sess.run(\n-                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate, reconstructed])\n-                    img = facenet.put_images_on_grid(rec_, shape=(16,8))\n-                    misc.imsave(os.path.join(model_dir, \'reconstructed_%06d.png\' % step), img)\n+                    (\n+                        _,\n+                        reconstruction_loss_,\n+                        kl_loss_mean_,\n+                        total_loss_,\n+                        learning_rate_,\n+                        rec_,\n+                    ) = sess.run(\n+                        [\n+                            train_op,\n+                            reconstruction_loss,\n+                            kl_loss_mean,\n+                            total_loss,\n+                            learning_rate,\n+                            reconstructed,\n+                        ]\n+                    )\n+                    img = facenet.put_images_on_grid(rec_, shape=(16, 8))\n+                    misc.imsave(\n+                        os.path.join(model_dir, "reconstructed_%06d.png" % step), img\n+                    )\n                 else:\n-                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_ = sess.run(\n-                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate])\n-                log[\'total_loss\'] = np.append(log[\'total_loss\'], total_loss_)\n-                log[\'reconstruction_loss\'] = np.append(log[\'reconstruction_loss\'], reconstruction_loss_)\n-                log[\'kl_loss\'] = np.append(log[\'kl_loss\'], kl_loss_mean_)\n-                log[\'learning_rate\'] = np.append(log[\'learning_rate\'], learning_rate_)\n+                    (\n+                        _,\n+                        reconstruction_loss_,\n+                        kl_loss_mean_,\n+                        total_loss_,\n+                        learning_rate_,\n+                    ) = sess.run(\n+                        [\n+                            train_op,\n+                            reconstruction_loss,\n+                            kl_loss_mean,\n+                            total_loss,\n+                            learning_rate,\n+                        ]\n+                    )\n+                log["total_loss"] = np.append(log["total_loss"], total_loss_)\n+                log["reconstruction_loss"] = np.append(\n+                    log["reconstruction_loss"], reconstruction_loss_\n+                )\n+                log["kl_loss"] = np.append(log["kl_loss"], kl_loss_mean_)\n+                log["learning_rate"] = np.append(log["learning_rate"], learning_rate_)\n \n                 duration = time.time() - start_time\n-                print(\'Step: %d \\tTime: %.3f \\trec_loss: %.3f \\tkl_loss: %.3f \\ttotal_loss: %.3f\' % (step, duration, reconstruction_loss_, kl_loss_mean_, total_loss_))\n+                print(\n+                    "Step: %d \\tTime: %.3f \\trec_loss: %.3f \\tkl_loss: %.3f \\ttotal_loss: %.3f"\n+                    % (step, duration, reconstruction_loss_, kl_loss_mean_, total_loss_)\n+                )\n \n                 if save_state:\n-                    print(\'Saving checkpoint file\')\n-                    checkpoint_path = os.path.join(model_dir, \'model.ckpt\')\n-                    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-                    print(\'Saving log\')\n-                    with h5py.File(log_file_name, \'w\') as f:\n+                    print("Saving checkpoint file")\n+                    checkpoint_path = os.path.join(model_dir, "model.ckpt")\n+                    saver.save(\n+                        sess, checkpoint_path, global_step=step, write_meta_graph=False\n+                    )\n+                    print("Saving log")\n+                    with h5py.File(log_file_name, "w") as f:\n                         for key, value in iteritems(log):\n                             f.create_dataset(key, data=value)\n \n+\n def get_variables_to_train():\n     train_variables = []\n     for var in tf.trainable_variables():\n-        if \'Inception\' not in var.name:\n+        if "Inception" not in var.name:\n             train_variables.append(var)\n     return train_variables\n \n+\n def get_facenet_variables_to_restore():\n     facenet_variables = []\n     for var in tf.global_variables():\n-        if var.name.startswith(\'Inception\'):\n-            if \'Adam\' not in var.name:\n+        if var.name.startswith("Inception"):\n+            if "Adam" not in var.name:\n                 facenet_variables.append(var)\n     return facenet_variables\n \n+\n def kl_divergence_loss(mean, log_variance):\n-    kl = 0.5 * tf.reduce_sum( tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, reduction_indices = 1)\n+    kl = 0.5 * tf.reduce_sum(\n+        tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, reduction_indices=1\n+    )\n     return kl\n \n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\')\n-    parser.add_argument(\'model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\')\n-    parser.add_argument(\'pretrained_model\', type=str,\n-        help=\'Pretrained model to use to calculate features for perceptual loss.\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/vae\')\n-    parser.add_argument(\'--loss_features\', type=str,\n-        help=\'Comma separated list of features to use for perceptual loss. Features should be defined \' +\n-          \'in the end_points dictionary.\', default=\'Conv2d_1a_3x3,Conv2d_2a_3x3, Conv2d_2b_3x3\')\n-    parser.add_argument(\'--reconstruction_loss_type\', type=str, choices=[\'PLAIN\', \'PERCEPTUAL\'],\n-        help=\'The type of reconstruction loss to use\', default=\'PERCEPTUAL\')\n-    parser.add_argument(\'--max_nrof_steps\', type=int,\n-        help=\'Number of steps to run.\', default=50000)\n-    parser.add_argument(\'--save_every_n_steps\', type=int,\n-        help=\'Number of steps between storing of model checkpoint and log files\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=128)\n-    parser.add_argument(\'--input_image_size\', type=int,\n-        help=\'Image size of input images (height, width) in pixels. If perceptual loss is used this \' \n-        + \'should be the input image size for the perceptual loss model\', default=160)\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--initial_learning_rate\', type=float,\n-        help=\'Initial learning rate.\', default=0.0005)\n-    parser.add_argument(\'--learning_rate_decay_steps\', type=int,\n-        help=\'Number of steps between learning rate decay.\', default=1)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--alfa\', type=float,\n-        help=\'Kullback-Leibler divergence loss factor.\', default=1.0)\n-    parser.add_argument(\'--beta\', type=float,\n-        help=\'Reconstruction loss factor.\', default=0.5)\n-    \n+\n+    parser.add_argument(\n+        "vae_def",\n+        type=str,\n+        help="Model definition for the variational autoencoder. Points to a module containing the definition.",\n+    )\n+    parser.add_argument(\n+        "data_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned face patches.",\n+    )\n+    parser.add_argument(\n+        "model_def",\n+        type=str,\n+        help="Model definition. Points to a module containing the definition of the inference graph.",\n+    )\n+    parser.add_argument(\n+        "pretrained_model",\n+        type=str,\n+        help="Pretrained model to use to calculate features for perceptual loss.",\n+    )\n+    parser.add_argument(\n+        "--models_base_dir",\n+        type=str,\n+        help="Directory where to write trained models and checkpoints.",\n+        default="~/vae",\n+    )\n+    parser.add_argument(\n+        "--loss_features",\n+        type=str,\n+        help="Comma separated list of features to use for perceptual loss. Features should be defined "\n+        + "in the end_points dictionary.",\n+        default="Conv2d_1a_3x3,Conv2d_2a_3x3, Conv2d_2b_3x3",\n+    )\n+    parser.add_argument(\n+        "--reconstruction_loss_type",\n+        type=str,\n+        choices=["PLAIN", "PERCEPTUAL"],\n+        help="The type of reconstruction loss to use",\n+        default="PERCEPTUAL",\n+    )\n+    parser.add_argument(\n+        "--max_nrof_steps", type=int, help="Number of steps to run.", default=50000\n+    )\n+    parser.add_argument(\n+        "--save_every_n_steps",\n+        type=int,\n+        help="Number of steps between storing of model checkpoint and log files",\n+        default=500,\n+    )\n+    parser.add_argument(\n+        "--batch_size",\n+        type=int,\n+        help="Number of images to process in a batch.",\n+        default=128,\n+    )\n+    parser.add_argument(\n+        "--input_image_size",\n+        type=int,\n+        help="Image size of input images (height, width) in pixels. If perceptual loss is used this "\n+        + "should be the input image size for the perceptual loss model",\n+        default=160,\n+    )\n+    parser.add_argument(\n+        "--latent_var_size",\n+        type=int,\n+        help="Dimensionality of the latent variable.",\n+        default=100,\n+    )\n+    parser.add_argument(\n+        "--initial_learning_rate",\n+        type=float,\n+        help="Initial learning rate.",\n+        default=0.0005,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_steps",\n+        type=int,\n+        help="Number of steps between learning rate decay.",\n+        default=1,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_factor",\n+        type=float,\n+        help="Learning rate decay factor.",\n+        default=1.0,\n+    )\n+    parser.add_argument("--seed", type=int, help="Random seed.", default=666)\n+    parser.add_argument(\n+        "--alfa",\n+        type=float,\n+        help="Kullback-Leibler divergence loss factor.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--beta", type=float, help="Reconstruction loss factor.", default=0.5\n+    )\n+\n     return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n+\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/image_retrieve.py b/model/src/image_retrieve.py\nindex 540aad7..4c9bcf2 100644\n--- a/model/src/image_retrieve.py\n+++ b/model/src/image_retrieve.py\n@@ -1,8 +1,9 @@\n-from pymongo import MongoClient\n+import io\n from datetime import datetime\n+\n import pymongo\n from PIL import Image\n-import io\n+from pymongo import MongoClient\n \n # connect to db\n uri = "mongodb+srv://npn279:grab2023@cluster0.ek6wvyn.mongodb.net/?retryWrites=true&w=majority"\n@@ -14,9 +15,9 @@ db = client.Bootcamp\n # select a collection (table) USER\n collection = db.USER\n \n-document = collection.find_one({\'_id\': \'user3\'})\n+document = collection.find_one({"_id": "user3"})\n \n-image_binary = document[\'profile_image\']\n+image_binary = document["profile_image"]\n \n image_data = io.BytesIO(image_binary)\n image = Image.open(image_data)\ndiff --git a/model/src/lfw.py b/model/src/lfw.py\nindex 0e506be..2ee62f7 100644\n--- a/model/src/lfw.py\n+++ b/model/src/lfw.py\n@@ -2,19 +2,19 @@\n """\n \n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,65 +23,92 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n import os\n+\n import numpy as np\n+\n from model.src import facenet_config as facenet\n \n \n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n+def evaluate(\n+    embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False\n+):\n     # Calculate evaluation metrics\n     thresholds = np.arange(0, 4, 0.01)\n     embeddings1 = embeddings[0::2]\n     embeddings2 = embeddings[1::2]\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n+    tpr, fpr, accuracy = facenet.calculate_roc(\n+        thresholds,\n+        embeddings1,\n+        embeddings2,\n+        np.asarray(actual_issame),\n+        nrof_folds=nrof_folds,\n+        distance_metric=distance_metric,\n+        subtract_mean=subtract_mean,\n+    )\n     thresholds = np.arange(0, 4, 0.001)\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n+    val, val_std, far = facenet.calculate_val(\n+        thresholds,\n+        embeddings1,\n+        embeddings2,\n+        np.asarray(actual_issame),\n+        1e-3,\n+        nrof_folds=nrof_folds,\n+        distance_metric=distance_metric,\n+        subtract_mean=subtract_mean,\n+    )\n     return tpr, fpr, accuracy, val, val_std, far\n \n+\n def get_paths(lfw_dir, pairs):\n     nrof_skipped_pairs = 0\n     path_list = []\n     issame_list = []\n     for pair in pairs:\n         if len(pair) == 3:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2])))\n+            path0 = add_extension(\n+                os.path.join(lfw_dir, pair[0], pair[0] + "_" + "%04d" % int(pair[1]))\n+            )\n+            path1 = add_extension(\n+                os.path.join(lfw_dir, pair[0], pair[0] + "_" + "%04d" % int(pair[2]))\n+            )\n             issame = True\n         elif len(pair) == 4:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3])))\n+            path0 = add_extension(\n+                os.path.join(lfw_dir, pair[0], pair[0] + "_" + "%04d" % int(pair[1]))\n+            )\n+            path1 = add_extension(\n+                os.path.join(lfw_dir, pair[2], pair[2] + "_" + "%04d" % int(pair[3]))\n+            )\n             issame = False\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n-            path_list += (path0,path1)\n+        if os.path.exists(path0) and os.path.exists(\n+            path1\n+        ):  # Only add the pair if both paths exist\n+            path_list += (path0, path1)\n             issame_list.append(issame)\n         else:\n             nrof_skipped_pairs += 1\n-    if nrof_skipped_pairs>0:\n-        print(\'Skipped %d image pairs\' % nrof_skipped_pairs)\n-    \n+    if nrof_skipped_pairs > 0:\n+        print("Skipped %d image pairs" % nrof_skipped_pairs)\n+\n     return path_list, issame_list\n-  \n+\n+\n def add_extension(path):\n-    if os.path.exists(path+\'.jpg\'):\n-        return path+\'.jpg\'\n-    elif os.path.exists(path+\'.png\'):\n-        return path+\'.png\'\n+    if os.path.exists(path + ".jpg"):\n+        return path + ".jpg"\n+    elif os.path.exists(path + ".png"):\n+        return path + ".png"\n     else:\n         raise RuntimeError(\'No file "%s" with extension png or jpg.\' % path)\n \n+\n def read_pairs(pairs_filename):\n     pairs = []\n-    with open(pairs_filename, \'r\') as f:\n+    with open(pairs_filename, "r") as f:\n         for line in f.readlines()[1:]:\n             pair = line.strip().split()\n             pairs.append(pair)\n     return np.array(pairs)\n-\n-\n-\ndiff --git a/model/src/models/__init__.py b/model/src/models/__init__.py\nindex efa6252..9c0fa90 100644\n--- a/model/src/models/__init__.py\n+++ b/model/src/models/__init__.py\n@@ -1,2 +1 @@\n # flake8: noqa\n-\ndiff --git a/model/src/models/dummy.py b/model/src/models/dummy.py\nindex 7afe1ef..85abc5e 100644\n--- a/model/src/models/dummy.py\n+++ b/model/src/models/dummy.py\n@@ -1,19 +1,19 @@\n """Dummy model used only for testing\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -22,33 +22,46 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n+import numpy as np\n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n-import numpy as np\n-  \n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\n+\n+\n+def inference(\n+    images,\n+    keep_probability,\n+    phase_train=True,  # @UnusedVariable\n+    bottleneck_layer_size=128,\n+    bottleneck_layer_activation=None,\n+    weight_decay=0.0,\n+    reuse=None,\n+):  # @UnusedVariable\n     batch_norm_params = {\n         # Decay for the moving averages.\n-        \'decay\': 0.995,\n+        "decay": 0.995,\n         # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n+        "epsilon": 0.001,\n         # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n+        "updates_collections": None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        "variables_collections": [tf.GraphKeys.TRAINABLE_VARIABLES],\n     }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n+\n+    with slim.arg_scope(\n+        [slim.conv2d, slim.fully_connected],\n+        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n+        weights_regularizer=slim.l2_regularizer(weight_decay),\n+        normalizer_fn=slim.batch_norm,\n+        normalizer_params=batch_norm_params,\n+    ):\n         size = np.prod(images.get_shape()[1:].as_list())\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \n-                scope=\'Bottleneck\', reuse=False)\n+        net = slim.fully_connected(\n+            tf.reshape(images, (-1, size)),\n+            bottleneck_layer_size,\n+            activation_fn=None,\n+            scope="Bottleneck",\n+            reuse=False,\n+        )\n         return net, None\ndiff --git a/model/src/models/inception_resnet_v1.py b/model/src/models/inception_resnet_v1.py\nindex 475e81b..773eac2 100644\n--- a/model/src/models/inception_resnet_v1.py\n+++ b/model/src/models/inception_resnet_v1.py\n@@ -19,49 +19,63 @@ As described in http://arxiv.org/abs/1602.07261.\n     on Learning\n   Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n """\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n \n+\n # Inception-Resnet-A\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n+    with tf.variable_scope(scope, "Block35", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 32, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 32, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope="Conv2d_0b_3x3")\n+        with tf.variable_scope("Branch_2"):\n+            tower_conv2_0 = slim.conv2d(net, 32, 1, scope="Conv2d_0a_1x1")\n+            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope="Conv2d_0b_3x3")\n+            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope="Conv2d_0c_3x3")\n         mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n     return net\n \n+\n # Inception-Resnet-B\n def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n+    with tf.variable_scope(scope, "Block17", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 128, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 128, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(\n+                tower_conv1_0, 128, [1, 7], scope="Conv2d_0b_1x7"\n+            )\n+            tower_conv1_2 = slim.conv2d(\n+                tower_conv1_1, 128, [7, 1], scope="Conv2d_0c_7x1"\n+            )\n         mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n@@ -71,89 +85,119 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-C\n def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n+    with tf.variable_scope(scope, "Block8", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 192, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 192, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(\n+                tower_conv1_0, 192, [1, 3], scope="Conv2d_0b_1x3"\n+            )\n+            tower_conv1_2 = slim.conv2d(\n+                tower_conv1_1, 192, [3, 1], scope="Conv2d_0c_3x1"\n+            )\n         mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n     return net\n-  \n+\n+\n def reduction_a(net, k, l, m, n):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n-                                 scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n-                                    stride=2, padding=\'VALID\',\n-                                    scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n+    with tf.variable_scope("Branch_0"):\n+        tower_conv = slim.conv2d(\n+            net, n, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+        )\n+    with tf.variable_scope("Branch_1"):\n+        tower_conv1_0 = slim.conv2d(net, k, 1, scope="Conv2d_0a_1x1")\n+        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3, scope="Conv2d_0b_3x3")\n+        tower_conv1_2 = slim.conv2d(\n+            tower_conv1_1, m, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+        )\n+    with tf.variable_scope("Branch_2"):\n+        tower_pool = slim.max_pool2d(\n+            net, 3, stride=2, padding="VALID", scope="MaxPool_1a_3x3"\n+        )\n     net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n     return net\n \n+\n def reduction_b(net):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_3\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                        tower_conv2_2, tower_pool], 3)\n+    with tf.variable_scope("Branch_0"):\n+        tower_conv = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+        tower_conv_1 = slim.conv2d(\n+            tower_conv, 384, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+        )\n+    with tf.variable_scope("Branch_1"):\n+        tower_conv1 = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+        tower_conv1_1 = slim.conv2d(\n+            tower_conv1, 256, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+        )\n+    with tf.variable_scope("Branch_2"):\n+        tower_conv2 = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3, scope="Conv2d_0b_3x3")\n+        tower_conv2_2 = slim.conv2d(\n+            tower_conv2_1, 256, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+        )\n+    with tf.variable_scope("Branch_3"):\n+        tower_pool = slim.max_pool2d(\n+            net, 3, stride=2, padding="VALID", scope="MaxPool_1a_3x3"\n+        )\n+    net = tf.concat([tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n     return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n+\n+\n+def inference(\n+    images,\n+    keep_probability,\n+    phase_train=True,\n+    bottleneck_layer_size=128,\n+    weight_decay=0.0,\n+    reuse=None,\n+):\n     batch_norm_params = {\n         # Decay for the moving averages.\n-        \'decay\': 0.995,\n+        "decay": 0.995,\n         # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n+        "epsilon": 0.001,\n         # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n+        "updates_collections": None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        "variables_collections": [tf.GraphKeys.TRAINABLE_VARIABLES],\n     }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v1(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v1(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None, \n-                        scope=\'InceptionResnetV1\'):\n+\n+    with slim.arg_scope(\n+        [slim.conv2d, slim.fully_connected],\n+        weights_initializer=slim.initializers.xavier_initializer(),\n+        weights_regularizer=slim.l2_regularizer(weight_decay),\n+        normalizer_fn=slim.batch_norm,\n+        normalizer_params=batch_norm_params,\n+    ):\n+        return inception_resnet_v1(\n+            images,\n+            is_training=phase_train,\n+            dropout_keep_prob=keep_probability,\n+            bottleneck_layer_size=bottleneck_layer_size,\n+            reuse=reuse,\n+        )\n+\n+\n+def inception_resnet_v1(\n+    inputs,\n+    is_training=True,\n+    dropout_keep_prob=0.8,\n+    bottleneck_layer_size=128,\n+    reuse=None,\n+    scope="InceptionResnetV1",\n+):\n     """Creates the Inception Resnet V1 model.\n     Args:\n       inputs: a 4-D tensor of size [batch_size, height, width, 3].\n@@ -168,79 +212,91 @@ def inception_resnet_v1(inputs, is_training=True,\n       end_points: the set of end_points from the inception model.\n     """\n     end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n+\n+    with tf.variable_scope(scope, "InceptionResnetV1", [inputs], reuse=reuse):\n+        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n+            with slim.arg_scope(\n+                [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n+                stride=1,\n+                padding="SAME",\n+            ):\n+\n                 # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n+                net = slim.conv2d(\n+                    inputs, 32, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+                )\n+                end_points["Conv2d_1a_3x3"] = net\n                 # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n+                net = slim.conv2d(net, 32, 3, padding="VALID", scope="Conv2d_2a_3x3")\n+                end_points["Conv2d_2a_3x3"] = net\n                 # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n+                net = slim.conv2d(net, 64, 3, scope="Conv2d_2b_3x3")\n+                end_points["Conv2d_2b_3x3"] = net\n                 # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n+                net = slim.max_pool2d(\n+                    net, 3, stride=2, padding="VALID", scope="MaxPool_3a_3x3"\n+                )\n+                end_points["MaxPool_3a_3x3"] = net\n                 # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n+                net = slim.conv2d(net, 80, 1, padding="VALID", scope="Conv2d_3b_1x1")\n+                end_points["Conv2d_3b_1x1"] = net\n                 # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n+                net = slim.conv2d(net, 192, 3, padding="VALID", scope="Conv2d_4a_3x3")\n+                end_points["Conv2d_4a_3x3"] = net\n                 # 35 x 35 x 256\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_4b_3x3\')\n-                end_points[\'Conv2d_4b_3x3\'] = net\n-                \n+                net = slim.conv2d(\n+                    net, 256, 3, stride=2, padding="VALID", scope="Conv2d_4b_3x3"\n+                )\n+                end_points["Conv2d_4b_3x3"] = net\n+\n                 # 5 x Inception-resnet-A\n                 net = slim.repeat(net, 5, block35, scale=0.17)\n-                end_points[\'Mixed_5a\'] = net\n-        \n+                end_points["Mixed_5a"] = net\n+\n                 # Reduction-A\n-                with tf.variable_scope(\'Mixed_6a\'):\n+                with tf.variable_scope("Mixed_6a"):\n                     net = reduction_a(net, 192, 192, 256, 384)\n-                end_points[\'Mixed_6a\'] = net\n-                \n+                end_points["Mixed_6a"] = net\n+\n                 # 10 x Inception-Resnet-B\n                 net = slim.repeat(net, 10, block17, scale=0.10)\n-                end_points[\'Mixed_6b\'] = net\n-                \n+                end_points["Mixed_6b"] = net\n+\n                 # Reduction-B\n-                with tf.variable_scope(\'Mixed_7a\'):\n+                with tf.variable_scope("Mixed_7a"):\n                     net = reduction_b(net)\n-                end_points[\'Mixed_7a\'] = net\n-                \n+                end_points["Mixed_7a"] = net\n+\n                 # 5 x Inception-Resnet-C\n                 net = slim.repeat(net, 5, block8, scale=0.20)\n-                end_points[\'Mixed_8a\'] = net\n-                \n+                end_points["Mixed_8a"] = net\n+\n                 net = block8(net, activation_fn=None)\n-                end_points[\'Mixed_8b\'] = net\n-                \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n+                end_points["Mixed_8b"] = net\n+\n+                with tf.variable_scope("Logits"):\n+                    end_points["PrePool"] = net\n+                    # pylint: disable=no-member\n+                    net = slim.avg_pool2d(\n+                        net,\n+                        net.get_shape()[1:3],\n+                        padding="VALID",\n+                        scope="AvgPool_1a_8x8",\n+                    )\n                     net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n+\n+                    net = slim.dropout(\n+                        net, dropout_keep_prob, is_training=is_training, scope="Dropout"\n+                    )\n+\n+                    end_points["PreLogitsFlatten"] = net\n+\n+                net = slim.fully_connected(\n+                    net,\n+                    bottleneck_layer_size,\n+                    activation_fn=None,\n+                    scope="Bottleneck",\n+                    reuse=False,\n+                )\n+\n     return net, end_points\ndiff --git a/model/src/models/inception_resnet_v2.py b/model/src/models/inception_resnet_v2.py\nindex 0fb176f..f62a7f3 100644\n--- a/model/src/models/inception_resnet_v2.py\n+++ b/model/src/models/inception_resnet_v2.py\n@@ -19,49 +19,63 @@ As described in http://arxiv.org/abs/1602.07261.\n     on Learning\n   Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n """\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n \n+\n # Inception-Resnet-A\n def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n+    with tf.variable_scope(scope, "Block35", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 32, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 32, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope="Conv2d_0b_3x3")\n+        with tf.variable_scope("Branch_2"):\n+            tower_conv2_0 = slim.conv2d(net, 32, 1, scope="Conv2d_0a_1x1")\n+            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope="Conv2d_0b_3x3")\n+            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope="Conv2d_0c_3x3")\n         mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n     return net\n \n+\n # Inception-Resnet-B\n def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n+    with tf.variable_scope(scope, "Block17", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 192, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 128, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(\n+                tower_conv1_0, 160, [1, 7], scope="Conv2d_0b_1x7"\n+            )\n+            tower_conv1_2 = slim.conv2d(\n+                tower_conv1_1, 192, [7, 1], scope="Conv2d_0c_7x1"\n+            )\n         mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n@@ -71,49 +85,74 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n # Inception-Resnet-C\n def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n     """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n+    with tf.variable_scope(scope, "Block8", [net], reuse=reuse):\n+        with tf.variable_scope("Branch_0"):\n+            tower_conv = slim.conv2d(net, 192, 1, scope="Conv2d_1x1")\n+        with tf.variable_scope("Branch_1"):\n+            tower_conv1_0 = slim.conv2d(net, 192, 1, scope="Conv2d_0a_1x1")\n+            tower_conv1_1 = slim.conv2d(\n+                tower_conv1_0, 224, [1, 3], scope="Conv2d_0b_1x3"\n+            )\n+            tower_conv1_2 = slim.conv2d(\n+                tower_conv1_1, 256, [3, 1], scope="Conv2d_0c_3x1"\n+            )\n         mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n+        up = slim.conv2d(\n+            mixed,\n+            net.get_shape()[3],\n+            1,\n+            normalizer_fn=None,\n+            activation_fn=None,\n+            scope="Conv2d_1x1",\n+        )\n         net += scale * up\n         if activation_fn:\n             net = activation_fn(net)\n     return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n+\n+\n+def inference(\n+    images,\n+    keep_probability,\n+    phase_train=True,\n+    bottleneck_layer_size=128,\n+    weight_decay=0.0,\n+    reuse=None,\n+):\n     batch_norm_params = {\n         # Decay for the moving averages.\n-        \'decay\': 0.995,\n+        "decay": 0.995,\n         # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n+        "epsilon": 0.001,\n         # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n+        "updates_collections": None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-}\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v2(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v2(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None,\n-                        scope=\'InceptionResnetV2\'):\n+        "variables_collections": [tf.GraphKeys.TRAINABLE_VARIABLES],\n+    }\n+    with slim.arg_scope(\n+        [slim.conv2d, slim.fully_connected],\n+        weights_initializer=slim.initializers.xavier_initializer(),\n+        weights_regularizer=slim.l2_regularizer(weight_decay),\n+        normalizer_fn=slim.batch_norm,\n+        normalizer_params=batch_norm_params,\n+    ):\n+        return inception_resnet_v2(\n+            images,\n+            is_training=phase_train,\n+            dropout_keep_prob=keep_probability,\n+            bottleneck_layer_size=bottleneck_layer_size,\n+            reuse=reuse,\n+        )\n+\n+\n+def inception_resnet_v2(\n+    inputs,\n+    is_training=True,\n+    dropout_keep_prob=0.8,\n+    bottleneck_layer_size=128,\n+    reuse=None,\n+    scope="InceptionResnetV2",\n+):\n     """Creates the Inception Resnet V2 model.\n     Args:\n       inputs: a 4-D tensor of size [batch_size, height, width, 3].\n@@ -128,128 +167,180 @@ def inception_resnet_v2(inputs, is_training=True,\n       end_points: the set of end_points from the inception model.\n     """\n     end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n+\n+    with tf.variable_scope(scope, "InceptionResnetV2", [inputs], reuse=reuse):\n+        with slim.arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):\n+            with slim.arg_scope(\n+                [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n+                stride=1,\n+                padding="SAME",\n+            ):\n+\n                 # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n+                net = slim.conv2d(\n+                    inputs, 32, 3, stride=2, padding="VALID", scope="Conv2d_1a_3x3"\n+                )\n+                end_points["Conv2d_1a_3x3"] = net\n                 # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n+                net = slim.conv2d(net, 32, 3, padding="VALID", scope="Conv2d_2a_3x3")\n+                end_points["Conv2d_2a_3x3"] = net\n                 # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n+                net = slim.conv2d(net, 64, 3, scope="Conv2d_2b_3x3")\n+                end_points["Conv2d_2b_3x3"] = net\n                 # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n+                net = slim.max_pool2d(\n+                    net, 3, stride=2, padding="VALID", scope="MaxPool_3a_3x3"\n+                )\n+                end_points["MaxPool_3a_3x3"] = net\n                 # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n+                net = slim.conv2d(net, 80, 1, padding="VALID", scope="Conv2d_3b_1x1")\n+                end_points["Conv2d_3b_1x1"] = net\n                 # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n+                net = slim.conv2d(net, 192, 3, padding="VALID", scope="Conv2d_4a_3x3")\n+                end_points["Conv2d_4a_3x3"] = net\n                 # 35 x 35 x 192\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_5a_3x3\')\n-                end_points[\'MaxPool_5a_3x3\'] = net\n-        \n+                net = slim.max_pool2d(\n+                    net, 3, stride=2, padding="VALID", scope="MaxPool_5a_3x3"\n+                )\n+                end_points["MaxPool_5a_3x3"] = net\n+\n                 # 35 x 35 x 320\n-                with tf.variable_scope(\'Mixed_5b\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n-                                                    scope=\'Conv2d_0b_5x5\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n-                                                    scope=\'Conv2d_0c_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n-                                                     scope=\'AvgPool_0a_3x3\')\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n-                                                   scope=\'Conv2d_0b_1x1\')\n-                    net = tf.concat([tower_conv, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool_1], 3)\n-        \n-                end_points[\'Mixed_5b\'] = net\n+                with tf.variable_scope("Mixed_5b"):\n+                    with tf.variable_scope("Branch_0"):\n+                        tower_conv = slim.conv2d(net, 96, 1, scope="Conv2d_1x1")\n+                    with tf.variable_scope("Branch_1"):\n+                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv1_1 = slim.conv2d(\n+                            tower_conv1_0, 64, 5, scope="Conv2d_0b_5x5"\n+                        )\n+                    with tf.variable_scope("Branch_2"):\n+                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv2_1 = slim.conv2d(\n+                            tower_conv2_0, 96, 3, scope="Conv2d_0b_3x3"\n+                        )\n+                        tower_conv2_2 = slim.conv2d(\n+                            tower_conv2_1, 96, 3, scope="Conv2d_0c_3x3"\n+                        )\n+                    with tf.variable_scope("Branch_3"):\n+                        tower_pool = slim.avg_pool2d(\n+                            net, 3, stride=1, padding="SAME", scope="AvgPool_0a_3x3"\n+                        )\n+                        tower_pool_1 = slim.conv2d(\n+                            tower_pool, 64, 1, scope="Conv2d_0b_1x1"\n+                        )\n+                    net = tf.concat(\n+                        [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3\n+                    )\n+\n+                end_points["Mixed_5b"] = net\n                 net = slim.repeat(net, 10, block35, scale=0.17)\n-        \n+\n                 # 17 x 17 x 1024\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n-                                                 scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n-                                                    stride=2, padding=\'VALID\',\n-                                                    scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n+                with tf.variable_scope("Mixed_6a"):\n+                    with tf.variable_scope("Branch_0"):\n+                        tower_conv = slim.conv2d(\n+                            net,\n+                            384,\n+                            3,\n+                            stride=2,\n+                            padding="VALID",\n+                            scope="Conv2d_1a_3x3",\n+                        )\n+                    with tf.variable_scope("Branch_1"):\n+                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv1_1 = slim.conv2d(\n+                            tower_conv1_0, 256, 3, scope="Conv2d_0b_3x3"\n+                        )\n+                        tower_conv1_2 = slim.conv2d(\n+                            tower_conv1_1,\n+                            384,\n+                            3,\n+                            stride=2,\n+                            padding="VALID",\n+                            scope="Conv2d_1a_3x3",\n+                        )\n+                    with tf.variable_scope("Branch_2"):\n+                        tower_pool = slim.max_pool2d(\n+                            net, 3, stride=2, padding="VALID", scope="MaxPool_1a_3x3"\n+                        )\n                     net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_6a\'] = net\n+\n+                end_points["Mixed_6a"] = net\n                 net = slim.repeat(net, 20, block17, scale=0.10)\n-        \n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_7a\'] = net\n-        \n+\n+                with tf.variable_scope("Mixed_7a"):\n+                    with tf.variable_scope("Branch_0"):\n+                        tower_conv = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv_1 = slim.conv2d(\n+                            tower_conv,\n+                            384,\n+                            3,\n+                            stride=2,\n+                            padding="VALID",\n+                            scope="Conv2d_1a_3x3",\n+                        )\n+                    with tf.variable_scope("Branch_1"):\n+                        tower_conv1 = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv1_1 = slim.conv2d(\n+                            tower_conv1,\n+                            288,\n+                            3,\n+                            stride=2,\n+                            padding="VALID",\n+                            scope="Conv2d_1a_3x3",\n+                        )\n+                    with tf.variable_scope("Branch_2"):\n+                        tower_conv2 = slim.conv2d(net, 256, 1, scope="Conv2d_0a_1x1")\n+                        tower_conv2_1 = slim.conv2d(\n+                            tower_conv2, 288, 3, scope="Conv2d_0b_3x3"\n+                        )\n+                        tower_conv2_2 = slim.conv2d(\n+                            tower_conv2_1,\n+                            320,\n+                            3,\n+                            stride=2,\n+                            padding="VALID",\n+                            scope="Conv2d_1a_3x3",\n+                        )\n+                    with tf.variable_scope("Branch_3"):\n+                        tower_pool = slim.max_pool2d(\n+                            net, 3, stride=2, padding="VALID", scope="MaxPool_1a_3x3"\n+                        )\n+                    net = tf.concat(\n+                        [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3\n+                    )\n+\n+                end_points["Mixed_7a"] = net\n+\n                 net = slim.repeat(net, 9, block8, scale=0.20)\n                 net = block8(net, activation_fn=None)\n-        \n-                net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n-                end_points[\'Conv2d_7b_1x1\'] = net\n-        \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n+\n+                net = slim.conv2d(net, 1536, 1, scope="Conv2d_7b_1x1")\n+                end_points["Conv2d_7b_1x1"] = net\n+\n+                with tf.variable_scope("Logits"):\n+                    end_points["PrePool"] = net\n+                    # pylint: disable=no-member\n+                    net = slim.avg_pool2d(\n+                        net,\n+                        net.get_shape()[1:3],\n+                        padding="VALID",\n+                        scope="AvgPool_1a_8x8",\n+                    )\n                     net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n+\n+                    net = slim.dropout(\n+                        net, dropout_keep_prob, is_training=is_training, scope="Dropout"\n+                    )\n+\n+                    end_points["PreLogitsFlatten"] = net\n+\n+                net = slim.fully_connected(\n+                    net,\n+                    bottleneck_layer_size,\n+                    activation_fn=None,\n+                    scope="Bottleneck",\n+                    reuse=False,\n+                )\n+\n     return net, end_points\ndiff --git a/model/src/models/squeezenet.py b/model/src/models/squeezenet.py\nindex ae117e1..8fabfdb 100644\n--- a/model/src/models/squeezenet.py\n+++ b/model/src/models/squeezenet.py\n@@ -1,67 +1,92 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n import tensorflow as tf\n import tensorflow.contrib.slim as slim\n \n-def fire_module(inputs,\n-                squeeze_depth,\n-                expand_depth,\n-                reuse=None,\n-                scope=None,\n-                outputs_collections=None):\n-    with tf.variable_scope(scope, \'fire\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n-                            outputs_collections=None):\n+\n+def fire_module(\n+    inputs,\n+    squeeze_depth,\n+    expand_depth,\n+    reuse=None,\n+    scope=None,\n+    outputs_collections=None,\n+):\n+    with tf.variable_scope(scope, "fire", [inputs], reuse=reuse):\n+        with slim.arg_scope([slim.conv2d, slim.max_pool2d], outputs_collections=None):\n             net = squeeze(inputs, squeeze_depth)\n             outputs = expand(net, expand_depth)\n             return outputs\n \n+\n def squeeze(inputs, num_outputs):\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'squeeze\')\n+    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope="squeeze")\n+\n \n def expand(inputs, num_outputs):\n-    with tf.variable_scope(\'expand\'):\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'1x1\')\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\'3x3\')\n+    with tf.variable_scope("expand"):\n+        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope="1x1")\n+        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope="3x3")\n     return tf.concat([e1x1, e3x3], 3)\n \n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n+\n+def inference(\n+    images,\n+    keep_probability,\n+    phase_train=True,\n+    bottleneck_layer_size=128,\n+    weight_decay=0.0,\n+    reuse=None,\n+):\n     batch_norm_params = {\n         # Decay for the moving averages.\n-        \'decay\': 0.995,\n+        "decay": 0.995,\n         # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n+        "epsilon": 0.001,\n         # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n+        "updates_collections": None,\n         # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n+        "variables_collections": [tf.GraphKeys.TRAINABLE_VARIABLES],\n     }\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        with tf.variable_scope(\'squeezenet\', [images], reuse=reuse):\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                                is_training=phase_train):\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\'conv1\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool1\')\n-                net = fire_module(net, 16, 64, scope=\'fire2\')\n-                net = fire_module(net, 16, 64, scope=\'fire3\')\n-                net = fire_module(net, 32, 128, scope=\'fire4\')\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\'maxpool4\')\n-                net = fire_module(net, 32, 128, scope=\'fire5\')\n-                net = fire_module(net, 48, 192, scope=\'fire6\')\n-                net = fire_module(net, 48, 192, scope=\'fire7\')\n-                net = fire_module(net, 64, 256, scope=\'fire8\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool8\')\n-                net = fire_module(net, 64, 256, scope=\'fire9\')\n+    with slim.arg_scope(\n+        [slim.conv2d, slim.fully_connected],\n+        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n+        weights_regularizer=slim.l2_regularizer(weight_decay),\n+        normalizer_fn=slim.batch_norm,\n+        normalizer_params=batch_norm_params,\n+    ):\n+        with tf.variable_scope("squeezenet", [images], reuse=reuse):\n+            with slim.arg_scope(\n+                [slim.batch_norm, slim.dropout], is_training=phase_train\n+            ):\n+                net = slim.conv2d(images, 96, [7, 7], stride=2, scope="conv1")\n+                net = slim.max_pool2d(net, [3, 3], stride=2, scope="maxpool1")\n+                net = fire_module(net, 16, 64, scope="fire2")\n+                net = fire_module(net, 16, 64, scope="fire3")\n+                net = fire_module(net, 32, 128, scope="fire4")\n+                net = slim.max_pool2d(net, [2, 2], stride=2, scope="maxpool4")\n+                net = fire_module(net, 32, 128, scope="fire5")\n+                net = fire_module(net, 48, 192, scope="fire6")\n+                net = fire_module(net, 48, 192, scope="fire7")\n+                net = fire_module(net, 64, 256, scope="fire8")\n+                net = slim.max_pool2d(net, [3, 3], stride=2, scope="maxpool8")\n+                net = fire_module(net, 64, 256, scope="fire9")\n                 net = slim.dropout(net, keep_probability)\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'conv10\')\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\'avgpool10\')\n-                net = tf.squeeze(net, [1, 2], name=\'logits\')\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n+                net = slim.conv2d(\n+                    net,\n+                    1000,\n+                    [1, 1],\n+                    activation_fn=None,\n+                    normalizer_fn=None,\n+                    scope="conv10",\n+                )\n+                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope="avgpool10")\n+                net = tf.squeeze(net, [1, 2], name="logits")\n+                net = slim.fully_connected(\n+                    net,\n+                    bottleneck_layer_size,\n+                    activation_fn=None,\n+                    scope="Bottleneck",\n+                    reuse=False,\n+                )\n     return net, None\ndiff --git a/model/src/train_softmax.py b/model/src/train_softmax.py\nindex 2921513..cdcc73b 100644\n--- a/model/src/train_softmax.py\n+++ b/model/src/train_softmax.py\n@@ -1,19 +1,19 @@\n """Training a face recognizer with TensorFlow using softmax cross entropy loss\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -22,165 +22,224 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from datetime import datetime\n+import argparse\n+import importlib\n+import math\n import os.path\n-import time\n-import sys\n import random\n-import tensorflow as tf\n-import numpy as np\n-import importlib\n-import argparse\n-from model.src import facenet_config as facenet\n+import sys\n+import time\n+from datetime import datetime\n \n-import lfw\n import h5py\n-import math\n+import lfw\n+import numpy as np\n+import tensorflow as tf\n import tensorflow.contrib.slim as slim\n-from tensorflow.python.ops import data_flow_ops\n from tensorflow.python.framework import ops\n-from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import array_ops, data_flow_ops\n+\n+from model.src import facenet_config as facenet\n+\n \n def main(args):\n-  \n+\n     network = importlib.import_module(args.model_def)\n     image_size = (args.image_size, args.image_size)\n \n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n+    subdir = datetime.strftime(datetime.now(), "%Y%m%d-%H%M%S")\n     log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n     if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n         os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n+    model_dir = os.path.join(os.path.expanduser(args.pretrained_base_dir), subdir)\n     if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n         os.makedirs(model_dir)\n \n-    stat_file_name = os.path.join(log_dir, \'stat.h5\')\n+    stat_file_name = os.path.join(log_dir, "stat.h5")\n \n     # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n+    facenet.write_arguments_to_file(args, os.path.join(log_dir, "arguments.txt"))\n+\n     # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n+    src_path, _ = os.path.split(os.path.realpath(__file__))\n+    facenet.store_revision_info(src_path, log_dir, " ".join(sys.argv))\n \n     np.random.seed(seed=args.seed)\n     random.seed(args.seed)\n     dataset = facenet.get_dataset(args.data_dir)\n     if args.filter_filename:\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\n-        \n-    if args.validation_set_split_ratio>0.0:\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \'SPLIT_IMAGES\')\n+        dataset = filter_dataset(\n+            dataset,\n+            os.path.expanduser(args.filter_filename),\n+            args.filter_percentile,\n+            args.filter_min_nrof_images_per_class,\n+        )\n+\n+    if args.validation_set_split_ratio > 0.0:\n+        train_set, val_set = facenet.split_dataset(\n+            dataset,\n+            args.validation_set_split_ratio,\n+            args.min_nrof_val_images_per_class,\n+            "SPLIT_IMAGES",\n+        )\n     else:\n         train_set, val_set = dataset, []\n-        \n+\n     nrof_classes = len(train_set)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n+\n+    print("Model directory: %s" % model_dir)\n+    print("Log directory: %s" % log_dir)\n     pretrained_model = None\n     if args.pretrained_model:\n         pretrained_model = os.path.expanduser(args.pretrained_model)\n-        print(\'Pre-trained model: %s\' % pretrained_model)\n-    \n+        print("Pre-trained model: %s" % pretrained_model)\n+\n     if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n+        print("LFW directory: %s" % args.lfw_dir)\n         # Read the file containing the pairs used for testing\n         pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n         # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-    \n+        lfw_paths, actual_issame = lfw.get_paths(\n+            os.path.expanduser(args.lfw_dir), pairs\n+        )\n+\n     with tf.Graph().as_default():\n         tf.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n-        \n+\n         # Get a list of image paths and their labels\n         image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n-        assert len(image_list)>0, \'The training set should not be empty\'\n-        \n+        assert len(image_list) > 0, "The training set should not be empty"\n+\n         val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\n \n-        # Create a queue that produces indices into the image_list and label_list \n+        # Create a queue that produces indices into the image_list and label_list\n         labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n         range_size = array_ops.shape(labels)[0]\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\n-                             shuffle=True, seed=None, capacity=32)\n-        \n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \'index_dequeue\')\n-        \n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-        \n+        index_queue = tf.train.range_input_producer(\n+            range_size, num_epochs=None, shuffle=True, seed=None, capacity=32\n+        )\n+\n+        index_dequeue_op = index_queue.dequeue_many(\n+            args.batch_size * args.epoch_size, "index_dequeue"\n+        )\n+\n+        learning_rate_placeholder = tf.placeholder(tf.float32, name="learning_rate")\n+        batch_size_placeholder = tf.placeholder(tf.int32, name="batch_size")\n+        phase_train_placeholder = tf.placeholder(tf.bool, name="phase_train")\n+        image_paths_placeholder = tf.placeholder(\n+            tf.string, shape=(None, 1), name="image_paths"\n+        )\n+        labels_placeholder = tf.placeholder(tf.int32, shape=(None, 1), name="labels")\n+        control_placeholder = tf.placeholder(tf.int32, shape=(None, 1), name="control")\n+\n         nrof_preprocess_threads = 4\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\n-                                    shapes=[(1,), (1,), (1,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'enqueue_op\')\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        label_batch = tf.identity(label_batch, \'label_batch\')\n-        \n-        print(\'Number of classes in training set: %d\' % nrof_classes)\n-        print(\'Number of examples in training set: %d\' % len(image_list))\n-\n-        print(\'Number of classes in validation set: %d\' % len(val_set))\n-        print(\'Number of examples in validation set: %d\' % len(val_image_list))\n-        \n-        print(\'Building training graph\')\n-        \n+        input_queue = data_flow_ops.FIFOQueue(\n+            capacity=2000000,\n+            dtypes=[tf.string, tf.int32, tf.int32],\n+            shapes=[(1,), (1,), (1,)],\n+            shared_name=None,\n+            name=None,\n+        )\n+        enqueue_op = input_queue.enqueue_many(\n+            [image_paths_placeholder, labels_placeholder, control_placeholder],\n+            name="enqueue_op",\n+        )\n+        image_batch, label_batch = facenet.create_input_pipeline(\n+            input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder\n+        )\n+\n+        image_batch = tf.identity(image_batch, "image_batch")\n+        image_batch = tf.identity(image_batch, "input")\n+        label_batch = tf.identity(label_batch, "label_batch")\n+\n+        print("Number of classes in training set: %d" % nrof_classes)\n+        print("Number of examples in training set: %d" % len(image_list))\n+\n+        print("Number of classes in validation set: %d" % len(val_set))\n+        print("Number of examples in validation set: %d" % len(val_image_list))\n+\n+        print("Building training graph")\n+\n         # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \n-            weight_decay=args.weight_decay)\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \n-                weights_initializer=slim.initializers.xavier_initializer(), \n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\n-                scope=\'Logits\', reuse=False)\n+        prelogits, _ = network.inference(\n+            image_batch,\n+            args.keep_probability,\n+            phase_train=phase_train_placeholder,\n+            bottleneck_layer_size=args.embedding_size,\n+            weight_decay=args.weight_decay,\n+        )\n+        logits = slim.fully_connected(\n+            prelogits,\n+            len(train_set),\n+            activation_fn=None,\n+            weights_initializer=slim.initializers.xavier_initializer(),\n+            weights_regularizer=slim.l2_regularizer(args.weight_decay),\n+            scope="Logits",\n+            reuse=False,\n+        )\n \n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n+        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name="embeddings")\n \n         # Norm for the prelogits\n         eps = 1e-4\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n+        prelogits_norm = tf.reduce_mean(\n+            tf.norm(tf.abs(prelogits) + eps, ord=args.prelogits_norm_p, axis=1)\n+        )\n+        tf.add_to_collection(\n+            tf.GraphKeys.REGULARIZATION_LOSSES,\n+            prelogits_norm * args.prelogits_norm_loss_factor,\n+        )\n \n         # Add center loss\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n+        prelogits_center_loss, _ = facenet.center_loss(\n+            prelogits, label_batch, args.center_loss_alfa, nrof_classes\n+        )\n+        tf.add_to_collection(\n+            tf.GraphKeys.REGULARIZATION_LOSSES,\n+            prelogits_center_loss * args.center_loss_factor,\n+        )\n \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n+        learning_rate = tf.train.exponential_decay(\n+            learning_rate_placeholder,\n+            global_step,\n+            args.learning_rate_decay_epochs * args.epoch_size,\n+            args.learning_rate_decay_factor,\n+            staircase=True,\n+        )\n+        tf.summary.scalar("learning_rate", learning_rate)\n \n         # Calculate the average cross entropy loss across the batch\n         cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n-            labels=label_batch, logits=logits, name=\'cross_entropy_per_example\')\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n-        tf.add_to_collection(\'losses\', cross_entropy_mean)\n-        \n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\n+            labels=label_batch, logits=logits, name="cross_entropy_per_example"\n+        )\n+        cross_entropy_mean = tf.reduce_mean(cross_entropy, name="cross_entropy")\n+        tf.add_to_collection("losses", cross_entropy_mean)\n+\n+        correct_prediction = tf.cast(\n+            tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32\n+        )\n         accuracy = tf.reduce_mean(correct_prediction)\n-        \n+\n         # Calculate the total losses\n         regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n+        total_loss = tf.add_n(\n+            [cross_entropy_mean] + regularization_losses, name="total_loss"\n+        )\n \n         # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\n-        \n+        train_op = facenet.train(\n+            total_loss,\n+            global_step,\n+            args.optimizer,\n+            learning_rate,\n+            args.moving_average_decay,\n+            tf.global_variables(),\n+            args.log_histograms,\n+        )\n+\n         # Create a saver\n         saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n \n@@ -188,8 +247,12 @@ def main(args):\n         summary_op = tf.summary.merge_all()\n \n         # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n+        gpu_options = tf.GPUOptions(\n+            per_process_gpu_memory_fraction=args.gpu_memory_fraction\n+        )\n+        sess = tf.Session(\n+            config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False)\n+        )\n         sess.run(tf.global_variables_initializer())\n         sess.run(tf.local_variables_initializer())\n         summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n@@ -199,85 +262,162 @@ def main(args):\n         with sess.as_default():\n \n             if pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % pretrained_model)\n+                print("Restoring pretrained model: %s" % pretrained_model)\n                 saver.restore(sess, pretrained_model)\n \n             # Training and validation loop\n-            print(\'Running training\')\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\n+            print("Running training")\n+            nrof_steps = args.max_nrof_epochs * args.epoch_size\n+            nrof_val_samples = int(\n+                math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs)\n+            )  # Validate every validate_every_n_epochs as well as in the last epoch\n             stat = {\n-                \'loss\': np.zeros((nrof_steps,), np.float32),\n-                \'center_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'reg_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'xent_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'prelogits_norm\': np.zeros((nrof_steps,), np.float32),\n-                \'accuracy\': np.zeros((nrof_steps,), np.float32),\n-                \'val_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_xent_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_accuracy\': np.zeros((nrof_val_samples,), np.float32),\n-                \'lfw_accuracy\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'lfw_valrate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'learning_rate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_train\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_validate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_evaluate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'prelogits_hist\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\n-              }\n-            for epoch in range(1,args.max_nrof_epochs+1):\n+                "loss": np.zeros((nrof_steps,), np.float32),\n+                "center_loss": np.zeros((nrof_steps,), np.float32),\n+                "reg_loss": np.zeros((nrof_steps,), np.float32),\n+                "xent_loss": np.zeros((nrof_steps,), np.float32),\n+                "prelogits_norm": np.zeros((nrof_steps,), np.float32),\n+                "accuracy": np.zeros((nrof_steps,), np.float32),\n+                "val_loss": np.zeros((nrof_val_samples,), np.float32),\n+                "val_xent_loss": np.zeros((nrof_val_samples,), np.float32),\n+                "val_accuracy": np.zeros((nrof_val_samples,), np.float32),\n+                "lfw_accuracy": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "lfw_valrate": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "learning_rate": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "time_train": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "time_validate": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "time_evaluate": np.zeros((args.max_nrof_epochs,), np.float32),\n+                "prelogits_hist": np.zeros((args.max_nrof_epochs, 1000), np.float32),\n+            }\n+            for epoch in range(1, args.max_nrof_epochs + 1):\n                 step = sess.run(global_step, feed_dict=None)\n                 # Train for one epoch\n                 t = time.time()\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\n-                stat[\'time_train\'][epoch-1] = time.time() - t\n-                \n+                cont = train(\n+                    args,\n+                    sess,\n+                    epoch,\n+                    image_list,\n+                    label_list,\n+                    index_dequeue_op,\n+                    enqueue_op,\n+                    image_paths_placeholder,\n+                    labels_placeholder,\n+                    learning_rate_placeholder,\n+                    phase_train_placeholder,\n+                    batch_size_placeholder,\n+                    control_placeholder,\n+                    global_step,\n+                    total_loss,\n+                    train_op,\n+                    summary_op,\n+                    summary_writer,\n+                    regularization_losses,\n+                    args.learning_rate_schedule_file,\n+                    stat,\n+                    cross_entropy_mean,\n+                    accuracy,\n+                    learning_rate,\n+                    prelogits,\n+                    prelogits_center_loss,\n+                    args.random_rotate,\n+                    args.random_crop,\n+                    args.random_flip,\n+                    prelogits_norm,\n+                    args.prelogits_hist_max,\n+                    args.use_fixed_image_standardization,\n+                )\n+                stat["time_train"][epoch - 1] = time.time() - t\n+\n                 if not cont:\n                     break\n-                  \n+\n                 t = time.time()\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-                        phase_train_placeholder, batch_size_placeholder, \n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\n-                stat[\'time_validate\'][epoch-1] = time.time() - t\n+                if len(val_image_list) > 0 and (\n+                    (epoch - 1) % args.validate_every_n_epochs\n+                    == args.validate_every_n_epochs - 1\n+                    or epoch == args.max_nrof_epochs\n+                ):\n+                    validate(\n+                        args,\n+                        sess,\n+                        epoch,\n+                        val_image_list,\n+                        val_label_list,\n+                        enqueue_op,\n+                        image_paths_placeholder,\n+                        labels_placeholder,\n+                        control_placeholder,\n+                        phase_train_placeholder,\n+                        batch_size_placeholder,\n+                        stat,\n+                        total_loss,\n+                        regularization_losses,\n+                        cross_entropy_mean,\n+                        accuracy,\n+                        args.validate_every_n_epochs,\n+                        args.use_fixed_image_standardization,\n+                    )\n+                stat["time_validate"][epoch - 1] = time.time() - t\n \n                 # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\n+                save_variables_and_metagraph(\n+                    sess, saver, summary_writer, model_dir, subdir, epoch\n+                )\n \n                 # Evaluate on LFW\n                 t = time.time()\n                 if args.lfw_dir:\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\n-                stat[\'time_evaluate\'][epoch-1] = time.time() - t\n+                    evaluate(\n+                        sess,\n+                        enqueue_op,\n+                        image_paths_placeholder,\n+                        labels_placeholder,\n+                        phase_train_placeholder,\n+                        batch_size_placeholder,\n+                        control_placeholder,\n+                        embeddings,\n+                        label_batch,\n+                        lfw_paths,\n+                        actual_issame,\n+                        args.lfw_batch_size,\n+                        args.lfw_nrof_folds,\n+                        log_dir,\n+                        step,\n+                        summary_writer,\n+                        stat,\n+                        epoch,\n+                        args.lfw_distance_metric,\n+                        args.lfw_subtract_mean,\n+                        args.lfw_use_flipped_images,\n+                        args.use_fixed_image_standardization,\n+                    )\n+                stat["time_evaluate"][epoch - 1] = time.time() - t\n \n-                print(\'Saving statistics\')\n-                with h5py.File(stat_file_name, \'w\') as f:\n+                print("Saving statistics")\n+                with h5py.File(stat_file_name, "w") as f:\n                     for key, value in stat.iteritems():\n                         f.create_dataset(key, data=value)\n-    \n+\n     return model_dir\n-  \n+\n+\n def find_threshold(var, percentile):\n     hist, bin_edges = np.histogram(var, 100)\n     cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n-    #plt.plot(bin_centers, cdf)\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\n+    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n+    # plt.plot(bin_centers, cdf)\n+    threshold = np.interp(percentile * 0.01, cdf, bin_centers)\n     return threshold\n-  \n+\n+\n def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\n-    with h5py.File(data_filename,\'r\') as f:\n-        distance_to_center = np.array(f.get(\'distance_to_center\'))\n-        label_list = np.array(f.get(\'label_list\'))\n-        image_list = np.array(f.get(\'image_list\'))\n+    with h5py.File(data_filename, "r") as f:\n+        distance_to_center = np.array(f.get("distance_to_center"))\n+        label_list = np.array(f.get("label_list"))\n+        image_list = np.array(f.get("image_list"))\n         distance_to_center_threshold = find_threshold(distance_to_center, percentile)\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\n+        indices = np.where(distance_to_center >= distance_to_center_threshold)[0]\n         filtered_dataset = dataset\n         removelist = []\n         for i in indices:\n@@ -285,89 +425,215 @@ def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class\n             image = image_list[i]\n             if image in filtered_dataset[label].image_paths:\n                 filtered_dataset[label].image_paths.remove(image)\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\n+            if len(filtered_dataset[label].image_paths) < min_nrof_images_per_class:\n                 removelist.append(label)\n \n         ix = sorted(list(set(removelist)), reverse=True)\n         for i in ix:\n-            del(filtered_dataset[i])\n+            del filtered_dataset[i]\n \n     return filtered_dataset\n-  \n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \n-      stat, cross_entropy_mean, accuracy, \n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\n+\n+\n+def train(\n+    args,\n+    sess,\n+    epoch,\n+    image_list,\n+    label_list,\n+    index_dequeue_op,\n+    enqueue_op,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    learning_rate_placeholder,\n+    phase_train_placeholder,\n+    batch_size_placeholder,\n+    control_placeholder,\n+    step,\n+    loss,\n+    train_op,\n+    summary_op,\n+    summary_writer,\n+    reg_losses,\n+    learning_rate_schedule_file,\n+    stat,\n+    cross_entropy_mean,\n+    accuracy,\n+    learning_rate,\n+    prelogits,\n+    prelogits_center_loss,\n+    random_rotate,\n+    random_crop,\n+    random_flip,\n+    prelogits_norm,\n+    prelogits_hist_max,\n+    use_fixed_image_standardization,\n+):\n     batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n+\n+    if args.learning_rate > 0.0:\n         lr = args.learning_rate\n     else:\n         lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-        \n-    if lr<=0:\n-        return False \n+\n+    if lr <= 0:\n+        return False\n \n     index_epoch = sess.run(index_dequeue_op)\n     label_epoch = np.array(label_list)[index_epoch]\n     image_epoch = np.array(image_list)[index_epoch]\n-    \n+\n     # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n+    labels_array = np.expand_dims(np.array(label_epoch), 1)\n+    image_paths_array = np.expand_dims(np.array(image_epoch), 1)\n+    control_value = (\n+        facenet.RANDOM_ROTATE * random_rotate\n+        + facenet.RANDOM_CROP * random_crop\n+        + facenet.RANDOM_FLIP * random_flip\n+        + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n+    )\n     control_array = np.ones_like(labels_array) * control_value\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n+    sess.run(\n+        enqueue_op,\n+        {\n+            image_paths_placeholder: image_paths_array,\n+            labels_placeholder: labels_array,\n+            control_placeholder: control_array,\n+        },\n+    )\n \n     # Training loop\n     train_time = 0\n     while batch_number < args.epoch_size:\n         start_time = time.time()\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\n+        feed_dict = {\n+            learning_rate_placeholder: lr,\n+            phase_train_placeholder: True,\n+            batch_size_placeholder: args.batch_size,\n+        }\n+        tensor_list = [\n+            loss,\n+            train_op,\n+            step,\n+            reg_losses,\n+            prelogits,\n+            cross_entropy_mean,\n+            learning_rate,\n+            prelogits_norm,\n+            accuracy,\n+            prelogits_center_loss,\n+        ]\n         if batch_number % 100 == 0:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\n+            (\n+                loss_,\n+                _,\n+                step_,\n+                reg_losses_,\n+                prelogits_,\n+                cross_entropy_mean_,\n+                lr_,\n+                prelogits_norm_,\n+                accuracy_,\n+                center_loss_,\n+                summary_str,\n+            ) = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\n             summary_writer.add_summary(summary_str, global_step=step_)\n         else:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\n-         \n+            (\n+                loss_,\n+                _,\n+                step_,\n+                reg_losses_,\n+                prelogits_,\n+                cross_entropy_mean_,\n+                lr_,\n+                prelogits_norm_,\n+                accuracy_,\n+                center_loss_,\n+            ) = sess.run(tensor_list, feed_dict=feed_dict)\n+\n         duration = time.time() - start_time\n-        stat[\'loss\'][step_-1] = loss_\n-        stat[\'center_loss\'][step_-1] = center_loss_\n-        stat[\'reg_loss\'][step_-1] = np.sum(reg_losses_)\n-        stat[\'xent_loss\'][step_-1] = cross_entropy_mean_\n-        stat[\'prelogits_norm\'][step_-1] = prelogits_norm_\n-        stat[\'learning_rate\'][epoch-1] = lr_\n-        stat[\'accuracy\'][step_-1] = accuracy_\n-        stat[\'prelogits_hist\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\n-        \n+        stat["loss"][step_ - 1] = loss_\n+        stat["center_loss"][step_ - 1] = center_loss_\n+        stat["reg_loss"][step_ - 1] = np.sum(reg_losses_)\n+        stat["xent_loss"][step_ - 1] = cross_entropy_mean_\n+        stat["prelogits_norm"][step_ - 1] = prelogits_norm_\n+        stat["learning_rate"][epoch - 1] = lr_\n+        stat["accuracy"][step_ - 1] = accuracy_\n+        stat["prelogits_hist"][epoch - 1, :] += np.histogram(\n+            np.minimum(np.abs(prelogits_), prelogits_hist_max),\n+            bins=1000,\n+            range=(0.0, prelogits_hist_max),\n+        )[0]\n+\n         duration = time.time() - start_time\n-        print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tRegLoss %2.3f\\tAccuracy %2.3f\\tLr %2.5f\\tCl %2.3f\' %\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\n+        print(\n+            "Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tRegLoss %2.3f\\tAccuracy %2.3f\\tLr %2.5f\\tCl %2.3f"\n+            % (\n+                epoch,\n+                batch_number + 1,\n+                args.epoch_size,\n+                duration,\n+                loss_,\n+                cross_entropy_mean_,\n+                np.sum(reg_losses_),\n+                accuracy_,\n+                lr_,\n+                center_loss_,\n+            )\n+        )\n         batch_number += 1\n         train_time += duration\n     # Add validation loss and accuracy to summary\n     summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/total\', simple_value=train_time)\n+    # pylint: disable=maybe-no-member\n+    summary.value.add(tag="time/total", simple_value=train_time)\n     summary_writer.add_summary(summary, global_step=step_)\n     return True\n \n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-             phase_train_placeholder, batch_size_placeholder, \n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\n-  \n-    print(\'Running forward pass on validation set\')\n+\n+def validate(\n+    args,\n+    sess,\n+    epoch,\n+    image_list,\n+    label_list,\n+    enqueue_op,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    control_placeholder,\n+    phase_train_placeholder,\n+    batch_size_placeholder,\n+    stat,\n+    loss,\n+    regularization_losses,\n+    cross_entropy_mean,\n+    accuracy,\n+    validate_every_n_epochs,\n+    use_fixed_image_standardization,\n+):\n+\n+    print("Running forward pass on validation set")\n \n     nrof_batches = len(label_list) // args.lfw_batch_size\n     nrof_images = nrof_batches * args.lfw_batch_size\n-    \n+\n     # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n+    labels_array = np.expand_dims(np.array(label_list[:nrof_images]), 1)\n+    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]), 1)\n+    control_array = (\n+        np.ones_like(labels_array, np.int32)\n+        * facenet.FIXED_STANDARDIZATION\n+        * use_fixed_image_standardization\n+    )\n+    sess.run(\n+        enqueue_op,\n+        {\n+            image_paths_placeholder: image_paths_array,\n+            labels_placeholder: labels_array,\n+            control_placeholder: control_array,\n+        },\n+    )\n \n     loss_array = np.zeros((nrof_batches,), np.float32)\n     xent_array = np.zeros((nrof_batches,), np.float32)\n@@ -376,206 +642,416 @@ def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_\n     # Training loop\n     start_time = time.time()\n     for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\n+        feed_dict = {\n+            phase_train_placeholder: False,\n+            batch_size_placeholder: args.lfw_batch_size,\n+        }\n+        loss_, cross_entropy_mean_, accuracy_ = sess.run(\n+            [loss, cross_entropy_mean, accuracy], feed_dict=feed_dict\n+        )\n+        loss_array[i], xent_array[i], accuracy_array[i] = (\n+            loss_,\n+            cross_entropy_mean_,\n+            accuracy_,\n+        )\n         if i % 10 == 9:\n-            print(\'.\', end=\'\')\n+            print(".", end="")\n             sys.stdout.flush()\n-    print(\'\')\n+    print("")\n \n     duration = time.time() - start_time\n \n-    val_index = (epoch-1)//validate_every_n_epochs\n-    stat[\'val_loss\'][val_index] = np.mean(loss_array)\n-    stat[\'val_xent_loss\'][val_index] = np.mean(xent_array)\n-    stat[\'val_accuracy\'][val_index] = np.mean(accuracy_array)\n+    val_index = (epoch - 1) // validate_every_n_epochs\n+    stat["val_loss"][val_index] = np.mean(loss_array)\n+    stat["val_xent_loss"][val_index] = np.mean(xent_array)\n+    stat["val_accuracy"][val_index] = np.mean(accuracy_array)\n \n-    print(\'Validation Epoch: %d\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tAccuracy %2.3f\' %\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\n+    print(\n+        "Validation Epoch: %d\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tAccuracy %2.3f"\n+        % (\n+            epoch,\n+            duration,\n+            np.mean(loss_array),\n+            np.mean(xent_array),\n+            np.mean(accuracy_array),\n+        )\n+    )\n \n \n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n+def evaluate(\n+    sess,\n+    enqueue_op,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    phase_train_placeholder,\n+    batch_size_placeholder,\n+    control_placeholder,\n+    embeddings,\n+    labels,\n+    image_paths,\n+    actual_issame,\n+    batch_size,\n+    nrof_folds,\n+    log_dir,\n+    step,\n+    summary_writer,\n+    stat,\n+    epoch,\n+    distance_metric,\n+    subtract_mean,\n+    use_flipped_images,\n+    use_fixed_image_standardization,\n+):\n     start_time = time.time()\n     # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n+    print("Runnning forward pass on LFW images")\n+\n     # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n+    nrof_embeddings = len(actual_issame) * 2  # nrof_pairs * nrof_images_per_pair\n     nrof_flips = 2 if use_flipped_images else 1\n     nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n+    labels_array = np.expand_dims(np.arange(0, nrof_images), 1)\n+    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths), nrof_flips), 1)\n     control_array = np.zeros_like(labels_array, np.int32)\n     if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n+        control_array += np.ones_like(labels_array) * facenet.FIXED_STANDARDIZATION\n     if use_flipped_images:\n         # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n+        control_array += (labels_array % 2) * facenet.FLIP\n+    sess.run(\n+        enqueue_op,\n+        {\n+            image_paths_placeholder: image_paths_array,\n+            labels_placeholder: labels_array,\n+            control_placeholder: control_array,\n+        },\n+    )\n+\n     embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n+    assert (\n+        nrof_images % batch_size == 0\n+    ), "The number of LFW images must be an integer multiple of the LFW batch size"\n     nrof_batches = nrof_images // batch_size\n     emb_array = np.zeros((nrof_images, embedding_size))\n     lab_array = np.zeros((nrof_images,))\n     for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n+        feed_dict = {phase_train_placeholder: False, batch_size_placeholder: batch_size}\n         emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n         lab_array[lab] = lab\n         emb_array[lab, :] = emb\n         if i % 10 == 9:\n-            print(\'.\', end=\'\')\n+            print(".", end="")\n             sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n+    print("")\n+    embeddings = np.zeros((nrof_embeddings, embedding_size * nrof_flips))\n     if use_flipped_images:\n         # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n+        embeddings[:, :embedding_size] = emb_array[0::2, :]\n+        embeddings[:, embedding_size:] = emb_array[1::2, :]\n     else:\n         embeddings = emb_array\n \n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n+    assert (\n+        np.array_equal(lab_array, np.arange(nrof_images)) == True\n+    ), "Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline"\n+    _, _, accuracy, val, val_std, far = lfw.evaluate(\n+        embeddings,\n+        actual_issame,\n+        nrof_folds=nrof_folds,\n+        distance_metric=distance_metric,\n+        subtract_mean=subtract_mean,\n+    )\n+\n+    print("Accuracy: %2.5f+-%2.5f" % (np.mean(accuracy), np.std(accuracy)))\n+    print("Validation rate: %2.5f+-%2.5f @ FAR=%2.5f" % (val, val_std, far))\n     lfw_time = time.time() - start_time\n     # Add validation loss and accuracy to summary\n     summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n+    # pylint: disable=maybe-no-member\n+    summary.value.add(tag="lfw/accuracy", simple_value=np.mean(accuracy))\n+    summary.value.add(tag="lfw/val_rate", simple_value=val)\n+    summary.value.add(tag="time/lfw", simple_value=lfw_time)\n     summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n-    stat[\'lfw_accuracy\'][epoch-1] = np.mean(accuracy)\n-    stat[\'lfw_valrate\'][epoch-1] = val\n+    with open(os.path.join(log_dir, "lfw_result.txt"), "at") as f:\n+        f.write("%d\\t%.5f\\t%.5f\\n" % (step, np.mean(accuracy), val))\n+    stat["lfw_accuracy"][epoch - 1] = np.mean(accuracy)\n+    stat["lfw_valrate"][epoch - 1] = val\n+\n \n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n+def save_variables_and_metagraph(\n+    sess, saver, summary_writer, model_dir, model_name, step\n+):\n     # Save the model checkpoint\n-    print(\'Saving variables\')\n+    print("Saving variables")\n     start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n+    checkpoint_path = os.path.join(model_dir, "model-%s.ckpt" % model_name)\n     saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n     save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n+    print("Variables saved in %.2f seconds" % save_time_variables)\n+    metagraph_filename = os.path.join(model_dir, "model-%s.meta" % model_name)\n+    save_time_metagraph = 0\n     if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n+        print("Saving metagraph")\n         start_time = time.time()\n         saver.export_meta_graph(metagraph_filename)\n         save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n+        print("Metagraph saved in %.2f seconds" % save_time_metagraph)\n     summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n+    # pylint: disable=maybe-no-member\n+    summary.value.add(tag="time/save_variables", simple_value=save_time_variables)\n+    summary.value.add(tag="time/save_metagraph", simple_value=save_time_metagraph)\n     summary_writer.add_summary(summary, step)\n-  \n+\n \n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--random_rotate\', \n-        help=\'Performs random rotations of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--center_loss_factor\', type=float,\n-        help=\'Center loss factor.\', default=0.0)\n-    parser.add_argument(\'--center_loss_alfa\', type=float,\n-        help=\'Center update rate for center loss.\', default=0.95)\n-    parser.add_argument(\'--prelogits_norm_loss_factor\', type=float,\n-        help=\'Loss based on the norm of the activations in the prelogits layer.\', default=0.0)\n-    parser.add_argument(\'--prelogits_norm_p\', type=float,\n-        help=\'Norm to use for prelogits norm loss.\', default=1.0)\n-    parser.add_argument(\'--prelogits_hist_max\', type=float,\n-        help=\'The max value for the prelogits histogram.\', default=10.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--nrof_preprocess_threads\', type=int,\n-        help=\'Number of preprocessing (data loading and augmentation) threads.\', default=4)\n-    parser.add_argument(\'--log_histograms\', \n-        help=\'Enables logging of weight/bias histograms in tensorboard.\', action=\'store_true\')\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-    parser.add_argument(\'--filter_filename\', type=str,\n-        help=\'File containing image data used for dataset filtering\', default=\'\')\n-    parser.add_argument(\'--filter_percentile\', type=float,\n-        help=\'Keep only the percentile images closed to its class center\', default=100.0)\n-    parser.add_argument(\'--filter_min_nrof_images_per_class\', type=int,\n-        help=\'Keep only the classes with this number of examples or more\', default=0)\n-    parser.add_argument(\'--validate_every_n_epochs\', type=int,\n-        help=\'Number of epoch between validation\', default=5)\n-    parser.add_argument(\'--validation_set_split_ratio\', type=float,\n-        help=\'The ratio of the total dataset to use for validation\', default=0.0)\n-    parser.add_argument(\'--min_nrof_val_images_per_class\', type=float,\n-        help=\'Classes with fewer images will be removed from the validation set\', default=0)\n- \n+\n+    parser.add_argument(\n+        "--logs_base_dir",\n+        type=str,\n+        help="Directory where to write event logs.",\n+        default="~/logs/facenet",\n+    )\n+    parser.add_argument(\n+        "--models_base_dir",\n+        type=str,\n+        help="Directory where to write trained models and checkpoints.",\n+        default="~/models/facenet",\n+    )\n+    parser.add_argument(\n+        "--gpu_memory_fraction",\n+        type=float,\n+        help="Upper bound on the amount of GPU memory that will be used by the process.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--pretrained_model",\n+        type=str,\n+        help="Load a pretrained model before training starts.",\n+    )\n+    parser.add_argument(\n+        "--data_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned face patches.",\n+        default="~/datasets/casia/casia_maxpy_mtcnnalign_182_160",\n+    )\n+    parser.add_argument(\n+        "--model_def",\n+        type=str,\n+        help="Model definition. Points to a module containing the definition of the inference graph.",\n+        default="models.inception_resnet_v1",\n+    )\n+    parser.add_argument(\n+        "--max_nrof_epochs", type=int, help="Number of epochs to run.", default=500\n+    )\n+    parser.add_argument(\n+        "--batch_size",\n+        type=int,\n+        help="Number of images to process in a batch.",\n+        default=90,\n+    )\n+    parser.add_argument(\n+        "--image_size",\n+        type=int,\n+        help="Image size (height, width) in pixels.",\n+        default=160,\n+    )\n+    parser.add_argument(\n+        "--epoch_size", type=int, help="Number of batches per epoch.", default=1000\n+    )\n+    parser.add_argument(\n+        "--embedding_size",\n+        type=int,\n+        help="Dimensionality of the embedding.",\n+        default=128,\n+    )\n+    parser.add_argument(\n+        "--random_crop",\n+        help="Performs random cropping of training images. If false, the center image_size pixels from the training images are used. "\n+        + "If the size of the images in the data directory is equal to image_size no cropping is performed",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--random_flip",\n+        help="Performs random horizontal flipping of training images.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--random_rotate",\n+        help="Performs random rotations of training images.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--use_fixed_image_standardization",\n+        help="Performs fixed standardization of images.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--keep_probability",\n+        type=float,\n+        help="Keep probability of dropout for the fully connected layer(s).",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--weight_decay", type=float, help="L2 weight regularization.", default=0.0\n+    )\n+    parser.add_argument(\n+        "--center_loss_factor", type=float, help="Center loss factor.", default=0.0\n+    )\n+    parser.add_argument(\n+        "--center_loss_alfa",\n+        type=float,\n+        help="Center update rate for center loss.",\n+        default=0.95,\n+    )\n+    parser.add_argument(\n+        "--prelogits_norm_loss_factor",\n+        type=float,\n+        help="Loss based on the norm of the activations in the prelogits layer.",\n+        default=0.0,\n+    )\n+    parser.add_argument(\n+        "--prelogits_norm_p",\n+        type=float,\n+        help="Norm to use for prelogits norm loss.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--prelogits_hist_max",\n+        type=float,\n+        help="The max value for the prelogits histogram.",\n+        default=10.0,\n+    )\n+    parser.add_argument(\n+        "--optimizer",\n+        type=str,\n+        choices=["ADAGRAD", "ADADELTA", "ADAM", "RMSPROP", "MOM"],\n+        help="The optimization algorithm to use",\n+        default="ADAGRAD",\n+    )\n+    parser.add_argument(\n+        "--learning_rate",\n+        type=float,\n+        help="Initial learning rate. If set to a negative value a learning rate "\n+        + \'schedule can be specified in the file "learning_rate_schedule.txt"\',\n+        default=0.1,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_epochs",\n+        type=int,\n+        help="Number of epochs between learning rate decay.",\n+        default=100,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_factor",\n+        type=float,\n+        help="Learning rate decay factor.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--moving_average_decay",\n+        type=float,\n+        help="Exponential decay for tracking of training parameters.",\n+        default=0.9999,\n+    )\n+    parser.add_argument("--seed", type=int, help="Random seed.", default=666)\n+    parser.add_argument(\n+        "--nrof_preprocess_threads",\n+        type=int,\n+        help="Number of preprocessing (data loading and augmentation) threads.",\n+        default=4,\n+    )\n+    parser.add_argument(\n+        "--log_histograms",\n+        help="Enables logging of weight/bias histograms in tensorboard.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--learning_rate_schedule_file",\n+        type=str,\n+        help="File containing the learning rate schedule that is used when learning_rate is set to to -1.",\n+        default="data/learning_rate_schedule.txt",\n+    )\n+    parser.add_argument(\n+        "--filter_filename",\n+        type=str,\n+        help="File containing image data used for dataset filtering",\n+        default="",\n+    )\n+    parser.add_argument(\n+        "--filter_percentile",\n+        type=float,\n+        help="Keep only the percentile images closed to its class center",\n+        default=100.0,\n+    )\n+    parser.add_argument(\n+        "--filter_min_nrof_images_per_class",\n+        type=int,\n+        help="Keep only the classes with this number of examples or more",\n+        default=0,\n+    )\n+    parser.add_argument(\n+        "--validate_every_n_epochs",\n+        type=int,\n+        help="Number of epoch between validation",\n+        default=5,\n+    )\n+    parser.add_argument(\n+        "--validation_set_split_ratio",\n+        type=float,\n+        help="The ratio of the total dataset to use for validation",\n+        default=0.0,\n+    )\n+    parser.add_argument(\n+        "--min_nrof_val_images_per_class",\n+        type=float,\n+        help="Classes with fewer images will be removed from the validation set",\n+        default=0,\n+    )\n+\n     # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--lfw_distance_metric\', type=int,\n-        help=\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\', default=0)\n-    parser.add_argument(\'--lfw_use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--lfw_subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n+    parser.add_argument(\n+        "--lfw_pairs",\n+        type=str,\n+        help="The file containing the pairs to use for validation.",\n+        default="data/pairs.txt",\n+    )\n+    parser.add_argument(\n+        "--lfw_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned face patches.",\n+        default="",\n+    )\n+    parser.add_argument(\n+        "--lfw_batch_size",\n+        type=int,\n+        help="Number of images to process in a batch in the LFW test set.",\n+        default=100,\n+    )\n+    parser.add_argument(\n+        "--lfw_nrof_folds",\n+        type=int,\n+        help="Number of folds to use for cross validation. Mainly used for testing.",\n+        default=10,\n+    )\n+    parser.add_argument(\n+        "--lfw_distance_metric",\n+        type=int,\n+        help="Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.",\n+        default=0,\n+    )\n+    parser.add_argument(\n+        "--lfw_use_flipped_images",\n+        help="Concatenates embeddings for the image and its horizontally flipped counterpart.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--lfw_subtract_mean",\n+        help="Subtract feature mean before calculating distance.",\n+        action="store_true",\n+    )\n     return parser.parse_args(argv)\n-  \n \n-if __name__ == \'__main__\':\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/train_tripletloss.py b/model/src/train_tripletloss.py\nindex b2180bd..71a6c97 100644\n--- a/model/src/train_tripletloss.py\n+++ b/model/src/train_tripletloss.py\n@@ -2,19 +2,19 @@\n FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -23,32 +23,30 @@ FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.o\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-from datetime import datetime\n-import os.path\n-import time\n-import sys\n-import tensorflow as tf\n-import numpy as np\n+import argparse\n import importlib\n import itertools\n-import argparse\n-from model.src import facenet_config as facenet\n+import os.path\n+import sys\n+import time\n+from datetime import datetime\n \n import lfw\n-\n+import numpy as np\n+import tensorflow as tf\n+from six.moves import xrange  # @UnresolvedImport\n from tensorflow.python.ops import data_flow_ops\n \n-from six.moves import xrange  # @UnresolvedImport\n+from model.src import facenet_config as facenet\n+\n \n def main(args):\n-  \n+\n     network = importlib.import_module(args.model_def)\n \n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n+    subdir = datetime.strftime(datetime.now(), "%Y%m%d-%H%M%S")\n     log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n     if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n         os.makedirs(log_dir)\n@@ -57,48 +55,56 @@ def main(args):\n         os.makedirs(model_dir)\n \n     # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n+    facenet.write_arguments_to_file(args, os.path.join(log_dir, "arguments.txt"))\n+\n     # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n+    src_path, _ = os.path.split(os.path.realpath(__file__))\n+    facenet.store_revision_info(src_path, log_dir, " ".join(sys.argv))\n \n     np.random.seed(seed=args.seed)\n     train_set = facenet.get_dataset(args.data_dir)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n+\n+    print("Model directory: %s" % model_dir)\n+    print("Log directory: %s" % log_dir)\n     if args.pretrained_model:\n-        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n-    \n+        print("Pre-trained model: %s" % os.path.expanduser(args.pretrained_model))\n+\n     if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n+        print("LFW directory: %s" % args.lfw_dir)\n         # Read the file containing the pairs used for testing\n         pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n         # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-        \n-    \n+        lfw_paths, actual_issame = lfw.get_paths(\n+            os.path.expanduser(args.lfw_dir), pairs\n+        )\n+\n     with tf.Graph().as_default():\n         tf.set_random_seed(args.seed)\n         global_step = tf.Variable(0, trainable=False)\n \n         # Placeholder for the learning rate\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        \n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        \n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        \n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n-        \n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n-                                    dtypes=[tf.string, tf.int64],\n-                                    shapes=[(3,), (3,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n-        \n+        learning_rate_placeholder = tf.placeholder(tf.float32, name="learning_rate")\n+\n+        batch_size_placeholder = tf.placeholder(tf.int32, name="batch_size")\n+\n+        phase_train_placeholder = tf.placeholder(tf.bool, name="phase_train")\n+\n+        image_paths_placeholder = tf.placeholder(\n+            tf.string, shape=(None, 3), name="image_paths"\n+        )\n+        labels_placeholder = tf.placeholder(tf.int64, shape=(None, 3), name="labels")\n+\n+        input_queue = data_flow_ops.FIFOQueue(\n+            capacity=100000,\n+            dtypes=[tf.string, tf.int64],\n+            shapes=[(3,), (3,)],\n+            shared_name=None,\n+            name=None,\n+        )\n+        enqueue_op = input_queue.enqueue_many(\n+            [image_paths_placeholder, labels_placeholder]\n+        )\n+\n         nrof_preprocess_threads = 4\n         images_and_labels = []\n         for _ in range(nrof_preprocess_threads):\n@@ -107,50 +113,72 @@ def main(args):\n             for filename in tf.unstack(filenames):\n                 file_contents = tf.read_file(filename)\n                 image = tf.image.decode_image(file_contents, channels=3)\n-                \n+\n                 if args.random_crop:\n                     image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n                 else:\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n+                    image = tf.image.resize_image_with_crop_or_pad(\n+                        image, args.image_size, args.image_size\n+                    )\n                 if args.random_flip:\n                     image = tf.image.random_flip_left_right(image)\n-    \n-                #pylint: disable=no-member\n+\n+                # pylint: disable=no-member\n                 image.set_shape((args.image_size, args.image_size, 3))\n                 images.append(tf.image.per_image_standardization(image))\n             images_and_labels.append([images, label])\n-    \n+\n         image_batch, labels_batch = tf.train.batch_join(\n-            images_and_labels, batch_size=batch_size_placeholder, \n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n+            images_and_labels,\n+            batch_size=batch_size_placeholder,\n+            shapes=[(args.image_size, args.image_size, 3), ()],\n+            enqueue_many=True,\n             capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        labels_batch = tf.identity(labels_batch, \'label_batch\')\n+            allow_smaller_final_batch=True,\n+        )\n+        image_batch = tf.identity(image_batch, "image_batch")\n+        image_batch = tf.identity(image_batch, "input")\n+        labels_batch = tf.identity(labels_batch, "label_batch")\n \n         # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n-            weight_decay=args.weight_decay)\n-        \n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n+        prelogits, _ = network.inference(\n+            image_batch,\n+            args.keep_probability,\n+            phase_train=phase_train_placeholder,\n+            bottleneck_layer_size=args.embedding_size,\n+            weight_decay=args.weight_decay,\n+        )\n+\n+        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name="embeddings")\n         # Split embeddings into anchor, positive and negative and calculate triplet loss\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n+        anchor, positive, negative = tf.unstack(\n+            tf.reshape(embeddings, [-1, 3, args.embedding_size]), 3, 1\n+        )\n         triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n-        \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n+\n+        learning_rate = tf.train.exponential_decay(\n+            learning_rate_placeholder,\n+            global_step,\n+            args.learning_rate_decay_epochs * args.epoch_size,\n+            args.learning_rate_decay_factor,\n+            staircase=True,\n+        )\n+        tf.summary.scalar("learning_rate", learning_rate)\n \n         # Calculate the total losses\n         regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n+        total_loss = tf.add_n([triplet_loss] + regularization_losses, name="total_loss")\n \n         # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables())\n-        \n+        train_op = facenet.train(\n+            total_loss,\n+            global_step,\n+            args.optimizer,\n+            learning_rate,\n+            args.moving_average_decay,\n+            tf.global_variables(),\n+        )\n+\n         # Create a saver\n         saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n \n@@ -158,12 +186,18 @@ def main(args):\n         summary_op = tf.summary.merge_all()\n \n         # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n+        gpu_options = tf.GPUOptions(\n+            per_process_gpu_memory_fraction=args.gpu_memory_fraction\n+        )\n+        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n \n         # Initialize variables\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n+        sess.run(\n+            tf.global_variables_initializer(), feed_dict={phase_train_placeholder: True}\n+        )\n+        sess.run(\n+            tf.local_variables_initializer(), feed_dict={phase_train_placeholder: True}\n+        )\n \n         summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n         coord = tf.train.Coordinator()\n@@ -172,7 +206,7 @@ def main(args):\n         with sess.as_default():\n \n             if args.pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n+                print("Restoring pretrained model: %s" % args.pretrained_model)\n                 saver.restore(sess, os.path.expanduser(args.pretrained_model))\n \n             # Training and validation loop\n@@ -181,66 +215,155 @@ def main(args):\n                 step = sess.run(global_step, feed_dict=None)\n                 epoch = step // args.epoch_size\n                 # Train for one epoch\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\n+                train(\n+                    args,\n+                    sess,\n+                    train_set,\n+                    epoch,\n+                    image_paths_placeholder,\n+                    labels_placeholder,\n+                    labels_batch,\n+                    batch_size_placeholder,\n+                    learning_rate_placeholder,\n+                    phase_train_placeholder,\n+                    enqueue_op,\n+                    input_queue,\n+                    global_step,\n+                    embeddings,\n+                    total_loss,\n+                    train_op,\n+                    summary_op,\n+                    summary_writer,\n+                    args.learning_rate_schedule_file,\n+                    args.embedding_size,\n+                    anchor,\n+                    positive,\n+                    negative,\n+                    triplet_loss,\n+                )\n \n                 # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n+                save_variables_and_metagraph(\n+                    sess, saver, summary_writer, model_dir, subdir, step\n+                )\n \n                 # Evaluate on LFW\n                 if args.lfw_dir:\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n+                    evaluate(\n+                        sess,\n+                        lfw_paths,\n+                        embeddings,\n+                        labels_batch,\n+                        image_paths_placeholder,\n+                        labels_placeholder,\n+                        batch_size_placeholder,\n+                        learning_rate_placeholder,\n+                        phase_train_placeholder,\n+                        enqueue_op,\n+                        actual_issame,\n+                        args.batch_size,\n+                        args.lfw_nrof_folds,\n+                        log_dir,\n+                        step,\n+                        summary_writer,\n+                        args.embedding_size,\n+                    )\n \n     return model_dir\n \n \n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n-          embedding_size, anchor, positive, negative, triplet_loss):\n+def train(\n+    args,\n+    sess,\n+    dataset,\n+    epoch,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    labels_batch,\n+    batch_size_placeholder,\n+    learning_rate_placeholder,\n+    phase_train_placeholder,\n+    enqueue_op,\n+    input_queue,\n+    global_step,\n+    embeddings,\n+    loss,\n+    train_op,\n+    summary_op,\n+    summary_writer,\n+    learning_rate_schedule_file,\n+    embedding_size,\n+    anchor,\n+    positive,\n+    negative,\n+    triplet_loss,\n+):\n     batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n+\n+    if args.learning_rate > 0.0:\n         lr = args.learning_rate\n     else:\n         lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n     while batch_number < args.epoch_size:\n         # Sample people randomly from the dataset\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n-        \n-        print(\'Running forward pass on sampled images: \', end=\'\')\n+        image_paths, num_per_class = sample_people(\n+            dataset, args.people_per_batch, args.images_per_person\n+        )\n+\n+        print("Running forward pass on sampled images: ", end="")\n         start_time = time.time()\n         nrof_examples = args.people_per_batch * args.images_per_person\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n+        labels_array = np.reshape(np.arange(nrof_examples), (-1, 3))\n+        image_paths_array = np.reshape(\n+            np.expand_dims(np.array(image_paths), 1), (-1, 3)\n+        )\n+        sess.run(\n+            enqueue_op,\n+            {\n+                image_paths_placeholder: image_paths_array,\n+                labels_placeholder: labels_array,\n+            },\n+        )\n         emb_array = np.zeros((nrof_examples, embedding_size))\n         nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n         for i in range(nrof_batches):\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\n-            emb_array[lab,:] = emb\n-        print(\'%.3f\' % (time.time()-start_time))\n+            batch_size = min(nrof_examples - i * args.batch_size, args.batch_size)\n+            emb, lab = sess.run(\n+                [embeddings, labels_batch],\n+                feed_dict={\n+                    batch_size_placeholder: batch_size,\n+                    learning_rate_placeholder: lr,\n+                    phase_train_placeholder: True,\n+                },\n+            )\n+            emb_array[lab, :] = emb\n+        print("%.3f" % (time.time() - start_time))\n \n         # Select triplets based on the embeddings\n-        print(\'Selecting suitable triplets for training\')\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n-            image_paths, args.people_per_batch, args.alpha)\n+        print("Selecting suitable triplets for training")\n+        triplets, nrof_random_negs, nrof_triplets = select_triplets(\n+            emb_array, num_per_class, image_paths, args.people_per_batch, args.alpha\n+        )\n         selection_time = time.time() - start_time\n-        print(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n-            (nrof_random_negs, nrof_triplets, selection_time))\n+        print(\n+            "(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds"\n+            % (nrof_random_negs, nrof_triplets, selection_time)\n+        )\n \n         # Perform training on the selected triplets\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n+        nrof_batches = int(np.ceil(nrof_triplets * 3 / args.batch_size))\n         triplet_paths = list(itertools.chain(*triplets))\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n+        labels_array = np.reshape(np.arange(len(triplet_paths)), (-1, 3))\n+        triplet_paths_array = np.reshape(\n+            np.expand_dims(np.array(triplet_paths), 1), (-1, 3)\n+        )\n+        sess.run(\n+            enqueue_op,\n+            {\n+                image_paths_placeholder: triplet_paths_array,\n+                labels_placeholder: labels_array,\n+            },\n+        )\n         nrof_examples = len(triplet_paths)\n         train_time = 0\n         i = 0\n@@ -250,33 +373,45 @@ def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholde\n         step = 0\n         while i < nrof_batches:\n             start_time = time.time()\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n-            emb_array[lab,:] = emb\n+            batch_size = min(nrof_examples - i * args.batch_size, args.batch_size)\n+            feed_dict = {\n+                batch_size_placeholder: batch_size,\n+                learning_rate_placeholder: lr,\n+                phase_train_placeholder: True,\n+            }\n+            err, _, step, emb, lab = sess.run(\n+                [loss, train_op, global_step, embeddings, labels_batch],\n+                feed_dict=feed_dict,\n+            )\n+            emb_array[lab, :] = emb\n             loss_array[i] = err\n             duration = time.time() - start_time\n-            print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\n+            print(\n+                "Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f"\n+                % (epoch, batch_number + 1, args.epoch_size, duration, err)\n+            )\n             batch_number += 1\n             i += 1\n             train_time += duration\n-            summary.value.add(tag=\'loss\', simple_value=err)\n-            \n+            summary.value.add(tag="loss", simple_value=err)\n+\n         # Add validation loss and accuracy to summary\n-        #pylint: disable=maybe-no-member\n-        summary.value.add(tag=\'time/selection\', simple_value=selection_time)\n+        # pylint: disable=maybe-no-member\n+        summary.value.add(tag="time/selection", simple_value=selection_time)\n         summary_writer.add_summary(summary, step)\n     return step\n-  \n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n+\n+\n+def select_triplets(\n+    embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha\n+):\n     """ Select the triplets for training\n     """\n     trip_idx = 0\n     emb_start_idx = 0\n     num_trips = 0\n     triplets = []\n-    \n+\n     # VGG Face: Choosing good triplets is crucial and should strike a balance between\n     #  selecting informative (i.e. challenging) examples and swamping training with examples that\n     #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n@@ -286,21 +421,25 @@ def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_b\n \n     for i in xrange(people_per_batch):\n         nrof_images = int(nrof_images_per_class[i])\n-        for j in xrange(1,nrof_images):\n+        for j in xrange(1, nrof_images):\n             a_idx = emb_start_idx + j - 1\n             neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\n+            for pair in xrange(j, nrof_images):  # For every possible positive pair.\n                 p_idx = emb_start_idx + pair\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n+                pos_dist_sqr = np.sum(np.square(embeddings[a_idx] - embeddings[p_idx]))\n+                neg_dists_sqr[emb_start_idx : emb_start_idx + nrof_images] = np.NaN\n+                # all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n+                all_neg = np.where(neg_dists_sqr - pos_dist_sqr < alpha)[\n+                    0\n+                ]  # VGG Face selecction\n                 nrof_random_negs = all_neg.shape[0]\n-                if nrof_random_negs>0:\n+                if nrof_random_negs > 0:\n                     rnd_idx = np.random.randint(nrof_random_negs)\n                     n_idx = all_neg[rnd_idx]\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n-                    #print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n+                    triplets.append(\n+                        (image_paths[a_idx], image_paths[p_idx], image_paths[n_idx])\n+                    )\n+                    # print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' %\n                     #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n                     trip_idx += 1\n \n@@ -311,177 +450,300 @@ def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_b\n     np.random.shuffle(triplets)\n     return triplets, num_trips, len(triplets)\n \n+\n def sample_people(dataset, people_per_batch, images_per_person):\n     nrof_images = people_per_batch * images_per_person\n-  \n+\n     # Sample classes from the dataset\n     nrof_classes = len(dataset)\n     class_indices = np.arange(nrof_classes)\n     np.random.shuffle(class_indices)\n-    \n+\n     i = 0\n     image_paths = []\n     num_per_class = []\n     sampled_class_indices = []\n     # Sample images from these classes until we have enough\n-    while len(image_paths)<nrof_images:\n+    while len(image_paths) < nrof_images:\n         class_index = class_indices[i]\n         nrof_images_in_class = len(dataset[class_index])\n         image_indices = np.arange(nrof_images_in_class)\n         np.random.shuffle(image_indices)\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n+        nrof_images_from_class = min(\n+            nrof_images_in_class, images_per_person, nrof_images - len(image_paths)\n+        )\n         idx = image_indices[0:nrof_images_from_class]\n         image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n-        sampled_class_indices += [class_index]*nrof_images_from_class\n+        sampled_class_indices += [class_index] * nrof_images_from_class\n         image_paths += image_paths_for_class\n         num_per_class.append(nrof_images_from_class)\n-        i+=1\n-  \n+        i += 1\n+\n     return image_paths, num_per_class\n \n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\n+\n+def evaluate(\n+    sess,\n+    image_paths,\n+    embeddings,\n+    labels_batch,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    batch_size_placeholder,\n+    learning_rate_placeholder,\n+    phase_train_placeholder,\n+    enqueue_op,\n+    actual_issame,\n+    batch_size,\n+    nrof_folds,\n+    log_dir,\n+    step,\n+    summary_writer,\n+    embedding_size,\n+):\n     start_time = time.time()\n     # Run forward pass to calculate embeddings\n-    print(\'Running forward pass on LFW images: \', end=\'\')\n-    \n-    nrof_images = len(actual_issame)*2\n-    assert(len(image_paths)==nrof_images)\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n+    print("Running forward pass on LFW images: ", end="")\n+\n+    nrof_images = len(actual_issame) * 2\n+    assert len(image_paths) == nrof_images\n+    labels_array = np.reshape(np.arange(nrof_images), (-1, 3))\n+    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths), 1), (-1, 3))\n+    sess.run(\n+        enqueue_op,\n+        {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array},\n+    )\n     emb_array = np.zeros((nrof_images, embedding_size))\n     nrof_batches = int(np.ceil(nrof_images / batch_size))\n     label_check_array = np.zeros((nrof_images,))\n     for i in xrange(nrof_batches):\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n-        emb_array[lab,:] = emb\n+        batch_size = min(nrof_images - i * batch_size, batch_size)\n+        emb, lab = sess.run(\n+            [embeddings, labels_batch],\n+            feed_dict={\n+                batch_size_placeholder: batch_size,\n+                learning_rate_placeholder: 0.0,\n+                phase_train_placeholder: False,\n+            },\n+        )\n+        emb_array[lab, :] = emb\n         label_check_array[lab] = 1\n-    print(\'%.3f\' % (time.time()-start_time))\n-    \n-    assert(np.all(label_check_array==1))\n-    \n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n-    \n-    print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n+    print("%.3f" % (time.time() - start_time))\n+\n+    assert np.all(label_check_array == 1)\n+\n+    _, _, accuracy, val, val_std, far = lfw.evaluate(\n+        emb_array, actual_issame, nrof_folds=nrof_folds\n+    )\n+\n+    print("Accuracy: %1.3f+-%1.3f" % (np.mean(accuracy), np.std(accuracy)))\n+    print("Validation rate: %2.5f+-%2.5f @ FAR=%2.5f" % (val, val_std, far))\n     lfw_time = time.time() - start_time\n     # Add validation loss and accuracy to summary\n     summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n+    # pylint: disable=maybe-no-member\n+    summary.value.add(tag="lfw/accuracy", simple_value=np.mean(accuracy))\n+    summary.value.add(tag="lfw/val_rate", simple_value=val)\n+    summary.value.add(tag="time/lfw", simple_value=lfw_time)\n     summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n+    with open(os.path.join(log_dir, "lfw_result.txt"), "at") as f:\n+        f.write("%d\\t%.5f\\t%.5f\\n" % (step, np.mean(accuracy), val))\n \n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n+\n+def save_variables_and_metagraph(\n+    sess, saver, summary_writer, model_dir, model_name, step\n+):\n     # Save the model checkpoint\n-    print(\'Saving variables\')\n+    print("Saving variables")\n     start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n+    checkpoint_path = os.path.join(model_dir, "model-%s.ckpt" % model_name)\n     saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n     save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n+    print("Variables saved in %.2f seconds" % save_time_variables)\n+    metagraph_filename = os.path.join(model_dir, "model-%s.meta" % model_name)\n+    save_time_metagraph = 0\n     if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n+        print("Saving metagraph")\n         start_time = time.time()\n         saver.export_meta_graph(metagraph_filename)\n         save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n+        print("Metagraph saved in %.2f seconds" % save_time_metagraph)\n     summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n+    # pylint: disable=maybe-no-member\n+    summary.value.add(tag="time/save_variables", simple_value=save_time_variables)\n+    summary.value.add(tag="time/save_metagraph", simple_value=save_time_metagraph)\n     summary_writer.add_summary(summary, step)\n-  \n-  \n+\n+\n def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n+    with open(filename, "r") as f:\n         for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n+            line = line.split("#", 1)[0]\n             if line:\n-                par = line.strip().split(\':\')\n+                par = line.strip().split(":")\n                 e = int(par[0])\n                 lr = float(par[1])\n                 if e <= epoch:\n                     learning_rate = lr\n                 else:\n                     return learning_rate\n-    \n+\n \n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--people_per_batch\', type=int,\n-        help=\'Number of people per batch.\', default=45)\n-    parser.add_argument(\'--images_per_person\', type=int,\n-        help=\'Number of images per person.\', default=40)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--alpha\', type=float,\n-        help=\'Positive to negative triplet distance margin.\', default=0.2)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n+\n+    parser.add_argument(\n+        "--logs_base_dir",\n+        type=str,\n+        help="Directory where to write event logs.",\n+        default="~/logs/facenet",\n+    )\n+    parser.add_argument(\n+        "--models_base_dir",\n+        type=str,\n+        help="Directory where to write trained models and checkpoints.",\n+        default="~/models/facenet",\n+    )\n+    parser.add_argument(\n+        "--gpu_memory_fraction",\n+        type=float,\n+        help="Upper bound on the amount of GPU memory that will be used by the process.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--pretrained_model",\n+        type=str,\n+        help="Load a pretrained model before training starts.",\n+    )\n+    parser.add_argument(\n+        "--data_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned face patches.",\n+        default="~/datasets/casia/casia_maxpy_mtcnnalign_182_160",\n+    )\n+    parser.add_argument(\n+        "--model_def",\n+        type=str,\n+        help="Model definition. Points to a module containing the definition of the inference graph.",\n+        default="models.inception_resnet_v1",\n+    )\n+    parser.add_argument(\n+        "--max_nrof_epochs", type=int, help="Number of epochs to run.", default=500\n+    )\n+    parser.add_argument(\n+        "--batch_size",\n+        type=int,\n+        help="Number of images to process in a batch.",\n+        default=90,\n+    )\n+    parser.add_argument(\n+        "--image_size",\n+        type=int,\n+        help="Image size (height, width) in pixels.",\n+        default=160,\n+    )\n+    parser.add_argument(\n+        "--people_per_batch", type=int, help="Number of people per batch.", default=45\n+    )\n+    parser.add_argument(\n+        "--images_per_person", type=int, help="Number of images per person.", default=40\n+    )\n+    parser.add_argument(\n+        "--epoch_size", type=int, help="Number of batches per epoch.", default=1000\n+    )\n+    parser.add_argument(\n+        "--alpha",\n+        type=float,\n+        help="Positive to negative triplet distance margin.",\n+        default=0.2,\n+    )\n+    parser.add_argument(\n+        "--embedding_size",\n+        type=int,\n+        help="Dimensionality of the embedding.",\n+        default=128,\n+    )\n+    parser.add_argument(\n+        "--random_crop",\n+        help="Performs random cropping of training images. If false, the center image_size pixels from the training images are used. "\n+        + "If the size of the images in the data directory is equal to image_size no cropping is performed",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--random_flip",\n+        help="Performs random horizontal flipping of training images.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--keep_probability",\n+        type=float,\n+        help="Keep probability of dropout for the fully connected layer(s).",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--weight_decay", type=float, help="L2 weight regularization.", default=0.0\n+    )\n+    parser.add_argument(\n+        "--optimizer",\n+        type=str,\n+        choices=["ADAGRAD", "ADADELTA", "ADAM", "RMSPROP", "MOM"],\n+        help="The optimization algorithm to use",\n+        default="ADAGRAD",\n+    )\n+    parser.add_argument(\n+        "--learning_rate",\n+        type=float,\n+        help="Initial learning rate. If set to a negative value a learning rate "\n+        + \'schedule can be specified in the file "learning_rate_schedule.txt"\',\n+        default=0.1,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_epochs",\n+        type=int,\n+        help="Number of epochs between learning rate decay.",\n+        default=100,\n+    )\n+    parser.add_argument(\n+        "--learning_rate_decay_factor",\n+        type=float,\n+        help="Learning rate decay factor.",\n+        default=1.0,\n+    )\n+    parser.add_argument(\n+        "--moving_average_decay",\n+        type=float,\n+        help="Exponential decay for tracking of training parameters.",\n+        default=0.9999,\n+    )\n+    parser.add_argument("--seed", type=int, help="Random seed.", default=666)\n+    parser.add_argument(\n+        "--learning_rate_schedule_file",\n+        type=str,\n+        help="File containing the learning rate schedule that is used when learning_rate is set to to -1.",\n+        default="data/learning_rate_schedule.txt",\n+    )\n \n     # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n+    parser.add_argument(\n+        "--lfw_pairs",\n+        type=str,\n+        help="The file containing the pairs to use for validation.",\n+        default="data/pairs.txt",\n+    )\n+    parser.add_argument(\n+        "--lfw_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned face patches.",\n+        default="",\n+    )\n+    parser.add_argument(\n+        "--lfw_nrof_folds",\n+        type=int,\n+        help="Number of folds to use for cross validation. Mainly used for testing.",\n+        default=10,\n+    )\n     return parser.parse_args(argv)\n-  \n \n-if __name__ == \'__main__\':\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/src/validate_on_lfw.py b/model/src/validate_on_lfw.py\nindex e4f8b0e..3f2815e 100644\n--- a/model/src/validate_on_lfw.py\n+++ b/model/src/validate_on_lfw.py\n@@ -4,19 +4,19 @@ is calculated and plotted. Both the model metagraph and the model parameters nee\n in the same directory, and the metagraph should have the extension \'.meta\'.\n """\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -25,141 +25,251 @@ in the same directory, and the metagraph should have the extension \'.meta\'.\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n+from __future__ import absolute_import, division, print_function\n \n-import tensorflow as tf\n-import numpy as np\n import argparse\n-from model.src import facenet_config as facenet\n-\n-import lfw\n import os\n import sys\n-from tensorflow.python.ops import data_flow_ops\n-from sklearn import metrics\n-from scipy.optimize import brentq\n+\n+import lfw\n+import numpy as np\n+import tensorflow as tf\n from scipy import interpolate\n+from scipy.optimize import brentq\n+from sklearn import metrics\n+from tensorflow.python.ops import data_flow_ops\n+\n+from model.src import facenet_config as facenet\n+\n \n def main(args):\n-  \n+\n     with tf.Graph().as_default():\n-      \n+\n         with tf.Session() as sess:\n-            \n+\n             # Read the file containing the pairs used for testing\n             pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n \n             # Get the paths for the corresponding images\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-            \n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n- \n+            paths, actual_issame = lfw.get_paths(\n+                os.path.expanduser(args.lfw_dir), pairs\n+            )\n+\n+            image_paths_placeholder = tf.placeholder(\n+                tf.string, shape=(None, 1), name="image_paths"\n+            )\n+            labels_placeholder = tf.placeholder(\n+                tf.int32, shape=(None, 1), name="labels"\n+            )\n+            batch_size_placeholder = tf.placeholder(tf.int32, name="batch_size")\n+            control_placeholder = tf.placeholder(\n+                tf.int32, shape=(None, 1), name="control"\n+            )\n+            phase_train_placeholder = tf.placeholder(tf.bool, name="phase_train")\n+\n             nrof_preprocess_threads = 4\n             image_size = (args.image_size, args.image_size)\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\n-                                        shapes=[(1,), (1,), (1,)],\n-                                        shared_name=None, name=None)\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'eval_enqueue_op\')\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-     \n+            eval_input_queue = data_flow_ops.FIFOQueue(\n+                capacity=2000000,\n+                dtypes=[tf.string, tf.int32, tf.int32],\n+                shapes=[(1,), (1,), (1,)],\n+                shared_name=None,\n+                name=None,\n+            )\n+            eval_enqueue_op = eval_input_queue.enqueue_many(\n+                [image_paths_placeholder, labels_placeholder, control_placeholder],\n+                name="eval_enqueue_op",\n+            )\n+            image_batch, label_batch = facenet.create_input_pipeline(\n+                eval_input_queue,\n+                image_size,\n+                nrof_preprocess_threads,\n+                batch_size_placeholder,\n+            )\n+\n             # Load the model\n-            input_map = {\'image_batch\': image_batch, \'label_batch\': label_batch, \'phase_train\': phase_train_placeholder}\n+            input_map = {\n+                "image_batch": image_batch,\n+                "label_batch": label_batch,\n+                "phase_train": phase_train_placeholder,\n+            }\n             facenet.load_model(args.model, input_map=input_map)\n \n             # Get output tensor\n             embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-#              \n+            #\n             coord = tf.train.Coordinator()\n             tf.train.start_queue_runners(coord=coord, sess=sess)\n \n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\n-                args.use_flipped_images, args.use_fixed_image_standardization)\n+            evaluate(\n+                sess,\n+                eval_enqueue_op,\n+                image_paths_placeholder,\n+                labels_placeholder,\n+                phase_train_placeholder,\n+                batch_size_placeholder,\n+                control_placeholder,\n+                embeddings,\n+                label_batch,\n+                paths,\n+                actual_issame,\n+                args.lfw_batch_size,\n+                args.lfw_nrof_folds,\n+                args.distance_metric,\n+                args.subtract_mean,\n+                args.use_flipped_images,\n+                args.use_fixed_image_standardization,\n+            )\n+\n \n-              \n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n+def evaluate(\n+    sess,\n+    enqueue_op,\n+    image_paths_placeholder,\n+    labels_placeholder,\n+    phase_train_placeholder,\n+    batch_size_placeholder,\n+    control_placeholder,\n+    embeddings,\n+    labels,\n+    image_paths,\n+    actual_issame,\n+    batch_size,\n+    nrof_folds,\n+    distance_metric,\n+    subtract_mean,\n+    use_flipped_images,\n+    use_fixed_image_standardization,\n+):\n     # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n+    print("Runnning forward pass on LFW images")\n+\n     # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n+    nrof_embeddings = len(actual_issame) * 2  # nrof_pairs * nrof_images_per_pair\n     nrof_flips = 2 if use_flipped_images else 1\n     nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n+    labels_array = np.expand_dims(np.arange(0, nrof_images), 1)\n+    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths), nrof_flips), 1)\n     control_array = np.zeros_like(labels_array, np.int32)\n     if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n+        control_array += np.ones_like(labels_array) * facenet.FIXED_STANDARDIZATION\n     if use_flipped_images:\n         # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n+        control_array += (labels_array % 2) * facenet.FLIP\n+    sess.run(\n+        enqueue_op,\n+        {\n+            image_paths_placeholder: image_paths_array,\n+            labels_placeholder: labels_array,\n+            control_placeholder: control_array,\n+        },\n+    )\n+\n     embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n+    assert (\n+        nrof_images % batch_size == 0\n+    ), "The number of LFW images must be an integer multiple of the LFW batch size"\n     nrof_batches = nrof_images // batch_size\n     emb_array = np.zeros((nrof_images, embedding_size))\n     lab_array = np.zeros((nrof_images,))\n     for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n+        feed_dict = {phase_train_placeholder: False, batch_size_placeholder: batch_size}\n         emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n         lab_array[lab] = lab\n         emb_array[lab, :] = emb\n         if i % 10 == 9:\n-            print(\'.\', end=\'\')\n+            print(".", end="")\n             sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n+    print("")\n+    embeddings = np.zeros((nrof_embeddings, embedding_size * nrof_flips))\n     if use_flipped_images:\n         # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n+        embeddings[:, :embedding_size] = emb_array[0::2, :]\n+        embeddings[:, embedding_size:] = emb_array[1::2, :]\n     else:\n         embeddings = emb_array\n \n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    \n+    assert (\n+        np.array_equal(lab_array, np.arange(nrof_images)) == True\n+    ), "Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline"\n+    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(\n+        embeddings,\n+        actual_issame,\n+        nrof_folds=nrof_folds,\n+        distance_metric=distance_metric,\n+        subtract_mean=subtract_mean,\n+    )\n+\n+    print("Accuracy: %2.5f+-%2.5f" % (np.mean(accuracy), np.std(accuracy)))\n+    print("Validation rate: %2.5f+-%2.5f @ FAR=%2.5f" % (val, val_std, far))\n+\n     auc = metrics.auc(fpr, tpr)\n-    print(\'Area Under Curve (AUC): %1.3f\' % auc)\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n-    print(\'Equal Error Rate (EER): %1.3f\' % eer)\n-    \n+    print("Area Under Curve (AUC): %1.3f" % auc)\n+    eer = brentq(lambda x: 1.0 - x - interpolate.interp1d(fpr, tpr)(x), 0.0, 1.0)\n+    print("Equal Error Rate (EER): %1.3f" % eer)\n+\n+\n def parse_arguments(argv):\n     parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--distance_metric\', type=int,\n-        help=\'Distance metric  0:euclidian, 1:cosine similarity.\', default=0)\n-    parser.add_argument(\'--use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n+\n+    parser.add_argument(\n+        "lfw_dir",\n+        type=str,\n+        help="Path to the data directory containing aligned LFW face patches.",\n+    )\n+    parser.add_argument(\n+        "--lfw_batch_size",\n+        type=int,\n+        help="Number of images to process in a batch in the LFW test set.",\n+        default=100,\n+    )\n+    parser.add_argument(\n+        "model",\n+        type=str,\n+        help="Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file",\n+    )\n+    parser.add_argument(\n+        "--image_size",\n+        type=int,\n+        help="Image size (height, width) in pixels.",\n+        default=160,\n+    )\n+    parser.add_argument(\n+        "--lfw_pairs",\n+        type=str,\n+        help="The file containing the pairs to use for validation.",\n+        default="data/pairs.txt",\n+    )\n+    parser.add_argument(\n+        "--lfw_nrof_folds",\n+        type=int,\n+        help="Number of folds to use for cross validation. Mainly used for testing.",\n+        default=10,\n+    )\n+    parser.add_argument(\n+        "--distance_metric",\n+        type=int,\n+        help="Distance metric  0:euclidian, 1:cosine similarity.",\n+        default=0,\n+    )\n+    parser.add_argument(\n+        "--use_flipped_images",\n+        help="Concatenates embeddings for the image and its horizontally flipped counterpart.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--subtract_mean",\n+        help="Subtract feature mean before calculating distance.",\n+        action="store_true",\n+    )\n+    parser.add_argument(\n+        "--use_fixed_image_standardization",\n+        help="Performs fixed standardization of images.",\n+        action="store_true",\n+    )\n     return parser.parse_args(argv)\n \n-if __name__ == \'__main__\':\n+\n+if __name__ == "__main__":\n     main(parse_arguments(sys.argv[1:]))\ndiff --git a/model/test/batch_norm_test.py b/model/test/batch_norm_test.py\nindex 48cfd55..408e218 100644\n--- a/model/test/batch_norm_test.py\n+++ b/model/test/batch_norm_test.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -21,46 +21,50 @@\n # SOFTWARE.\n \n import unittest\n-import tensorflow as tf\n+\n import models\n import numpy as np\n import numpy.testing as testing\n-\n-class BatchNormTest(unittest.TestCase):\n+import tensorflow as tf\n \n \n+class BatchNormTest(unittest.TestCase):\n     @unittest.skip("Skip batch norm test case")\n     def testBatchNorm(self):\n-      \n+\n         tf.set_random_seed(123)\n-  \n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\'input\')\n-        phase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n-        \n+\n+        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name="input")\n+        phase_train = tf.placeholder(tf.bool, name="phase_train")\n+\n         # generate random noise to pass into batch norm\n-        #x_gen = tf.random_normal([50,20,20,10])\n-        \n+        # x_gen = tf.random_normal([50,20,20,10])\n+\n         bn = models.network.batch_norm(x, phase_train)\n-        \n+\n         init = tf.global_variables_initializer()\n         sess = tf.Session(config=tf.ConfigProto())\n         sess.run(init)\n-  \n+\n         with sess.as_default():\n-        \n-            #generate a constant variable to pass into batch norm\n-            y = np.random.normal(0, 1, size=(50,20,20,10))\n-            \n+\n+            # generate a constant variable to pass into batch norm\n+            y = np.random.normal(0, 1, size=(50, 20, 20, 10))\n+\n             feed_dict = {x: y, phase_train: True}\n             sess.run(bn, feed_dict=feed_dict)\n-            \n+\n             feed_dict = {x: y, phase_train: False}\n             y1 = sess.run(bn, feed_dict=feed_dict)\n             y2 = sess.run(bn, feed_dict=feed_dict)\n-            \n-            testing.assert_almost_equal(y1, y2, 10, \'Output from two forward passes with phase_train==false should be equal\')\n+\n+            testing.assert_almost_equal(\n+                y1,\n+                y2,\n+                10,\n+                "Output from two forward passes with phase_train==false should be equal",\n+            )\n \n \n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/model/test/center_loss_test.py b/model/test/center_loss_test.py\nindex 196cd11..0369fb6 100644\n--- a/model/test/center_loss_test.py\n+++ b/model/test/center_loss_test.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -21,67 +21,99 @@\n # SOFTWARE.\n \n import unittest\n-import tensorflow as tf\n-import numpy as np\n-import facenet\n \n-class CenterLossTest(unittest.TestCase):\n-  \n+import facenet\n+import numpy as np\n+import tensorflow as tf\n \n \n+class CenterLossTest(unittest.TestCase):\n     def testCenterLoss(self):\n         batch_size = 16\n         nrof_features = 2\n         nrof_classes = 16\n         alfa = 0.5\n-        \n+\n         with tf.Graph().as_default():\n-        \n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\'features\')\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\'labels\')\n+\n+            features = tf.placeholder(\n+                tf.float32, shape=(batch_size, nrof_features), name="features"\n+            )\n+            labels = tf.placeholder(tf.int32, shape=(batch_size,), name="labels")\n \n             # Define center loss\n-            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\n-            \n-            label_to_center = np.array( [ \n-                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\n-                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\n-                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\n-                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \n-                 ])\n-                \n+            center_loss, centers = facenet.center_loss(\n+                features, labels, alfa, nrof_classes\n+            )\n+\n+            label_to_center = np.array(\n+                [\n+                    [-3, -3],\n+                    [-3, -1],\n+                    [-3, 1],\n+                    [-3, 3],\n+                    [-1, -3],\n+                    [-1, -1],\n+                    [-1, 1],\n+                    [-1, 3],\n+                    [1, -3],\n+                    [1, -1],\n+                    [1, 1],\n+                    [1, 3],\n+                    [3, -3],\n+                    [3, -1],\n+                    [3, 1],\n+                    [3, 3],\n+                ]\n+            )\n+\n             sess = tf.Session()\n             with sess.as_default():\n                 sess.run(tf.global_variables_initializer())\n                 np.random.seed(seed=666)\n-                \n-                for _ in range(0,100):\n+\n+                for _ in range(0, 100):\n                     # Create array of random labels\n-                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\n-                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\n+                    lbls = np.random.randint(\n+                        low=0, high=nrof_classes, size=(batch_size,)\n+                    )\n+                    feats = create_features(\n+                        label_to_center, batch_size, nrof_features, lbls\n+                    )\n+\n+                    center_loss_, centers_ = sess.run(\n+                        [center_loss, centers],\n+                        feed_dict={features: feats, labels: lbls},\n+                    )\n \n-                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\n-                    \n                 # After a large number of updates the estimated centers should be close to the true ones\n-                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\'Incorrect estimated centers\')\n-                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\'Incorrect center loss\')\n-                \n+                np.testing.assert_almost_equal(\n+                    centers_,\n+                    label_to_center,\n+                    decimal=5,\n+                    err_msg="Incorrect estimated centers",\n+                )\n+                np.testing.assert_almost_equal(\n+                    center_loss_, 0.0, decimal=5, err_msg="Incorrect center loss"\n+                )\n+\n \n def create_features(label_to_center, batch_size, nrof_features, labels):\n     # Map label to center\n-#     label_to_center_dict = { \n-#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\n-#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\n-#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\n-#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\n-#         }\n+    #     label_to_center_dict = {\n+    #          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\n+    #          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\n+    #          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\n+    #         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\n+    #         }\n     # Create array of features corresponding to the labels\n     feats = np.zeros((batch_size, nrof_features))\n     for i in range(batch_size):\n-        cntr =  label_to_center[labels[i]]\n+        cntr = label_to_center[labels[i]]\n         for j in range(nrof_features):\n-            feats[i,j] = cntr[j]\n+            feats[i, j] = cntr[j]\n     return feats\n-                      \n+\n+\n if __name__ == "__main__":\n     unittest.main()\ndiff --git a/model/test/restore_test.py b/model/test/restore_test.py\nindex befb04d..949532e 100644\n--- a/model/test/restore_test.py\n+++ b/model/test/restore_test.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -20,87 +20,95 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-import unittest\n-import tempfile\n import os\n import shutil\n-import tensorflow as tf\n+import tempfile\n+import unittest\n+\n import numpy as np\n+import tensorflow as tf\n+\n \n class TrainTest(unittest.TestCase):\n-  \n     @classmethod\n     def setUpClass(self):\n         self.tmp_dir = tempfile.mkdtemp()\n-        \n+\n     @classmethod\n     def tearDownClass(self):\n         # Recursively remove the temporary directory\n         shutil.rmtree(self.tmp_dir)\n \n     def test_restore_noema(self):\n-        \n+\n         # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n         x_data = np.random.rand(100).astype(np.float32)\n         y_data = x_data * 0.1 + 0.3\n-        \n+\n         # Try to find values for W and b that compute y_data = W * x_data + b\n         # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n         # figure that out for us.)\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n-        b = tf.Variable(tf.zeros([1]), name=\'b\')\n+        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name="W")\n+        b = tf.Variable(tf.zeros([1]), name="b")\n         y = W * x_data + b\n-        \n+\n         # Minimize the mean squared errors.\n         loss = tf.reduce_mean(tf.square(y - y_data))\n         optimizer = tf.train.GradientDescentOptimizer(0.5)\n         train = optimizer.minimize(loss)\n-        \n+\n         # Before starting, initialize the variables.  We will \'run\' this first.\n         init = tf.global_variables_initializer()\n \n         saver = tf.train.Saver(tf.trainable_variables())\n-        \n+\n         # Launch the graph.\n         sess = tf.Session()\n         sess.run(init)\n-        \n+\n         # Fit the line.\n         for _ in range(201):\n             sess.run(train)\n-        \n-        w_reference = sess.run(\'W:0\')\n-        b_reference = sess.run(\'b:0\')\n-        \n+\n+        w_reference = sess.run("W:0")\n+        b_reference = sess.run("b:0")\n+\n         saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\n-        \n+\n         tf.reset_default_graph()\n \n         saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n         sess = tf.Session()\n         saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\n-        \n-        w_restored = sess.run(\'W:0\')\n-        b_restored = sess.run(\'b:0\')\n-        \n-        self.assertAlmostEqual(w_reference, w_restored, \'Restored model use different weight than the original model\')\n-        self.assertAlmostEqual(b_reference, b_restored, \'Restored model use different weight than the original model\')\n \n+        w_restored = sess.run("W:0")\n+        b_restored = sess.run("b:0")\n+\n+        self.assertAlmostEqual(\n+            w_reference,\n+            w_restored,\n+            "Restored model use different weight than the original model",\n+        )\n+        self.assertAlmostEqual(\n+            b_reference,\n+            b_restored,\n+            "Restored model use different weight than the original model",\n+        )\n \n     @unittest.skip("Skip restore EMA test case for now")\n     def test_restore_ema(self):\n-        \n+\n         # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n         x_data = np.random.rand(100).astype(np.float32)\n         y_data = x_data * 0.1 + 0.3\n-        \n+\n         # Try to find values for W and b that compute y_data = W * x_data + b\n         # (We know that W should be 0.1 and b 0.3, but TensorFlow will\n         # figure that out for us.)\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\'W\')\n-        b = tf.Variable(tf.zeros([1]), name=\'b\')\n+        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name="W")\n+        b = tf.Variable(tf.zeros([1]), name="b")\n         y = W * x_data + b\n-        \n+\n         # Minimize the mean squared errors.\n         loss = tf.reduce_mean(tf.square(y - y_data))\n         optimizer = tf.train.GradientDescentOptimizer(0.5)\n@@ -111,71 +119,79 @@ class TrainTest(unittest.TestCase):\n         averages_op = ema.apply(tf.trainable_variables())\n         with tf.control_dependencies([opt_op]):\n             train_op = tf.group(averages_op)\n-  \n+\n         # Before starting, initialize the variables.  We will \'run\' this first.\n         init = tf.global_variables_initializer()\n \n         saver = tf.train.Saver(tf.trainable_variables())\n-        \n+\n         # Launch the graph.\n         sess = tf.Session()\n         sess.run(init)\n-        \n+\n         # Fit the line.\n         for _ in range(201):\n             sess.run(train_op)\n-        \n-        w_reference = sess.run(\'W/ExponentialMovingAverage:0\')\n-        b_reference = sess.run(\'b/ExponentialMovingAverage:0\')\n-        \n+\n+        w_reference = sess.run("W/ExponentialMovingAverage:0")\n+        b_reference = sess.run("b/ExponentialMovingAverage:0")\n+\n         saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\n-                \n+\n         tf.reset_default_graph()\n \n         tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\n         sess = tf.Session()\n-        \n-        print(\'------------------------------------------------------\')\n+\n+        print("------------------------------------------------------")\n         for var in tf.global_variables():\n-            print(\'all variables: \' + var.op.name)\n+            print("all variables: " + var.op.name)\n         for var in tf.trainable_variables():\n-            print(\'normal variable: \' + var.op.name)\n+            print("normal variable: " + var.op.name)\n         for var in tf.moving_average_variables():\n-            print(\'ema variable: \' + var.op.name)\n-        print(\'------------------------------------------------------\')\n+            print("ema variable: " + var.op.name)\n+        print("------------------------------------------------------")\n \n         mode = 1\n         restore_vars = {}\n         if mode == 0:\n             ema = tf.train.ExponentialMovingAverage(1.0)\n             for var in tf.trainable_variables():\n-                print(\'%s: %s\' % (ema.average_name(var), var.op.name))\n+                print("%s: %s" % (ema.average_name(var), var.op.name))\n                 restore_vars[ema.average_name(var)] = var\n         elif mode == 1:\n             for var in tf.trainable_variables():\n-                ema_name = var.op.name + \'/ExponentialMovingAverage\'\n-                print(\'%s: %s\' % (ema_name, var.op.name))\n+                ema_name = var.op.name + "/ExponentialMovingAverage"\n+                print("%s: %s" % (ema_name, var.op.name))\n                 restore_vars[ema_name] = var\n-            \n-        saver = tf.train.Saver(restore_vars, name=\'ema_restore\')\n-        \n+\n+        saver = tf.train.Saver(restore_vars, name="ema_restore")\n+\n         saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\n-        \n-        w_restored = sess.run(\'W:0\')\n-        b_restored = sess.run(\'b:0\')\n-        \n-        self.assertAlmostEqual(w_reference, w_restored, \'Restored model modes not use the EMA filtered weight\')\n-        self.assertAlmostEqual(b_reference, b_restored, \'Restored model modes not use the EMA filtered bias\')\n-\n-        \n+\n+        w_restored = sess.run("W:0")\n+        b_restored = sess.run("b:0")\n+\n+        self.assertAlmostEqual(\n+            w_reference,\n+            w_restored,\n+            "Restored model modes not use the EMA filtered weight",\n+        )\n+        self.assertAlmostEqual(\n+            b_reference,\n+            b_restored,\n+            "Restored model modes not use the EMA filtered bias",\n+        )\n+\n+\n # Create a checkpoint file pointing to the model\n def create_checkpoint_file(model_dir, model_file):\n-    checkpoint_filename = os.path.join(model_dir, \'checkpoint\')\n+    checkpoint_filename = os.path.join(model_dir, "checkpoint")\n     full_model_filename = os.path.join(model_dir, model_file)\n-    with open(checkpoint_filename, \'w\') as f:\n+    with open(checkpoint_filename, "w") as f:\n         f.write(\'model_checkpoint_path: "%s"\\n\' % full_model_filename)\n         f.write(\'all_model_checkpoint_paths: "%s"\\n\' % full_model_filename)\n-        \n+\n+\n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/model/test/train_test.py b/model/test/train_test.py\nindex 12cd663..c262b27 100644\n--- a/model/test/train_test.py\n+++ b/model/test/train_test.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -20,49 +20,64 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-import unittest\n-import tempfile\n-import numpy as np\n-import cv2\n import os\n import shutil\n-import download_and_extract  # @UnresolvedImport\n import subprocess\n+import tempfile\n+import unittest\n+\n+import cv2\n+import download_and_extract  # @UnresolvedImport\n+import numpy as np\n+\n \n def memory_usage_psutil():\n     # return the memory usage in MB\n     import psutil\n+\n     process = psutil.Process(os.getpid())\n     mem = process.memory_info()[0] / float(2 ** 20)\n     return mem\n \n+\n def align_dataset_if_needed(self):\n-    if not os.path.exists(\'data/lfw_aligned\'):\n-        argv = [\'python\',\n-                \'src/align/align_dataset_mtcnn.py\',\n-                \'data/lfw\',\n-                \'data/lfw_aligned\',\n-                \'--image_size\', \'160\',\n-                \'--margin\', \'32\' ]\n+    if not os.path.exists("data/lfw_aligned"):\n+        argv = [\n+            "python",\n+            "src/align/align_dataset_mtcnn.py",\n+            "data/lfw",\n+            "data/lfw_aligned",\n+            "--image_size",\n+            "160",\n+            "--margin",\n+            "32",\n+        ]\n         subprocess.call(argv)\n-        \n-        \n+\n+\n class TrainTest(unittest.TestCase):\n-  \n     @classmethod\n     def setUpClass(self):\n         self.tmp_dir = tempfile.mkdtemp()\n-        self.dataset_dir = os.path.join(self.tmp_dir, \'dataset\')\n+        self.dataset_dir = os.path.join(self.tmp_dir, "dataset")\n         create_mock_dataset(self.dataset_dir, 160)\n         self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\n         print(self.lfw_pairs_file)\n-        self.pretrained_model_name = \'20180402-114759\'\n-        download_and_extract.download_and_extract_file(self.pretrained_model_name, \'data/\')\n-        download_and_extract.download_and_extract_file(\'lfw-subset\', \'data/\')\n-        self.model_file = os.path.join(\'data\', self.pretrained_model_name, \'model-%s.ckpt-275\' % self.pretrained_model_name)\n-        self.pretrained_model = os.path.join(\'data\', self.pretrained_model_name)\n-        self.frozen_graph_filename = os.path.join(\'data\', self.pretrained_model_name+\'.pb\')\n-        print(\'Memory utilization (SetUpClass): %.3f MB\' % memory_usage_psutil())\n+        self.pretrained_model_name = "20180402-114759"\n+        download_and_extract.download_and_extract_file(\n+            self.pretrained_model_name, "data/"\n+        )\n+        download_and_extract.download_and_extract_file("lfw-subset", "data/")\n+        self.model_file = os.path.join(\n+            "data",\n+            self.pretrained_model_name,\n+            "model-%s.ckpt-275" % self.pretrained_model_name,\n+        )\n+        self.pretrained_model = os.path.join("data", self.pretrained_model_name)\n+        self.frozen_graph_filename = os.path.join(\n+            "data", self.pretrained_model_name + ".pb"\n+        )\n+        print("Memory utilization (SetUpClass): %.3f MB" % memory_usage_psutil())\n \n     @classmethod\n     def tearDownClass(self):\n@@ -70,177 +85,268 @@ class TrainTest(unittest.TestCase):\n         shutil.rmtree(self.tmp_dir)\n \n     def tearDown(self):\n-        print(\'Memory utilization (TearDown): %.3f MB\' % memory_usage_psutil())\n+        print("Memory utilization (TearDown): %.3f MB" % memory_usage_psutil())\n \n     def test_training_classifier_inception_resnet_v1(self):\n-        print(\'test_training_classifier_inception_resnet_v1\')\n-        argv = [\'python\',\n-                \'src/train_softmax.py\',\n-                \'--logs_base_dir\', self.tmp_dir,\n-                \'--models_base_dir\', self.tmp_dir,\n-                \'--data_dir\', self.dataset_dir,\n-                \'--model_def\', \'models.inception_resnet_v1\',\n-                \'--epoch_size\', \'1\',\n-                \'--max_nrof_epochs\', \'1\',\n-                \'--batch_size\', \'1\',\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_dir\', self.dataset_dir,\n-                \'--lfw_nrof_folds\', \'2\',\n-                \'--lfw_batch_size\', \'1\',\n-                \'--nrof_preprocess_threads\', \'1\' ]\n+        print("test_training_classifier_inception_resnet_v1")\n+        argv = [\n+            "python",\n+            "src/train_softmax.py",\n+            "--logs_base_dir",\n+            self.tmp_dir,\n+            "--models_base_dir",\n+            self.tmp_dir,\n+            "--data_dir",\n+            self.dataset_dir,\n+            "--model_def",\n+            "models.inception_resnet_v1",\n+            "--epoch_size",\n+            "1",\n+            "--max_nrof_epochs",\n+            "1",\n+            "--batch_size",\n+            "1",\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_dir",\n+            self.dataset_dir,\n+            "--lfw_nrof_folds",\n+            "2",\n+            "--lfw_batch_size",\n+            "1",\n+            "--nrof_preprocess_threads",\n+            "1",\n+        ]\n         subprocess.call(argv)\n \n     def test_training_classifier_inception_resnet_v2(self):\n-        print(\'test_training_classifier_inception_resnet_v2\')\n-        argv = [\'python\',\n-                \'src/train_softmax.py\',\n-                \'--logs_base_dir\', self.tmp_dir,\n-                \'--models_base_dir\', self.tmp_dir,\n-                \'--data_dir\', self.dataset_dir,\n-                \'--model_def\', \'models.inception_resnet_v2\',\n-                \'--epoch_size\', \'1\',\n-                \'--max_nrof_epochs\', \'1\',\n-                \'--batch_size\', \'1\',\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_dir\', self.dataset_dir,\n-                \'--lfw_nrof_folds\', \'2\',\n-                \'--lfw_batch_size\', \'1\' ]\n+        print("test_training_classifier_inception_resnet_v2")\n+        argv = [\n+            "python",\n+            "src/train_softmax.py",\n+            "--logs_base_dir",\n+            self.tmp_dir,\n+            "--models_base_dir",\n+            self.tmp_dir,\n+            "--data_dir",\n+            self.dataset_dir,\n+            "--model_def",\n+            "models.inception_resnet_v2",\n+            "--epoch_size",\n+            "1",\n+            "--max_nrof_epochs",\n+            "1",\n+            "--batch_size",\n+            "1",\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_dir",\n+            self.dataset_dir,\n+            "--lfw_nrof_folds",\n+            "2",\n+            "--lfw_batch_size",\n+            "1",\n+        ]\n         subprocess.call(argv)\n-  \n+\n     def test_training_classifier_squeezenet(self):\n-        print(\'test_training_classifier_squeezenet\')\n-        argv = [\'python\',\n-                \'src/train_softmax.py\',\n-                \'--logs_base_dir\', self.tmp_dir,\n-                \'--models_base_dir\', self.tmp_dir,\n-                \'--data_dir\', self.dataset_dir,\n-                \'--model_def\', \'models.squeezenet\',\n-                \'--epoch_size\', \'1\',\n-                \'--max_nrof_epochs\', \'1\',\n-                \'--batch_size\', \'1\',\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_dir\', self.dataset_dir,\n-                \'--lfw_nrof_folds\', \'2\',\n-                \'--lfw_batch_size\', \'1\',\n-                \'--nrof_preprocess_threads\', \'1\' ]\n+        print("test_training_classifier_squeezenet")\n+        argv = [\n+            "python",\n+            "src/train_softmax.py",\n+            "--logs_base_dir",\n+            self.tmp_dir,\n+            "--models_base_dir",\n+            self.tmp_dir,\n+            "--data_dir",\n+            self.dataset_dir,\n+            "--model_def",\n+            "models.squeezenet",\n+            "--epoch_size",\n+            "1",\n+            "--max_nrof_epochs",\n+            "1",\n+            "--batch_size",\n+            "1",\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_dir",\n+            self.dataset_dir,\n+            "--lfw_nrof_folds",\n+            "2",\n+            "--lfw_batch_size",\n+            "1",\n+            "--nrof_preprocess_threads",\n+            "1",\n+        ]\n         subprocess.call(argv)\n- \n+\n     def test_train_tripletloss_inception_resnet_v1(self):\n-        print(\'test_train_tripletloss_inception_resnet_v1\')\n-        argv = [\'python\',\n-                \'src/train_tripletloss.py\',\n-                \'--logs_base_dir\', self.tmp_dir,\n-                \'--models_base_dir\', self.tmp_dir,\n-                \'--data_dir\', self.dataset_dir,\n-                \'--model_def\', \'models.inception_resnet_v1\',\n-                \'--epoch_size\', \'1\',\n-                \'--max_nrof_epochs\', \'1\',\n-                \'--batch_size\', \'6\',\n-                \'--people_per_batch\', \'2\',\n-                \'--images_per_person\', \'3\',\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_dir\', self.dataset_dir,\n-                \'--lfw_nrof_folds\', \'2\' ]\n+        print("test_train_tripletloss_inception_resnet_v1")\n+        argv = [\n+            "python",\n+            "src/train_tripletloss.py",\n+            "--logs_base_dir",\n+            self.tmp_dir,\n+            "--models_base_dir",\n+            self.tmp_dir,\n+            "--data_dir",\n+            self.dataset_dir,\n+            "--model_def",\n+            "models.inception_resnet_v1",\n+            "--epoch_size",\n+            "1",\n+            "--max_nrof_epochs",\n+            "1",\n+            "--batch_size",\n+            "6",\n+            "--people_per_batch",\n+            "2",\n+            "--images_per_person",\n+            "3",\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_dir",\n+            self.dataset_dir,\n+            "--lfw_nrof_folds",\n+            "2",\n+        ]\n         subprocess.call(argv)\n-  \n+\n     def test_finetune_tripletloss_inception_resnet_v1(self):\n-        print(\'test_finetune_tripletloss_inception_resnet_v1\')\n-        argv = [\'python\',\n-                \'src/train_tripletloss.py\',\n-                \'--logs_base_dir\', self.tmp_dir,\n-                \'--models_base_dir\', self.tmp_dir,\n-                \'--data_dir\', self.dataset_dir,\n-                \'--model_def\', \'models.inception_resnet_v1\',\n-                \'--pretrained_model\', self.model_file,\n-                \'--embedding_size\', \'512\',\n-                \'--epoch_size\', \'1\',\n-                \'--max_nrof_epochs\', \'1\',\n-                \'--batch_size\', \'6\',\n-                \'--people_per_batch\', \'2\',\n-                \'--images_per_person\', \'3\',\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_dir\', self.dataset_dir,\n-                \'--lfw_nrof_folds\', \'2\' ]\n+        print("test_finetune_tripletloss_inception_resnet_v1")\n+        argv = [\n+            "python",\n+            "src/train_tripletloss.py",\n+            "--logs_base_dir",\n+            self.tmp_dir,\n+            "--models_base_dir",\n+            self.tmp_dir,\n+            "--data_dir",\n+            self.dataset_dir,\n+            "--model_def",\n+            "models.inception_resnet_v1",\n+            "--pretrained_model",\n+            self.model_file,\n+            "--embedding_size",\n+            "512",\n+            "--epoch_size",\n+            "1",\n+            "--max_nrof_epochs",\n+            "1",\n+            "--batch_size",\n+            "6",\n+            "--people_per_batch",\n+            "2",\n+            "--images_per_person",\n+            "3",\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_dir",\n+            self.dataset_dir,\n+            "--lfw_nrof_folds",\n+            "2",\n+        ]\n         subprocess.call(argv)\n-  \n+\n     def test_compare(self):\n-        print(\'test_compare\')\n-        argv = [\'python\',\n-                \'src/compare.py\',\n-                os.path.join(\'data/\', self.pretrained_model_name),\n-                \'data/images/Anthony_Hopkins_0001.jpg\',\n-                \'data/images/Anthony_Hopkins_0002.jpg\' ]\n+        print("test_compare")\n+        argv = [\n+            "python",\n+            "src/compare.py",\n+            os.path.join("data/", self.pretrained_model_name),\n+            "data/images/Anthony_Hopkins_0001.jpg",\n+            "data/images/Anthony_Hopkins_0002.jpg",\n+        ]\n         subprocess.call(argv)\n-         \n+\n     def test_validate_on_lfw(self):\n-        print(\'test_validate_on_lfw\')\n+        print("test_validate_on_lfw")\n         align_dataset_if_needed(self)\n-        argv = [\'python\',\n-                \'src/validate_on_lfw.py\', \n-                \'data/lfw_aligned\',\n-                self.pretrained_model,\n-                \'--lfw_pairs\', \'data/lfw/pairs_small.txt\',\n-                \'--lfw_nrof_folds\', \'2\',\n-                \'--lfw_batch_size\', \'6\']\n+        argv = [\n+            "python",\n+            "src/validate_on_lfw.py",\n+            "data/lfw_aligned",\n+            self.pretrained_model,\n+            "--lfw_pairs",\n+            "data/lfw/pairs_small.txt",\n+            "--lfw_nrof_folds",\n+            "2",\n+            "--lfw_batch_size",\n+            "6",\n+        ]\n         subprocess.call(argv)\n- \n+\n     def test_validate_on_lfw_frozen_graph(self):\n-        print(\'test_validate_on_lfw_frozen_graph\')\n-        self.pretrained_model = os.path.join(\'data\', self.pretrained_model_name)\n-        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\'.pb\')\n-        argv = [\'python\',\n-                \'src/validate_on_lfw.py\',\n-                self.dataset_dir,\n-                frozen_model,\n-                \'--lfw_pairs\', self.lfw_pairs_file,\n-                \'--lfw_nrof_folds\', \'2\',\n-                \'--lfw_batch_size\', \'6\']\n+        print("test_validate_on_lfw_frozen_graph")\n+        self.pretrained_model = os.path.join("data", self.pretrained_model_name)\n+        frozen_model = os.path.join(\n+            self.pretrained_model, self.pretrained_model_name + ".pb"\n+        )\n+        argv = [\n+            "python",\n+            "src/validate_on_lfw.py",\n+            self.dataset_dir,\n+            frozen_model,\n+            "--lfw_pairs",\n+            self.lfw_pairs_file,\n+            "--lfw_nrof_folds",\n+            "2",\n+            "--lfw_batch_size",\n+            "6",\n+        ]\n         subprocess.call(argv)\n- \n+\n     def test_freeze_graph(self):\n-        print(\'test_freeze_graph\')\n-        argv = [\'python\',\n-                \'src/freeze_graph.py\',\n-                self.pretrained_model,\n-                self.frozen_graph_filename ]\n+        print("test_freeze_graph")\n+        argv = [\n+            "python",\n+            "src/freeze_graph.py",\n+            self.pretrained_model,\n+            self.frozen_graph_filename,\n+        ]\n         subprocess.call(argv)\n \n+\n # Create a mock dataset with random pixel images\n def create_mock_dataset(dataset_dir, image_size):\n-   \n+\n     nrof_persons = 3\n     nrof_images_per_person = 2\n     np.random.seed(seed=666)\n     os.mkdir(dataset_dir)\n     for i in range(nrof_persons):\n-        class_name = \'%04d\' % (i+1)\n+        class_name = "%04d" % (i + 1)\n         class_dir = os.path.join(dataset_dir, class_name)\n         os.mkdir(class_dir)\n         for j in range(nrof_images_per_person):\n-            img_name = \'%04d\' % (j+1)\n-            img_path = os.path.join(class_dir, class_name+\'_\'+img_name + \'.png\')\n-            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\n-            cv2.imwrite(img_path, img) #@UndefinedVariable\n+            img_name = "%04d" % (j + 1)\n+            img_path = os.path.join(class_dir, class_name + "_" + img_name + ".png")\n+            img = np.random.uniform(\n+                low=0.0, high=255.0, size=(image_size, image_size, 3)\n+            )\n+            cv2.imwrite(img_path, img)  # @UndefinedVariable\n+\n \n # Create a mock LFW pairs file\n def create_mock_lfw_pairs(tmp_dir):\n-    pairs_filename = os.path.join(tmp_dir, \'pairs_mock.txt\')\n-    with open(pairs_filename, \'w\') as f:\n-        f.write(\'10 300\\n\')\n-        f.write(\'0001 1 2\\n\')\n-        f.write(\'0001 1 0002 1\\n\')\n-        f.write(\'0002 1 0003 1\\n\')\n-        f.write(\'0001 1 0003 1\\n\')\n-        f.write(\'0002 1 2\\n\')\n-        f.write(\'0001 2 0002 2\\n\')\n-        f.write(\'0002 2 0003 2\\n\')\n-        f.write(\'0001 2 0003 2\\n\')\n-        f.write(\'0003 1 2\\n\')\n-        f.write(\'0001 1 0002 2\\n\')\n-        f.write(\'0002 1 0003 2\\n\')\n-        f.write(\'0001 1 0003 2\\n\')\n+    pairs_filename = os.path.join(tmp_dir, "pairs_mock.txt")\n+    with open(pairs_filename, "w") as f:\n+        f.write("10 300\\n")\n+        f.write("0001 1 2\\n")\n+        f.write("0001 1 0002 1\\n")\n+        f.write("0002 1 0003 1\\n")\n+        f.write("0001 1 0003 1\\n")\n+        f.write("0002 1 2\\n")\n+        f.write("0001 2 0002 2\\n")\n+        f.write("0002 2 0003 2\\n")\n+        f.write("0001 2 0003 2\\n")\n+        f.write("0003 1 2\\n")\n+        f.write("0001 1 0002 2\\n")\n+        f.write("0002 1 0003 2\\n")\n+        f.write("0001 1 0003 2\\n")\n     return pairs_filename\n \n+\n if __name__ == "__main__":\n     unittest.main()\n-    \n\\ No newline at end of file\ndiff --git a/model/test/triplet_loss_test.py b/model/test/triplet_loss_test.py\nindex 2648b30..d4f7033 100644\n--- a/model/test/triplet_loss_test.py\n+++ b/model/test/triplet_loss_test.py\n@@ -1,17 +1,17 @@\n # MIT License\n-# \n+#\n # Copyright (c) 2016 David Sandberg\n-# \n+#\n # Permission is hereby granted, free of charge, to any person obtaining a copy\n # of this software and associated documentation files (the "Software"), to deal\n # in the Software without restriction, including without limitation the rights\n # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n # copies of the Software, and to permit persons to whom the Software is\n # furnished to do so, subject to the following conditions:\n-# \n+#\n # The above copyright notice and this permission notice shall be included in all\n # copies or substantial portions of the Software.\n-# \n+#\n # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n@@ -21,34 +21,47 @@\n # SOFTWARE.\n \n import unittest\n-import tensorflow as tf\n-import numpy as np\n+\n import facenet\n+import numpy as np\n+import tensorflow as tf\n+\n \n class DemuxEmbeddingsTest(unittest.TestCase):\n-  \n     def testDemuxEmbeddings(self):\n-        batch_size = 3*12\n+        batch_size = 3 * 12\n         embedding_size = 16\n         alpha = 0.2\n-        \n+\n         with tf.Graph().as_default():\n-        \n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\'embeddings\')\n-            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\n+\n+            embeddings = tf.placeholder(\n+                tf.float64, shape=(batch_size, embedding_size), name="embeddings"\n+            )\n+            anchor, positive, negative = tf.unstack(\n+                tf.reshape(embeddings, [-1, 3, embedding_size]), 3, 1\n+            )\n             triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\n-                \n+\n             sess = tf.Session()\n             with sess.as_default():\n                 np.random.seed(seed=666)\n                 emb = np.random.uniform(size=(batch_size, embedding_size))\n-                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\n-\n-                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\n-                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\n-                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\n-                \n-                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\'Triplet loss is incorrect\')\n-                      \n+                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings: emb})\n+\n+                pos_dist_sqr = np.sum(np.square(emb[0::3, :] - emb[1::3, :]), 1)\n+                neg_dist_sqr = np.sum(np.square(emb[0::3, :] - emb[2::3, :]), 1)\n+                np_triplet_loss = np.mean(\n+                    np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha)\n+                )\n+\n+                np.testing.assert_almost_equal(\n+                    tf_triplet_loss,\n+                    np_triplet_loss,\n+                    decimal=5,\n+                    err_msg="Triplet loss is incorrect",\n+                )\n+\n+\n if __name__ == "__main__":\n     unittest.main()\ndiff --git a/script.sh b/script.sh\nindex 2985533..667dda0 100644\n--- a/script.sh\n+++ b/script.sh\n@@ -11,11 +11,11 @@ source activate cinnamon\n alias python=python3\n \n # Download pretrained facenet\n-cd model\n-wget --load-cookies /tmp/cookies.txt "https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \'https://drive.google.com/uc?export=download&id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\' -O- | sed -rn \'s/.confirm=([0-9A-Za-z]+)._/\\1\\n/p\')&id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-" -O pretrained.zip && rm -rf /tmp/cookies.txt\n-unzip pretrained.zip\n-mv 20180402-114759 pretrained\n-cd ..\n+# cd model\n+# wget --load-cookies /tmp/cookies.txt "https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \'https://drive.google.com/uc?export=download&id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\' -O- | sed -rn \'s/.confirm=([0-9A-Za-z]+)._/\\1\\n/p\')&id=1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-" -O pretrained.zip && rm -rf /tmp/cookies.txt\n+# unzip pretrained.zip\n+# mv 20180402-114759 pretrained\n+# cd ..\n \n # Install the requirements from the \'model/requirements.txt\' file\n pip3 install -r model/requirements.txt'